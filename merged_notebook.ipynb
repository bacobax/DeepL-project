{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3085bf71",
   "metadata": {},
   "source": [
    "# Disclaimer on Notebook Version\n",
    "\n",
    "This notebook contains a simplified, self-contained version of the codebase developed for our project on zero-shot generalization in CLIP. The original project code is modular and includes features that are not compatible with notebook constraints.\n",
    "\n",
    "For the full implementation and all functionality, refer to:\n",
    "- GitHub repository: https://github.com/bacobax/DeepL-project  \n",
    "- Project report (PDF): https://drive.google.com/file/d/1Mi2i5793zcRzsxmeUoWpJyaR2JdbhuhQ/view?usp=sharing\n",
    "\n",
    "### Limitations of this Notebook Version\n",
    "\n",
    "To comply with the self-contained nature of Jupyter notebooks, some features have been removed or modified:\n",
    "\n",
    "- **YAML-based configuration**: Disabled to avoid external file dependencies. An example configuration is instead defined directly in the notebook.\n",
    "- **Clustering caching**: Saving and loading of clustering results is disabled. Clustering is recomputed every time the program runs.\n",
    "- **Log-based result plots**: Since logs are not easily handled in the notebook, the original repository is still used to generate result plots.\n",
    "\n",
    "This notebook aims to demonstrate the core components and workflow but is not a full replacement for the original repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18be3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#install the requirements for the project\n",
    "\n",
    "%pip install annotated-types==0.7.0 git+https://github.com/openai/CLIP.git@dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1 contourpy==1.3.1 cycler==0.12.1 docker-pycreds==0.4.0 easydict==1.13 gitdb==4.0.12 GitPython==3.1.44 git+https://github.com/greydanus/mnist1d@7878d96082abd200c546a07a4101fa90b30fdf7e pooch==1.8.2 pydantic==2.10.6 pydantic_core==2.27.2 PyWavelets==1.8.0 regex==2024.11.6 sentry-sdk==2.23.1 setproctitle==1.3.5 smmap==5.0.2 tensorboard-data-server==0.7.2 wandb==0.19.8 zstandard==0.23.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e2b33f",
   "metadata": {
    "id": "23e2b33f"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This project focuses on improving few-shot learning performance on a fixed dataset and experimental setup chosen by the instructor. The main constraint was to build upon the CoCoOp prompt learning method.\n",
    "\n",
    "Experiments were conducted on the Oxford 102 Flowers dataset, divided into base and novel classes as specified by the course guidelines. The model is trained using few-shot samples from the base classes and evaluated on both base and novel classes to assess generalization.\n",
    "\n",
    "Our approach explores modular training techniques such as KL divergence-based knowledge distillation and adversarial training to enhance CoCoOp‚Äôs ability to generalize to novel classes. The methods are designed to be flexible, allowing various combinations and settings to be tested.\n",
    "\n",
    "This report details our implementation, experiments, and results within the context of the course project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e14c7",
   "metadata": {
    "id": "357e14c7"
   },
   "source": [
    "# Training Systems\n",
    "\n",
    "### CoCoOp Infrastructure\n",
    "\n",
    "**CoOp** [^4] and **CoCoOp** [^3] are prompt learning methods that adapt pretrained vision-language models by optimizing learnable context vectors (prompts) to improve performance on downstream tasks. While CoOp learns static prompts shared across all inputs, CoCoOp generates dynamic, image-conditioned prompts via a meta-network to better generalize to unseen classes and domains.\n",
    "\n",
    "In our implementation, we first reproduced CoOp [^4] to obtain learned context vectors (`ctx`), which we then used to initialize CoCoOp [^3] rather than training it from scratch. This approach offers several benefits:\n",
    "\n",
    "- **Improved Generalization:** CoOp‚Äôs context vectors, learned from limited few-shot data, provide a strong discriminative foundation that helps CoCoOp generalize better across domains and unseen classes.\n",
    "\n",
    "- **Stabilized Meta-Network Training:** Initializing CoCoOp‚Äôs prompt learner close to a working solution improves training stability and convergence speed.\n",
    "\n",
    "- **Better Semantic Initialization:** CoOp‚Äôs class-level embeddings encode meaningful semantic information, giving CoCoOp‚Äôs dynamic prompts a valuable head start.\n",
    "\n",
    "This modular design leverages existing learned knowledge to improve efficiency and robustness in few-shot and domain generalization settings, aligning with CoCoOp‚Äôs goal of adaptive prompt generation based on visual context [^3].\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2557f607",
   "metadata": {
    "id": "2557f607"
   },
   "source": [
    "## Model CoOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4061fa",
   "metadata": {
    "id": "7d4061fa"
   },
   "source": [
    "### Custom CLIP CoOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc488d88",
   "metadata": {
    "id": "cc488d88"
   },
   "outputs": [],
   "source": [
    "# From: ./model/coop/custom_clip.py\n",
    "import clip\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "\n",
    "class CustomCLIPCoOp(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = PromptLearnerCoOp(cfg, classnames, clip_model)\n",
    "        self.clip_model = clip_model\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.cfg = cfg\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, image, label=None):\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        #print(\"Raw logit_scale:\", self.logit_scale.item())\n",
    "        #print(\"Exp logit_scale:\", logit_scale)\n",
    "\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        prompts = self.prompt_learner()\n",
    "        text_features = self.text_encoder(prompts.type(self.dtype), self.tokenized_prompts)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        if torch.isnan(image_features).any():\n",
    "            print(\"‚ö†Ô∏è NaNs in image_features!\")\n",
    "\n",
    "        if torch.isnan(text_features).any():\n",
    "            print(\"‚ö†Ô∏è NaNs in text_features!\")\n",
    "\n",
    "        if torch.isinf(image_features).any():\n",
    "            print(\"‚ö†Ô∏è Infs in image_features!\")\n",
    "\n",
    "        if torch.isinf(text_features).any():\n",
    "            print(\"‚ö†Ô∏è Infs in text_features!\")\n",
    "\n",
    "        #print(\"Image feature norm:\", image_features.norm(dim=-1).mean().item())\n",
    "        #print(\"Text feature norm:\", text_features.norm(dim=-1).mean().item())\n",
    "\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        #print(f\"is training: {self.prompt_learner.training}, label: {label}\")\n",
    "\n",
    "        if label is not None:\n",
    "\n",
    "            return F.cross_entropy(logits, label), logits\n",
    "\n",
    "        return logits\n",
    "    @contextmanager\n",
    "    def temporary_classnames(self, new_classnames):\n",
    "        # --- Save original state ---\n",
    "        original_classnames = self.prompt_learner.n_cls\n",
    "        original_tokenized_prompts = self.tokenized_prompts\n",
    "        original_token_prefix = self.prompt_learner.token_prefix\n",
    "        original_token_suffix = self.prompt_learner.token_suffix\n",
    "\n",
    "        # --- Apply temporary state ---\n",
    "        temp_prompt_learner = PromptLearnerCoOp(\n",
    "            cfg=self.cfg,\n",
    "            classnames=new_classnames,\n",
    "            clip_model=self.clip_model\n",
    "        )\n",
    "\n",
    "        self.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.token_prefix = temp_prompt_learner.token_prefix\n",
    "        self.prompt_learner.token_suffix = temp_prompt_learner.token_suffix\n",
    "        self.prompt_learner.n_cls = len(new_classnames)\n",
    "\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            # --- Restore original state ---\n",
    "            self.tokenized_prompts = original_tokenized_prompts\n",
    "            self.prompt_learner.tokenized_prompts = original_tokenized_prompts\n",
    "            self.prompt_learner.token_prefix = original_token_prefix\n",
    "            self.prompt_learner.token_suffix = original_token_suffix\n",
    "            self.prompt_learner.n_cls = original_classnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc1789",
   "metadata": {
    "id": "7ebc1789"
   },
   "source": [
    "### Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e0e05",
   "metadata": {
    "id": "bd7e0e05"
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
    "        x = self.transformer(x.to(dtype=self.transformer.resblocks[0].mlp.c_fc.weight.dtype))\n",
    "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5529e171",
   "metadata": {
    "id": "5529e171"
   },
   "source": [
    "### Prompt Learner CoOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115aa77f",
   "metadata": {
    "id": "115aa77f"
   },
   "outputs": [],
   "source": [
    "# From: ./model/coop/custom_clip.py\n",
    "\n",
    "class PromptLearnerCoOp(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        n_ctx = cfg.TRAINER.COOP.N_CTX\n",
    "        ctx_init = cfg.TRAINER.COOP.CTX_INIT\n",
    "        dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "        cfg_imsize = cfg.INPUT.SIZE[0]\n",
    "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal clip_imsize ({clip_imsize})\"\n",
    "\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        #print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        #print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts.to(clip_model.token_embedding.weight.device))\n",
    "            embedding = embedding.type(dtype)\n",
    "\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])\n",
    "\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "\n",
    "    def construct_prompts(self, ctx, prefix, suffix):\n",
    "        return torch.cat([prefix, ctx, suffix], dim=1)\n",
    "\n",
    "    def forward(self):\n",
    "        ctx = self.ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "        return self.construct_prompts(ctx, self.token_prefix, self.token_suffix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bcf7c0",
   "metadata": {
    "id": "a3bcf7c0"
   },
   "source": [
    "## Training System CoOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac3b2e8",
   "metadata": {
    "id": "aac3b2e8"
   },
   "outputs": [],
   "source": [
    "# From: ./training_systems/coop.py\n",
    "\"\"\"\n",
    "This module defines the CoOpSystem class for training and evaluating a prompt-tuned CLIP model using the CoOp method.\n",
    "It includes data loading, training with early stopping, evaluation, model saving/loading, and logging to TensorBoard.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from easydict import EasyDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from model.coop.custom_clip import CustomCLIPCoOp\n",
    "from utils.datasets import get_data, base_novel_categories, split_data, CLASS_NAMES\n",
    "from utils.training_coop import test_step, training_step, eval_step\n",
    "import clip\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch.optim import SGD, Adam\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CoOpSystem:\n",
    "    \"\"\"\n",
    "    Implements the CoOp prompt tuning system for training and evaluating CLIP-based models.\n",
    "\n",
    "    Attributes:\n",
    "        batch_size (int): Number of samples per training batch.\n",
    "        device (str): Device identifier (e.g., \"cuda:0\") to run training on.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        weight_decay (float): Weight decay used in the optimizer.\n",
    "        momentum (float): Momentum term (if applicable).\n",
    "        epochs (int): Number of training epochs.\n",
    "        run_name (str): Identifier for the experiment run (used for logging and file naming).\n",
    "        n_ctx (int): Number of context tokens for prompt tuning.\n",
    "        ctx_init (str): Initialization string for context tokens.\n",
    "        class_token_position (str): Position of the class token in the prompt.\n",
    "        csc (bool): Whether to use class-specific context.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 batch_size=16,\n",
    "                 device=\"cuda:0\",\n",
    "                 learning_rate=0.002,\n",
    "                 weight_decay=0.0005,\n",
    "                 momentum=0.9,\n",
    "                 epochs=2,\n",
    "                 run_name=\"exp1\",\n",
    "                 n_ctx=4,\n",
    "                 ctx_init=\"\",\n",
    "                 class_token_position=\"end\",\n",
    "                 csc=False,\n",
    "                 ):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.epochs = epochs\n",
    "        self.run_name = run_name\n",
    "        self.n_ctx = n_ctx\n",
    "        self.ctx_init = ctx_init\n",
    "        self.class_token_position = class_token_position\n",
    "        self.csc = csc\n",
    "\n",
    "        # Create a logger for the experiment\n",
    "        self.writer = SummaryWriter(log_dir=f\"runs/CoOp/{run_name}\")\n",
    "        self.writer.add_scalar(f\"lr\", self.learning_rate, 0)\n",
    "        self.writer.add_scalar(f\"momentum\", self.momentum, 0)\n",
    "\n",
    "        # Get dataloaders\n",
    "\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/16\", device=self.device)\n",
    "        self.clip_model = self.clip_model.to(self.device)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        resolution = self.clip_model.visual.input_resolution\n",
    "        self.train_set, self.val_set, self.test_set = get_data(resolution=resolution)\n",
    "\n",
    "        # split classes into base and novel\n",
    "        self.base_classes, self.novel_classes = base_novel_categories(self.train_set)\n",
    "\n",
    "        # split the three datasets\n",
    "        self.train_base, _ = split_data(self.train_set, self.base_classes)\n",
    "        self.val_base, self.val_novel = split_data(self.val_set, self.base_classes)\n",
    "        self.test_base, self.test_novel = split_data(self.test_set, self.base_classes)\n",
    "\n",
    "        #self.classnames, _ = embed_dataset_classnames(dataset_name, preprocess=preprocess, model=clip_model)\n",
    "\n",
    "        resolution = self.clip_model.visual.input_resolution\n",
    "\n",
    "        cfg = EasyDict()\n",
    "        # Training configuration\n",
    "        cfg.TRAINER = EasyDict()\n",
    "        cfg.TRAINER.COOP = EasyDict()\n",
    "        cfg.TRAINER.COOP.N_CTX = self.n_ctx  # Number of context tokens\n",
    "        cfg.TRAINER.COOP.CTX_INIT = self.ctx_init  # Leave empty for random initialization\n",
    "        cfg.INPUT = EasyDict()\n",
    "        cfg.INPUT.SIZE = [resolution, resolution]  # Must match CLIP model's input resolution\n",
    "\n",
    "        # Instantiate the network and move it to the chosen device (GPU)\n",
    "        self.model = CustomCLIPCoOp(\n",
    "            classnames=[CLASS_NAMES[idx] for idx in self.base_classes],\n",
    "            cfg=cfg,\n",
    "            clip_model=self.clip_model,\n",
    "        ).to(device)\n",
    "\n",
    "        print(\"Turning off gradients in both the image and the text encoder\")\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"prompt_learner\" in name:\n",
    "                param.requires_grad_(True)\n",
    "            else:\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        print(f\"Total parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        print(f\"Total trainable parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "        self.optimizer = self.get_optimizer(self.learning_rate, self.weight_decay, self.momentum)\n",
    "        self.cost_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the CoOp model on the base classes with early stopping and logs performance metrics.\n",
    "        Saves the best model and computes evaluation at the end of training.\n",
    "        \"\"\"\n",
    "        print(\"Before training:\")\n",
    "        print(\"Training the model...\")\n",
    "        print_epoch_interval = 1\n",
    "        pbar = tqdm(total=self.epochs, desc=\"OVERALL TRAINING\", position=0, leave=True)\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        patience = 3\n",
    "        counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            base_train_loss, base_train_accuracy = training_step(\n",
    "                model=self.model,\n",
    "                dataset=self.train_base,\n",
    "                optimizer=self.optimizer,\n",
    "                batch_size=self.batch_size,\n",
    "                classnames=self.base_classes,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "            if e % print_epoch_interval == 0:\n",
    "                base_val_loss, base_val_accuracy = eval_step(\n",
    "                    model=self.model,\n",
    "                    dataset=self.val_base,\n",
    "                    cost_function=self.cost_function,\n",
    "                    new_classnames=self.base_classes,\n",
    "                    device=self.device,\n",
    "                    batch_size=self.batch_size,\n",
    "                )\n",
    "\n",
    "                self.log_values(e, base_train_loss, base_train_accuracy, \"train_base\")\n",
    "                self.log_values(e, base_val_loss, base_val_accuracy, \"validation_base\")\n",
    "\n",
    "                pbar.set_postfix(train_acc=base_train_accuracy, val_acc=base_val_accuracy)\n",
    "\n",
    "                # Early stopping check\n",
    "                if base_val_accuracy > best_val_acc:\n",
    "                    best_val_acc = base_val_accuracy\n",
    "                    counter = 0\n",
    "                    best_model_state = self.model.state_dict()\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {e}, best validation accuracy: {best_val_acc:.4f}\")\n",
    "                        break\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Restore best model if early stopped\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "\n",
    "        print(\"After training:\")\n",
    "        self.compute_evaluation(self.epochs)\n",
    "        self.writer.close()\n",
    "\n",
    "        self.save_model()\n",
    "        self.save_prompt_learner()\n",
    "\n",
    "    def save_model(self, path=\"./bin/coop\"):\n",
    "        \"\"\"\n",
    "        Saves the entire model's state dictionary to disk under the specified path.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory path where the model checkpoint will be saved.\n",
    "        \"\"\"\n",
    "        #create folder if not exist\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        # Save the model\n",
    "        torch.save(self.model.state_dict(), os.path.join(path, f\"{self.run_name}.pth\"))\n",
    "\n",
    "    def save_prompt_learner(self, path=\"./bin/coop\"):\n",
    "        \"\"\"\n",
    "        Saves only the prompt learner component of the model to disk.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory path where the prompt learner checkpoint will be saved.\n",
    "        \"\"\"\n",
    "        # Create folder if not exist\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        # Save only the self.ctx parameter of the prompt learner\n",
    "        ctx_state = {\"ctx\": self.model.prompt_learner.ctx.detach().cpu()}\n",
    "        torch.save(ctx_state, os.path.join(path, f\"{self.run_name}_prompt_learner.pth\"))\n",
    "\n",
    "    def load_model(self, path=\"./bin\"):\n",
    "        \"\"\"\n",
    "        Loads a saved model checkpoint from disk and sets the model to evaluation mode.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory path from which the model checkpoint will be loaded.\n",
    "        \"\"\"\n",
    "        # Load the model\n",
    "        self.model.load_state_dict(torch.load(os.path.join(path, f\"{self.run_name}.pth\")))\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "    def compute_evaluation(self, epoch_idx, base=False):\n",
    "        \"\"\"\n",
    "        Evaluates the model (or zero-shot CLIP if base=True) on the base test set and logs accuracy.\n",
    "\n",
    "        Args:\n",
    "            epoch_idx (int): Index of the current epoch for logging.\n",
    "            base (bool): If True, use zero-shot CLIP model for evaluation instead of the trained model.\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy on the base test set.\n",
    "        \"\"\"\n",
    "        base_accuracy = test_step(\n",
    "            self.model if not base else self.clip_model,\n",
    "            self.test_base,\n",
    "            self.batch_size,\n",
    "            self.device,\n",
    "            self.base_classes,\n",
    "            label=\"test\",\n",
    "            base=base\n",
    "        )\n",
    "        # Log to TensorBoard\n",
    "        self.log_value(epoch_idx,  base_accuracy, \"base_classes\")\n",
    "\n",
    "        return base_accuracy\n",
    "\n",
    "    def get_optimizer(self, lr, wd, momentum):\n",
    "        \"\"\"\n",
    "        Instantiates and returns the optimizer for the model parameters.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate.\n",
    "            wd (float): Weight decay.\n",
    "            momentum (float): Momentum term (unused for Adam optimizer).\n",
    "\n",
    "        Returns:\n",
    "            torch.optim.Optimizer: Configured Adam optimizer instance.\n",
    "        \"\"\"\n",
    "        optimizer = Adam([\n",
    "            {\n",
    "                \"params\": self.model.parameters()\n",
    "            }\n",
    "        ], lr=lr, weight_decay=wd)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def log_value(self, step,  accuracy, prefix):\n",
    "        \"\"\"\n",
    "        Logs a single scalar value (accuracy) to TensorBoard.\n",
    "\n",
    "        Args:\n",
    "            step (int): Training step or epoch index.\n",
    "            accuracy (float): Accuracy value to log.\n",
    "            prefix (str): Tag prefix to categorize the metric in TensorBoard.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "    def log_values(self, step, loss, accuracy, prefix):\n",
    "        \"\"\"\n",
    "        Logs both loss and accuracy values to TensorBoard.\n",
    "\n",
    "        Args:\n",
    "            step (int): Training step or epoch index.\n",
    "            loss (float): Loss value to log.\n",
    "            accuracy (float): Accuracy value to log.\n",
    "            prefix (str): Tag prefix to categorize the metrics in TensorBoard.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "        self.writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95e26e",
   "metadata": {
    "id": "ff95e26e"
   },
   "source": [
    "## Model CoCoOp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4522ea",
   "metadata": {
    "id": "0c4522ea"
   },
   "source": [
    "### Custom CLIP CoCoOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba20b5f",
   "metadata": {
    "id": "dba20b5f"
   },
   "outputs": [],
   "source": [
    "# From: ./model/cocoop/custom_clip.py\n",
    "import os.path as osp\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "# from model.cocoop.prompt_learner import PromptLearner\n",
    "# from model.cocoop.mlp_adversary import GradientReversalLayer, AdversarialMLP\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "from easydict import EasyDict\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "\n",
    "class CustomCLIP(nn.Module):\n",
    "    \"\"\"\n",
    "    CustomCLIP is a modified CLIP wrapper supporting CoCoOp prompt learning and adversarial training extensions.\n",
    "    It enables image-text matching via learned prompts and exposes additional methods for class manipulation.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Initialize the CustomCLIP model with the given configuration, classnames, and base CLIP model.\n",
    "    Sets up the image and text encoders and prepares the prompt learner.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = PromptLearner(cfg, classnames, clip_model)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "        self.clip_model = clip_model\n",
    "        self.cfg = cfg\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Updates the prompt learner with a new list of classnames and resets its tokenized prompts.\n",
    "    \"\"\"\n",
    "    def change_classnames(self, new_classnames):\n",
    "        temp_prompt_learner = PromptLearner(\n",
    "            cfg=self.cfg,\n",
    "            classnames=new_classnames,\n",
    "            clip_model=self.clip_model\n",
    "        )\n",
    "\n",
    "        self.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.token_prefix = temp_prompt_learner.token_prefix\n",
    "        self.prompt_learner.token_suffix = temp_prompt_learner.token_suffix\n",
    "        self.prompt_learner.n_cls = len(new_classnames)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    A context manager that temporarily sets new classnames for inference, restoring the original ones afterward.\n",
    "    \"\"\"\n",
    "    @contextmanager\n",
    "    def temporary_classnames(self, new_classnames):\n",
    "        # --- Save original state ---\n",
    "        original_classnames = self.prompt_learner.n_cls\n",
    "        original_tokenized_prompts = self.tokenized_prompts\n",
    "        original_token_prefix = self.prompt_learner.token_prefix\n",
    "        original_token_suffix = self.prompt_learner.token_suffix\n",
    "\n",
    "        # --- Apply temporary state ---\n",
    "        temp_prompt_learner = PromptLearner(\n",
    "            cfg=self.cfg,\n",
    "            classnames=new_classnames,\n",
    "            clip_model=self.clip_model\n",
    "        )\n",
    "\n",
    "        self.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.token_prefix = temp_prompt_learner.token_prefix\n",
    "        self.prompt_learner.token_suffix = temp_prompt_learner.token_suffix\n",
    "        self.prompt_learner.n_cls = len(new_classnames)\n",
    "\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            # --- Restore original state ---\n",
    "            self.tokenized_prompts = original_tokenized_prompts\n",
    "            self.prompt_learner.tokenized_prompts = original_tokenized_prompts\n",
    "            self.prompt_learner.token_prefix = original_token_prefix\n",
    "            self.prompt_learner.token_suffix = original_token_suffix\n",
    "            self.prompt_learner.n_cls = original_classnames\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Performs a forward pass through the model, computing similarity logits between image features and prompt-conditioned text features.\n",
    "    Optionally returns loss, image features, and intermediate representations during training.\n",
    "    \"\"\"\n",
    "    def forward(self, image, label=None, get_image_features=False):\n",
    "        # tokenized_prompts: [num_classes, context_length] (e.g., [10, 77])\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "\n",
    "        # logit_scale: scalar (e.g., initialized as a learnable parameter like torch.tensor(1/0.07).log())\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        # image: [B, 3, H, W]\n",
    "        # image_features: [B, D] where D = transformer width (e.g., 512 for ViT-B/32)\n",
    "        #print(f\"image device: {image.device} | image encoder device: {next(self.image_encoder.parameters()).device}\")\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        if image_features.isnan().any():\n",
    "            raise ValueError(\"NaN detected in image_features.\")\n",
    "        # Normalize image features: each row to unit length\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # prompts: List of [num_classes, context_length, D] (one per image feature)\n",
    "        # Each element is generated conditioned on an image feature\n",
    "        prompts, ctx, bias = self.prompt_learner(image_features) # [B , n_cls, n_ctx, D]\n",
    "        if prompts.isnan().any():\n",
    "            raise ValueError(\"NaN detected in prompts.\")\n",
    "        # prompts: [B, n_cls, n_ctx, D] -> [B * n_cls, n_ctx, D]\n",
    "        logits = []\n",
    "        all_text_features = []\n",
    "        selected_text_features = []\n",
    "        # Iterate over batch\n",
    "        for pts_i, imf_i in zip(prompts, image_features):\n",
    "            # pts_i: [num_classes, context_length, D]\n",
    "            # tokenized_prompts: [num_classes, context_length]\n",
    "            # text_features: [num_classes, D]\n",
    "            text_features = self.text_encoder(pts_i, tokenized_prompts)\n",
    "            if text_features.isnan().any():\n",
    "                raise ValueError(\"NaN detected in text ft.\")\n",
    "            all_text_features.append(text_features)\n",
    "            # Normalize text features\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # imf_i: [D], text_features.T: [D, num_classes]\n",
    "            # l_i: [num_classes], similarity scores between image and all class prompts\n",
    "            l_i = logit_scale * imf_i @ text_features.t()\n",
    "            # Append l_i (1D tensor) to logits list\n",
    "            logits.append(l_i)\n",
    "            best_idx = l_i.argmax()\n",
    "            best_text_feat = text_features[best_idx]  # [D]\n",
    "            selected_text_features.append(best_text_feat)\n",
    "\n",
    "        all_text_features = torch.stack(all_text_features)  # [B, num_classes, D]\n",
    "        #avarage over num_classes\n",
    "        avg_text_features = all_text_features.mean(dim=1)\n",
    "\n",
    "        # Shape: [B, D]\n",
    "        selected_text_features = torch.stack(selected_text_features)\n",
    "        # logits: list of B tensors each of shape [num_classes]\n",
    "        # stacked into a tensor of shape [B, num_classes]\n",
    "        logits = torch.stack(logits)\n",
    "        if logits.isnan().any():\n",
    "            raise ValueError(\"NaN detected in logits\")\n",
    "\n",
    "        # If in training mode, compute and return cross-entropy loss\n",
    "        if self.prompt_learner.training:\n",
    "            # logits: [B, num_classes], label: [B]\n",
    "            if get_image_features:\n",
    "                # If get_image_features is True, return logits and image features\n",
    "                return logits, F.cross_entropy(logits, label), image_features, ctx, bias, avg_text_features, selected_text_features\n",
    "            else:\n",
    "                return logits, F.cross_entropy(logits, label)\n",
    "\n",
    "        # Otherwise, return logits for evaluation: [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "    \"\"\"\n",
    "    Utility function to print the data types of all parameters and buffers in each model component for debugging.\n",
    "    \"\"\"\n",
    "    def print_all_dtypes(self):\n",
    "        print(f\"CustomCLIP dtype: {self.dtype}\")\n",
    "        print(f\"  logit_scale dtype: {getattr(self.logit_scale, 'dtype', type(self.logit_scale))}\")\n",
    "        print(f\"  tokenized_prompts dtype: {getattr(self.tokenized_prompts, 'dtype', type(self.tokenized_prompts))}\")\n",
    "        print(f\"  image_encoder: {type(self.image_encoder)}\")\n",
    "        for name, param in self.image_encoder.named_parameters():\n",
    "            print(f\"    image_encoder param {name}: {param.dtype}\")\n",
    "        for name, buf in self.image_encoder.named_buffers():\n",
    "            print(f\"    image_encoder buffer {name}: {buf.dtype}\")\n",
    "        print(f\"  text_encoder: {type(self.text_encoder)}\")\n",
    "        for name, param in self.text_encoder.named_parameters():\n",
    "            print(f\"    text_encoder param {name}: {param.dtype}\")\n",
    "        for name, buf in self.text_encoder.named_buffers():\n",
    "            print(f\"    text_encoder buffer {name}: {buf.dtype}\")\n",
    "        print(f\"  prompt_learner: {type(self.prompt_learner)}\")\n",
    "        for name, param in self.prompt_learner.named_parameters():\n",
    "            print(f\"    prompt_learner param {name}: {param.dtype}\")\n",
    "        for name, buf in self.prompt_learner.named_buffers():\n",
    "            print(f\"    prompt_learner buffer {name}: {buf.dtype}\")\n",
    "        # Also print dtype for ctx, token_prefix, token_suffix if present\n",
    "        if hasattr(self.prompt_learner, 'ctx'):\n",
    "            print(f\"    prompt_learner.ctx dtype: {self.prompt_learner.ctx.dtype}\")\n",
    "        if hasattr(self.prompt_learner, 'token_prefix'):\n",
    "            print(f\"    prompt_learner.token_prefix dtype: {self.prompt_learner.token_prefix.dtype}\")\n",
    "        if hasattr(self.prompt_learner, 'token_suffix'):\n",
    "            print(f\"    prompt_learner.token_suffix dtype: {self.prompt_learner.token_suffix.dtype}\")\n",
    "        print(f\"  clip_model: {type(self.clip_model)}\")\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "            print(f\"    clip_model param {name}: {param.dtype}\")\n",
    "        for name, buf in self.clip_model.named_buffers():\n",
    "            print(f\"    clip_model buffer {name}: {buf.dtype}\")\n",
    "    \"\"\"\n",
    "    Loads a CustomCLIP instance from a saved checkpoint.\n",
    "    Sets up model configuration, restores state dict, and reassigns classnames.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def load_from_checkpoint(classnames, checkpoint_path, device, n_ctx, clip_model, ctx_8_coop, ctx_4_coop, ctx_init=\"\"):\n",
    "        ctx_load = (\n",
    "            ctx_4_coop\n",
    "            if n_ctx == 4\n",
    "            else ctx_8_coop\n",
    "        )\n",
    "        resolution = clip_model.visual.input_resolution\n",
    "        cfg = EasyDict(\n",
    "            {\n",
    "                \"TRAINER\": {\n",
    "                    \"COCOOP\": {\n",
    "                        \"CTX_LOAD\": ctx_load,\n",
    "                        \"N_CTX\": n_ctx,\n",
    "                        \"CTX_INIT\": ctx_init,\n",
    "                        \"PREC\": \"fp16\",\n",
    "                    }\n",
    "                },\n",
    "                \"INPUT\": {\"SIZE\": [resolution, resolution]},\n",
    "            }\n",
    "        )\n",
    "        state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        n_cls = state_dict[\"prompt_learner.token_prefix\"].shape[0]\n",
    "\n",
    "        clip_model, _ = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "        model = CustomCLIP(cfg, [\"X\"] * n_cls, clip_model)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.change_classnames(classnames)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47e1df4",
   "metadata": {
    "id": "f47e1df4"
   },
   "source": [
    "### Prompt Learner CoCoOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a2e86",
   "metadata": {
    "id": "268a2e86"
   },
   "outputs": [],
   "source": [
    "# From: ./model/cocoop/prompt_learner.py\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "\n",
    "class PromptLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    The PromptLearner dynamically generates context-conditioned prompts for each class using a meta-network and learned context vectors.\n",
    "    This module supports both random and pre-initialized context tokens, and computes prompt embeddings compatible with CLIP.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        \"\"\"\n",
    "        Initialize the PromptLearner with configuration parameters, class names, and a CLIP model.\n",
    "        Loads or initializes context vectors and builds the meta-network for instance-conditioned prompt adaptation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        n_ctx = cfg.TRAINER.COCOOP.N_CTX\n",
    "        ctx_init = cfg.TRAINER.COCOOP.CTX_INIT\n",
    "        dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "        cfg_imsize = cfg.INPUT.SIZE[0]\n",
    "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})\"\n",
    "\n",
    "        if ctx_init:\n",
    "            # use given words to initialize context vectors\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # random initialization\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        # Optional: Load pre-trained ctx from a file\n",
    "        if hasattr(cfg.TRAINER.COCOOP, \"CTX_LOAD\") and cfg.TRAINER.COCOOP.CTX_LOAD:\n",
    "            ctx_path = cfg.TRAINER.COCOOP.CTX_LOAD\n",
    "            if os.path.isfile(ctx_path):\n",
    "                #print(f\"üîÅ Loading ctx from: {ctx_path}\")\n",
    "                state_dict = torch.load(ctx_path, map_location=\"cpu\")\n",
    "                if \"ctx\" in state_dict:\n",
    "                    with torch.no_grad():\n",
    "                        self.ctx.copy_(state_dict[\"ctx\"])\n",
    "                else:\n",
    "                    raise KeyError(f\"'ctx' not found in {ctx_path}\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"CTX_LOAD path not found: {ctx_path}\")\n",
    "            #print(\"PROMPT LEARNER LOADED FROM A COOP PRETRAINED ONE\")\n",
    "\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
    "        ]))\n",
    "\n",
    "        if cfg.TRAINER.COCOOP.PREC == \"fp16\" and not torch.backends.mps.is_available():\n",
    "            self.meta_net.half()\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Using float32 for meta_net due to MPS\")\n",
    "\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])  # (n_cls, n_tkn)\n",
    "        with torch.no_grad():\n",
    "            device = clip_model.token_embedding.weight.device\n",
    "\n",
    "            # Ensure tokenized_prompts is on the right device BEFORE embedding\n",
    "            tokenized_prompts = tokenized_prompts.to(device)\n",
    "\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
    "\n",
    "            # Do not convert to fp16 on MPS (Apple doesn't support it fully)\n",
    "            if device.type == \"mps\" and dtype == torch.float16:\n",
    "                print(\"‚ö†Ô∏è fp16 not fully supported on MPS; using float32 instead\")\n",
    "                dtype = torch.float32\n",
    "\n",
    "            embedding = embedding.to(dtype)\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
    "        self.name_lens = name_lens\n",
    "\n",
    "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
    "        \"\"\"\n",
    "        Construct the full tokenized prompt from context tokens, prefix (SOS), and suffix (CLS, EOS).\n",
    "        Optionally uses label indexing for training-time class selection.\n",
    "        \"\"\"\n",
    "        # dim0 is either batch_size (during training) or n_cls (during testing)\n",
    "        # ctx: context tokens, with shape of (dim0, n_ctx, ctx_dim)\n",
    "        # prefix: the sos token, with shape of (n_cls, 1, ctx_dim)\n",
    "        # suffix: remaining tokens, with shape of (n_cls, *, ctx_dim)\n",
    "\n",
    "        if label is not None:\n",
    "            prefix = prefix[label]\n",
    "            suffix = suffix[label]\n",
    "\n",
    "        prompts = torch.cat(\n",
    "            [\n",
    "                prefix,  # (dim0, 1, dim)\n",
    "                ctx,     # (dim0, n_ctx, dim)\n",
    "                suffix,  # (dim0, *, dim)\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        \"\"\"\n",
    "        Generate the context-conditioned prompts for each class based on input image features.\n",
    "        Applies the meta-network to compute per-instance bias vectors, which shift the context tokens.\n",
    "        Returns the generated prompts along with the original context and computed bias.\n",
    "        \"\"\"\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx                     # (n_ctx, ctx_dim)\n",
    "        if im_features.isnan().any():\n",
    "            raise ValueError(\"NaN in im_features before meta_net\")\n",
    "\n",
    "        #print(\"im_features stats\", im_features.min().item(), im_features.max().item(), im_features.norm(dim=1).mean().item())\n",
    "\n",
    "        meta_net_dtype = next(self.meta_net.parameters()).dtype\n",
    "        # print(f\"meta_net_dtype: {meta_net_dtype}, im_features dtype: {im_features.dtype}\")\n",
    "        bias = self.meta_net(im_features.to(meta_net_dtype))  # (batch, ctx_dim)\n",
    "        if bias.isnan().any():\n",
    "            raise ValueError(\"NaN detected in bias\")\n",
    "        bias = bias.unsqueeze(1)           # (batch, 1, ctx_dim)\n",
    "        ctx = ctx.unsqueeze(0)             # (1, n_ctx, ctx_dim)\n",
    "        ctx_shifted = ctx + bias           # (batch, n_ctx, ctx_dim)\n",
    "        if ctx_shifted.isnan().any():\n",
    "            raise ValueError(\"NaN detected in ctx_shifted\")\n",
    "        # Use instance-conditioned context tokens for all classes\n",
    "        prompts = []\n",
    "        for ctx_shifted_i in ctx_shifted:\n",
    "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1) # (n_cls, n_ctx, ctx_dim)\n",
    "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tkn, ctx_dim)\n",
    "            if pts_i.isnan().any():\n",
    "                raise ValueError(\"NaN detected in pts_i\")\n",
    "            prompts.append(pts_i)\n",
    "        prompts = torch.stack(prompts)\n",
    "\n",
    "        return prompts, ctx, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0308b",
   "metadata": {
    "id": "93b0308b"
   },
   "source": [
    "## Training System CoOpOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88e019",
   "metadata": {
    "id": "ac88e019"
   },
   "outputs": [],
   "source": [
    "# From: ./training_systems/cocoop.py\n",
    "\"\"\"\n",
    "Main module for training the CoCoOp system, supporting both base and adversarial training phases.\n",
    "Includes configuration, data loading, model preparation, and training logic for zero-shot learning with CLIP.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from statistics import harmonic_mean\n",
    "from sympy.simplify.cse_main import preprocess_for_cse\n",
    "import torch\n",
    "from easydict import EasyDict\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import clip\n",
    "import numpy as np\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "# from model.cocoop.custom_clip import CustomCLIP\n",
    "# from model.cocoop.mlp_adversary import GradientReversalLayer, AdversarialMLP\n",
    "# from utils import (\n",
    "#     conditional_clustering,\n",
    "#     random_clustering,\n",
    "#     rotating_cluster_generator_shift,\n",
    "#     get_data,\n",
    "#     base_novel_categories,\n",
    "#     split_data,\n",
    "#     TensorboardLogger,\n",
    "#     CLASS_NAMES\n",
    "# )\n",
    "\n",
    "# from training_systems.training_methods import (\n",
    "#     Adversarial,\n",
    "#     KLCoCoOp,\n",
    "#     BaseCoCoOp,\n",
    "#     KLCoCoOpV2,\n",
    "# )\n",
    "# from training_systems.evaluation_methods import (\n",
    "#     ZeroShotTestStep,\n",
    "#     FineTunedTestStep,\n",
    "#     EvalStep,\n",
    "# )\n",
    "# from training_systems.core import DoubleDatasetTrainingMethod\n",
    "\n",
    "# --- Add this block for reproducibility ---\n",
    "def set_global_seed(seed):\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # For CUDA >= 10.2, for full determinism\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    # For PyTorch >= 1.8\n",
    "    if hasattr(torch, 'use_deterministic_algorithms'):\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "# --- End reproducibility block ---\n",
    "\n",
    "\n",
    "def checksum(model):\n",
    "    \"\"\"\n",
    "    Generate an MD5 checksum of the model's parameters to track changes across training.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to hash.\n",
    "\n",
    "    Returns:\n",
    "        str: MD5 hash string.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        all_params = torch.cat(\n",
    "            [p.view(-1).cpu() for p in model.parameters() if p.requires_grad]\n",
    "        )\n",
    "        return hashlib.md5(all_params.numpy().tobytes()).hexdigest()\n",
    "\n",
    "\n",
    "class CoCoOpSystem:\n",
    "    \"\"\"\n",
    "    Manages the full training process of the CoCoOp model, including configuration, training, evaluation,\n",
    "    checkpointing, and logging. Supports both base and adversarial training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        test_batch_size=16,\n",
    "        pseudo_base_ratio=0.7,\n",
    "        seed=42,\n",
    "        device=\"cuda\",\n",
    "        run_name=\"exp1\",\n",
    "        cnn_model=\"ViT-B/16\",\n",
    "        hparams_file,\n",
    "        optimizer_configs=None,\n",
    "        skip_tests=None,\n",
    "        train_base_checkpoint_path=None,\n",
    "        debug=False,\n",
    "        prompt_learner_opt=None,\n",
    "        kl_loss_opt=None,\n",
    "        adv_training_opt=None,\n",
    "        base_training_opt=None,\n",
    "        clustering_opt=None,\n",
    "        report=False,\n",
    "        pat=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CoCoOp system, load data, setup the model, loss functions, optimizers, and logger.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Batch size for training.\n",
    "            device (str): Device identifier (e.g., 'cuda' or 'cpu').\n",
    "            run_name (str): Unique name for the training run.\n",
    "            cnn_model (str): Backbone CLIP model name.\n",
    "            optimizer_configs (list): Optimizer settings for base and adversarial training.\n",
    "            skip_tests (list): Booleans to skip testing after each training stage.\n",
    "            train_base_checkpoint_path (str): Optional path to a pre-trained base model.\n",
    "            debug (bool): Enables logging of additional debug information.\n",
    "            prompt_learner_opt, kl_loss_opt, adv_training_opt, base_training_opt: Configuration dictionaries.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"run_name: {run_name}, using {cnn_model}, pat: {pat}\")\n",
    "        # --- Set global seed for reproducibility ---\n",
    "        self.seed = seed if seed is not None else 42\n",
    "        # set_global_seed(self.seed)\n",
    "        # --- End reproducibility ---\n",
    "        assert prompt_learner_opt is not None, \"prompt_learner_opt must be provided\"\n",
    "        assert kl_loss_opt is not None, \"kl_loss_opt must be provided\"\n",
    "        assert adv_training_opt is not None, \"adv_training_opt must be provided\"\n",
    "        assert base_training_opt is not None, \"base_training_opt must be provided\"\n",
    "        assert clustering_opt is not None, \"clustering_opt must be provided\"\n",
    "        assert (\n",
    "            optimizer_configs is not None and len(optimizer_configs) == 2\n",
    "        ), \"Two optimizer configs must be provided\"\n",
    "\n",
    "        # --- NEW: Pseudo-base/novel split param ---\n",
    "        self.pseudo_base_ratio = pseudo_base_ratio\n",
    "        self.pseudo_split_seed = seed\n",
    "\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.device = device\n",
    "        self.epochs = base_training_opt[\"epochs\"]\n",
    "        self.run_name = run_name\n",
    "        self.n_ctx = prompt_learner_opt[\"n_ctx\"]\n",
    "        self.ctx_init = prompt_learner_opt[\"ctx_init\"]\n",
    "        self.class_token_position = prompt_learner_opt[\"class_token_position\"]\n",
    "        self.csc = prompt_learner_opt[\"csc\"]\n",
    "        self.lambda_kl = kl_loss_opt[\"lambda_kl\"]\n",
    "        self.double_datasets_kl = kl_loss_opt.get(\"double_datasets_kl\", False)\n",
    "        self.rotation_period = kl_loss_opt.get(\"rotation_period\", \"relative\")\n",
    "        self.warmup_lambda_kl = kl_loss_opt.get(\"warmup_lambda_kl\", 0)  # Add warmup parameter\n",
    "        self.lambda_adv = adv_training_opt[\"lambda_adv\"]\n",
    "        self.gaussian_noise = adv_training_opt.get(\"gaussian_noise\", 0.0)\n",
    "        self.use_bias_ctx = adv_training_opt.get(\"use_bias_ctx\", False)\n",
    "        self.adv_training_epochs = adv_training_opt[\"adv_training_epochs\"]\n",
    "        self.cnn_model = cnn_model\n",
    "        self.warmup_epoch = base_training_opt[\"warmup_epoch\"]\n",
    "        self.warmup_cons_lr = base_training_opt[\"warmup_cons_lr\"]\n",
    "        self.using_kl = kl_loss_opt[\"using_kl\"]\n",
    "        self.grl_lambda = adv_training_opt[\"grl_lambda\"]\n",
    "        self.mlp_opt = EasyDict(adv_training_opt[\"mlp_opt\"])\n",
    "        self.skip_tests = (\n",
    "            skip_tests if skip_tests is not None else [False, False, False]\n",
    "        )\n",
    "        self.train_base_checkpoint_path = train_base_checkpoint_path\n",
    "        self.debug = debug\n",
    "        self.max_epoch = self.epochs\n",
    "        self.optimizer_configs = [EasyDict(conf) for conf in optimizer_configs]\n",
    "        self.warmup_lambda_adv = adv_training_opt[\"warmup_lambda_adv\"]\n",
    "        self.base_batch_size = base_training_opt[\"batch_size\"]\n",
    "        self.adv_batch_size = adv_training_opt[\"batch_size\"]\n",
    "        self.adv_accumulation_steps = adv_training_opt.get(\"accumulation_steps\", 1)\n",
    "        self.base_accumulation_steps = base_training_opt.get(\"accumulation_steps\", 1)\n",
    "        self.prompt_learner_warmup_epochs = adv_training_opt[\"prompt_learner_warmup_epochs\"] if \"prompt_learner_warmup_epochs\" in adv_training_opt else 0\n",
    "        self.pat = pat\n",
    "        print(\n",
    "            \"BATCH SIZES: \",\n",
    "            self.test_batch_size,\n",
    "            self.base_batch_size,\n",
    "            self.adv_batch_size,\n",
    "        )\n",
    "\n",
    "        self.ignore_no_improvement = adv_training_opt.get(\"ignore_no_improvement\", False)\n",
    "        if not report:\n",
    "            self.log_dir = f\"runs/CoCoOp/{self.run_name}\"\n",
    "        elif self.pat:\n",
    "            self.log_dir = f\"runs/report/{self.run_name}\"\n",
    "        else:\n",
    "            self.log_dir = f\"runs/report_no_pat/{self.run_name}\"\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "        self.writer.add_text(\"Hparams yaml file\", hparams_file)\n",
    "        self.logger = TensorboardLogger(self.writer)\n",
    "\n",
    "        self.logger.log_hparams(\n",
    "            {\n",
    "                \"batch_size_test\": self.test_batch_size,\n",
    "                \"base_batch_size\": self.base_batch_size,\n",
    "                \"adv_batch_size\": self.adv_batch_size,\n",
    "                \"epochs\": self.epochs,\n",
    "                \"n_ctx\": self.n_ctx,\n",
    "                \"ctx_init\": self.ctx_init,\n",
    "                \"class_token_position\": self.class_token_position,\n",
    "                \"csc\": self.csc,\n",
    "                \"lambda_kl_first\": self.lambda_kl[0],\n",
    "                \"lambda_kl_second\": self.lambda_kl[1],\n",
    "                \"warmup_epoch\": self.warmup_epoch,\n",
    "                \"warmup_cons_lr\": self.warmup_cons_lr,\n",
    "                \"lambda_adv\": self.lambda_adv,\n",
    "                \"cnn_model\": self.cnn_model,\n",
    "                \"grl_lambda\": self.grl_lambda,\n",
    "                \"prompt_learner_warmup_epochs\" : self.prompt_learner_warmup_epochs,\n",
    "                \"double_datasets_kl\": self.double_datasets_kl,\n",
    "                \"pseudo_base_ratio\": self.pseudo_base_ratio,\n",
    "                \"pseudo_split_seed\": self.seed,\n",
    "                \"rotation_period\": self.rotation_period,\n",
    "                \"warmup_lambda_kl\": self.warmup_lambda_kl,\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "        print(f\"patience {'disabled' if not self.pat else 'enabled'}\")\n",
    "        # Load model\n",
    "        self.clip_model, preprocess= clip.load(self.cnn_model)\n",
    "        self.clip_model = self.clip_model.to(self.device)\n",
    "        resolution = self.clip_model.visual.input_resolution\n",
    "        self.train_set, self.val_set, self.test_set = get_data(resolution=resolution, eval_transform=preprocess)\n",
    "        self.base_classes, self.novel_classes = base_novel_categories(self.train_set)\n",
    "        # --- NEW: Pseudo-base/novel split ---\n",
    "\n",
    "        # Helper to split a dataset by class list\n",
    "        # (moved to method below)\n",
    "\n",
    "        # Split train_base/val_base into pseudo_base/pseudo_novel\n",
    "        self.train_base, _ = split_data(self.train_set, self.base_classes)\n",
    "        self.val_base, self.val_novel = split_data(self.val_set, self.base_classes)\n",
    "        self.test_base, self.test_novel = split_data(self.test_set, self.base_classes)\n",
    "        print(f\"Base classes length: {len(self.base_classes)}, Novel classes length: {len(self.novel_classes)}\")\n",
    "        print(f\"Train base length: {len(self.train_base)}, Val base length: {len(self.val_base)}, Test base length: {len(self.test_base)}\")\n",
    "        print(f\"Val novel length: {len(self.val_novel)}, Test novel length: {len(self.test_novel)}\")\n",
    "\n",
    "\n",
    "        self.rotation_steps = int(len(self.base_classes)*(1-self.pseudo_base_ratio))\n",
    "\n",
    "        self.cluster_generator = rotating_cluster_generator_shift(\n",
    "            self.base_classes,\n",
    "            self.pseudo_base_ratio,\n",
    "            steps=self.rotation_steps,\n",
    "            seed=self.seed\n",
    "        )\n",
    "\n",
    "        _, self.pseudo_base_classes, self.pseudo_novel_classes = next(self.cluster_generator)\n",
    "\n",
    "        self.train_pseudo_base = self.split_by_classes(self.train_base, self.pseudo_base_classes)\n",
    "        self.train_pseudo_novel = self.split_by_classes(self.train_base, self.pseudo_novel_classes)\n",
    "        self.val_pseudo_base = self.split_by_classes(self.val_base, self.pseudo_base_classes)\n",
    "        self.val_pseudo_novel = self.split_by_classes(self.val_base, self.pseudo_novel_classes)\n",
    "\n",
    "        # --- Model/classnames: only pseudo_base for first phase ---\n",
    "        ctx_load = (\n",
    "            \"./bin/coop/coop_ctx_4_VIT16.pth\"\n",
    "            if self.n_ctx == 4\n",
    "            else \"./bin/coop/coop_ctx_8_VIT16.pth\"\n",
    "        )\n",
    "        cfg = EasyDict(\n",
    "            {\n",
    "                \"TRAINER\": {\n",
    "                    \"COCOOP\": {\n",
    "                        \"CTX_LOAD\": ctx_load,\n",
    "                        \"N_CTX\": self.n_ctx,\n",
    "                        \"CTX_INIT\": self.ctx_init,\n",
    "                        \"PREC\": \"fp16\",\n",
    "                    }\n",
    "                },\n",
    "                \"INPUT\": {\"SIZE\": [resolution, resolution]},\n",
    "            }\n",
    "        )\n",
    "        self.model = CustomCLIP(\n",
    "\n",
    "            classnames=[CLASS_NAMES[idx] for idx in self.pseudo_base_classes],\n",
    "            cfg=cfg,\n",
    "            clip_model=self.clip_model,\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Print all dtypes of every component inside CustomCLIP\n",
    "        # self.model.print_all_dtypes()\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"prompt_learner\" not in name:\n",
    "                param.requires_grad_(False)\n",
    "            else:\n",
    "                param.requires_grad_(True)\n",
    "\n",
    "        self.cost_function = nn.CrossEntropyLoss()\n",
    "        self.grl = GradientReversalLayer(lambda_=self.grl_lambda)\n",
    "\n",
    "        clip_dim = self.clip_model.visual.output_dim\n",
    "\n",
    "        self.mlp_adversary = AdversarialMLP(\n",
    "            input_dim=clip_dim+len(self.base_classes), opt=self.mlp_opt, output_dim=clustering_opt[\"n_clusters\"], use_bias_ctx=self.use_bias_ctx, n_ctx=self.n_ctx\n",
    "        ).to(self.device)\n",
    "\n",
    "        print(\"mlp adversary struct: \", self.mlp_adversary)\n",
    "        self.optimizer = self.get_optimizer(self.model, None, self.optimizer_configs[0])\n",
    "        self.lr_scheduler = LambdaLR(self.optimizer, self._lr_lambda)\n",
    "\n",
    "        # --- NEW: Cluster dict for adversarial phase ---\n",
    "\n",
    "        clustering_type = clustering_opt[\"clustering_type\"]\n",
    "\n",
    "        if clustering_type == \"random\":\n",
    "            # Use random clustering\n",
    "            self.cls_cluster_dict, _ = random_clustering(\n",
    "                n_cluster=clustering_opt[\"n_clusters\"],\n",
    "                seed=self.seed,\n",
    "                distribution=\"uniform\",\n",
    "            )\n",
    "        elif clustering_type == \"semantic\":\n",
    "            # Load clustering information\n",
    "            self.cls_cluster_dict, _ = conditional_clustering(\n",
    "                n_cluster=clustering_opt[\"n_clusters\"],\n",
    "                variance=clustering_opt[\"variance\"],\n",
    "                cnn=clustering_opt[\"vision_encoder\"],\n",
    "                device=self.device,\n",
    "            )\n",
    "        elif clustering_type == \"default\":\n",
    "            # Pseudo_base: cluster 0, pseudo_novel: cluster 1\n",
    "            self.pseudo_cls_cluster_dict = {c: 0 for c in self.pseudo_base_classes}\n",
    "            self.pseudo_cls_cluster_dict.update({c: 1 for c in self.pseudo_novel_classes})\n",
    "            # For adversarial phase, use this dict\n",
    "            self.cls_cluster_dict = self.pseudo_cls_cluster_dict\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown clustering type: {clustering_type}\")\n",
    "\n",
    "    def _set_test_methods(self):\n",
    "        \"\"\"\n",
    "        Initializes the zero-shot and fine-tuned test step evaluators for both base and novel class splits.\n",
    "        \"\"\"\n",
    "        self.zero_shot_base_classes_test_method = ZeroShotTestStep(\n",
    "            model=self.clip_model,\n",
    "            batch_size=self.test_batch_size,\n",
    "            categories=self.base_classes,\n",
    "        )\n",
    "        self.zero_shot_novel_classes_test_method = ZeroShotTestStep(\n",
    "            model=self.clip_model,\n",
    "            batch_size=self.test_batch_size,\n",
    "            categories=self.novel_classes,\n",
    "        )\n",
    "        self.zero_shot_pseudo_base_test_method = ZeroShotTestStep(\n",
    "            model=self.clip_model,\n",
    "            batch_size=self.test_batch_size,\n",
    "            categories=self.pseudo_base_classes,\n",
    "        )\n",
    "        self.zero_shot_pseudo_novel_test_method = ZeroShotTestStep(\n",
    "            model=self.clip_model,\n",
    "            batch_size=self.test_batch_size,\n",
    "            categories=self.pseudo_novel_classes,\n",
    "        )\n",
    "        self.finetuned_test_method = FineTunedTestStep(\n",
    "            model=self.model,\n",
    "            batch_size=self.test_batch_size,\n",
    "        )\n",
    "\n",
    "    def _set_eval_method(self):\n",
    "        \"\"\"\n",
    "        Initializes the evaluation method used during validation.\n",
    "        \"\"\"\n",
    "        self.eval_method = EvalStep(\n",
    "            model=self.model,\n",
    "            batch_size=self.test_batch_size,\n",
    "        )\n",
    "\n",
    "    def _set_train_methods(self):\n",
    "        \"\"\"\n",
    "        Initializes the training method used for both base and adversarial phases, depending on whether KL loss is enabled.\n",
    "        Chooses between standard and KL-regularized training methods.\n",
    "        \"\"\"\n",
    "        self.adversarial_method = Adversarial(\n",
    "                lambda_adv=0.05,\n",
    "                model=self.model,\n",
    "                optimizer=self.optimizer,\n",
    "                cls_cluster_dict=self.cls_cluster_dict,\n",
    "                grl=self.grl,\n",
    "                mlp_adversary=self.mlp_adversary,\n",
    "                debug=self.debug,\n",
    "                tmp_classes=self.base_classes,\n",
    "                gaussian_noise=self.gaussian_noise,\n",
    "                use_bias_ctx=self.use_bias_ctx\n",
    "            )\n",
    "\n",
    "        if self.using_kl[0]:\n",
    "            if self.double_datasets_kl:\n",
    "                self.basic_train_method = KLCoCoOpV2(\n",
    "                    model=self.model,\n",
    "                    optimizer=self.optimizer,\n",
    "                    debug=self.debug,\n",
    "                    lambda_kl=self.lambda_kl[0],\n",
    "                )\n",
    "            else:\n",
    "                self.basic_train_method = KLCoCoOp(\n",
    "                    model=self.model,\n",
    "                    optimizer=self.optimizer,\n",
    "                    debug=self.debug,\n",
    "                    lambda_kl=self.lambda_kl[0],\n",
    "                )\n",
    "        else:\n",
    "            self.basic_train_method = BaseCoCoOp(\n",
    "                model=self.model,\n",
    "                optimizer=self.optimizer,\n",
    "                debug=self.debug,\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Execute the full training pipeline: base phase, optionally followed by adversarial training and evaluation.\n",
    "        \"\"\"\n",
    "        self._set_test_methods()\n",
    "        self._set_eval_method()\n",
    "        self._set_train_methods()\n",
    "        if not self.skip_tests[0]:\n",
    "            print(\"Doing base accuracy test\")\n",
    "            base_acc, novel_acc = self.compute_evaluation(-1, base=True)\n",
    "            self._log_final_metrics(\n",
    "                \"Final metrics - CLIP ZERO SHOT\",\n",
    "                base_acc,\n",
    "                novel_acc,\n",
    "                -1,\n",
    "            )\n",
    "\n",
    "        best_model_path = os.path.join(self.log_dir, \"best_model.pth\")\n",
    "        # Ensure all methods are properly initialized prior to the base training phase\n",
    "\n",
    "        if self.train_base_checkpoint_path is None:\n",
    "            # Base training phase\n",
    "            base_end_epoch, _, best_base_epoch = self._train_base_phase(best_model_path)\n",
    "            self.best_base_epoch = best_base_epoch  # Store for later use if needed\n",
    "            # Log the best epoch to TensorBoard and logger\n",
    "            self.writer.add_scalar(\"Best Base Epoch\", best_base_epoch, base_end_epoch)\n",
    "\n",
    "            # Also print for visibility\n",
    "            print(f\"Best base model found at epoch: {best_base_epoch}\")\n",
    "            if self.epochs != 0:\n",
    "                if self.pat:\n",
    "                    print(f\"[DEBUG] Loading best model state dict after base phase from: {best_model_path}\")\n",
    "                    self.model.load_state_dict(torch.load(best_model_path))\n",
    "                    print(f\"[DEBUG] Loaded best model with classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "                else:\n",
    "                    print(f\"[DEBUG] Using last model state from base phase (patience disabled)\")\n",
    "                    print(f\"[DEBUG] Model has classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "                self.save_model(path=\"./bin/cocoop\", prefix=\"after_first_train_\")\n",
    "\n",
    "            if not self.skip_tests[1]:\n",
    "                print(\"Doing base accuracy test\")\n",
    "                base_acc, novel_acc = self.compute_evaluation(base_end_epoch)\n",
    "                self._log_final_metrics(\n",
    "                    \"Final metrics - After Base Training\",\n",
    "                    base_acc,\n",
    "                    novel_acc,\n",
    "                    base_end_epoch,\n",
    "                )\n",
    "        else:\n",
    "            base_end_epoch = 0\n",
    "            print(\"Skipping base training\")\n",
    "            print(f\"[DEBUG] Loading model state dict from: {self.train_base_checkpoint_path}\")\n",
    "            with self.model.temporary_classnames([CLASS_NAMES[c] for c in self.base_classes]):\n",
    "                self.model.load_state_dict(torch.load(self.train_base_checkpoint_path))\n",
    "            print(f\"[DEBUG] Loaded model with classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "\n",
    "        # Re-initialize test/eval/train methods after loading/training model and before adversarial phase\n",
    "        self._set_eval_method()\n",
    "        self._set_test_methods()\n",
    "\n",
    "        self.optimizer = self.get_optimizer(\n",
    "            self.model, self.mlp_adversary, self.optimizer_configs[1]\n",
    "        )\n",
    "        # After changing optimizer, ensure train methods use the new optimizer\n",
    "        self._set_train_methods()\n",
    "\n",
    "        checksum1 = None\n",
    "        if self.debug:\n",
    "            checksum1 = checksum(self.model)\n",
    "            # Adversarial phase\n",
    "            print(\"Before adv training:\", checksum1)\n",
    "\n",
    "        adv_end_epoch = self._train_adversarial_phase(base_end_epoch, best_model_path)\n",
    "\n",
    "        if self.debug and checksum1:\n",
    "            checksum2 = checksum(self.model)\n",
    "            print(\"After adv training:\", checksum2)\n",
    "            print(f\"checksum1: {checksum1}, checksum2: {checksum2}\")\n",
    "            if checksum1 != checksum2:\n",
    "                print(\"Model parameters have changed after adversarial training.\")\n",
    "\n",
    "        if not self.skip_tests[2]:\n",
    "            print(\"Doing post-adv. accuracy test\")\n",
    "            base_acc, novel_acc = self.compute_evaluation(adv_end_epoch)\n",
    "            self._log_final_metrics(\n",
    "                \"Final metrics - After Adversarial Training\",\n",
    "                base_acc,\n",
    "                novel_acc,\n",
    "                adv_end_epoch,\n",
    "            )\n",
    "\n",
    "        self.logger.close()\n",
    "        if self.adv_training_epochs != 0:\n",
    "            self.save_model(path=\"./bin/cocoop\", prefix=\"after_adv_train_\")\n",
    "            self.save_mlp_adversary()\n",
    "\n",
    "    def get_next_rotation(self):\n",
    "        \"\"\"\n",
    "        Get the next rotation of the pseudo base and pseudo novel classes.\n",
    "        \"\"\"\n",
    "        _, self.pseudo_base_classes, self.pseudo_novel_classes = next(self.cluster_generator)\n",
    "        self.train_pseudo_base = self.split_by_classes(self.train_base, self.pseudo_base_classes)\n",
    "        self.train_pseudo_novel = self.split_by_classes(self.train_base, self.pseudo_novel_classes)\n",
    "        self.val_pseudo_base = self.split_by_classes(self.val_base, self.pseudo_base_classes)\n",
    "        self.val_pseudo_novel = self.split_by_classes(self.val_base, self.pseudo_novel_classes)\n",
    "        return self.pseudo_base_classes, self.pseudo_novel_classes, self.train_pseudo_base, self.train_pseudo_novel, self.val_pseudo_base, self.val_pseudo_novel\n",
    "\n",
    "    def _train_base_phase(self, best_model_path):\n",
    "        \"\"\"\n",
    "        Train the model using KL regularization only (no adversarial objective).\n",
    "\n",
    "        Args:\n",
    "            best_model_path (str): Path to store the best base model.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, float, int]: Final epoch index, best validation score, and best epoch index.\n",
    "        \"\"\"\n",
    "        best_score = 0.0\n",
    "        patience = 8\n",
    "        patience_counter = 0\n",
    "        c = 0\n",
    "        method = self.basic_train_method\n",
    "        evaluation_period = 1 if self.pat else 2\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize rotation tracking\n",
    "        if self.rotation_period == \"random\":\n",
    "            # For random rotation, we'll track when the next rotation should happen\n",
    "            next_rotation_epoch = random.randint(1, 4)  # Sample from 1 to 4\n",
    "            rotation_epochs = None  # Not used for random\n",
    "        else:\n",
    "            rotation_epochs = int(patience * (3/4)) if self.rotation_period == \"relative\" else self.rotation_period\n",
    "            next_rotation_epoch = None  # Not used for fixed/relative\n",
    "\n",
    "        pbar = tqdm(total=self.max_epoch, desc=\"Base Training\")\n",
    "        best_epoch = -1  # Track the epoch of the best model\n",
    "        best_epoch_path = best_model_path + \".best_epoch.txt\"  # Path to store best epoch\n",
    "\n",
    "        for e in range(self.max_epoch):\n",
    "\n",
    "            # Apply lambda_kl warmup if enabled\n",
    "            if self.warmup_lambda_kl > 0 and (isinstance(method, KLCoCoOp) or isinstance(method, KLCoCoOpV2)):\n",
    "                progress = (e + 1) / self.warmup_lambda_kl\n",
    "                current_lambda_kl = 0.1 + (self.lambda_kl[0] - 0.1) * min(progress, 1)\n",
    "                # Only update lambda_kl for methods that support it (KLCoCoOp and KLCoCoOpV2)\n",
    "                method.update_lambda_kl(current_lambda_kl)\n",
    "\n",
    "            if self.using_kl[0]:\n",
    "                if isinstance(method, DoubleDatasetTrainingMethod):\n",
    "                    # Check if rotation should happen\n",
    "                    should_rotate = False\n",
    "                    if self.rotation_period == \"random\":\n",
    "                        if e == next_rotation_epoch:\n",
    "                            should_rotate = True\n",
    "                            # Sample next rotation epoch (1 to 4 epochs from now)\n",
    "                            next_rotation_epoch = e + random.randint(1, 4)\n",
    "                    else:\n",
    "                        # Fixed or relative rotation\n",
    "                        if rotation_epochs is not None and e % rotation_epochs == 0:\n",
    "                            should_rotate = True\n",
    "\n",
    "                    if should_rotate:\n",
    "                        if self.rotation_period == \"random\":\n",
    "                            print(f\"[DEBUG] Random rotation at epoch {e}, next rotation at epoch {next_rotation_epoch}\")\n",
    "                        (\n",
    "                            self.pseudo_base_classes,\n",
    "                            self.pseudo_novel_classes,\n",
    "                            self.train_pseudo_base,\n",
    "                            self.train_pseudo_novel,\n",
    "                            self.val_pseudo_base,\n",
    "                            self.val_pseudo_novel\n",
    "                        ) = self.get_next_rotation()\n",
    "                    kl_loss, ce_loss, acc = method.double_datasets_train_step(\n",
    "                        self.train_pseudo_base,\n",
    "                        self.train_pseudo_novel,\n",
    "                        self.base_batch_size,\n",
    "                        [\"pseudo_base\", \"pseudo_novel KL\"],\n",
    "                        self.pseudo_base_classes,\n",
    "                        self.pseudo_novel_classes,\n",
    "                    )\n",
    "                    total_loss = ce_loss + kl_loss\n",
    "                else:\n",
    "                    total_loss, acc, ce_loss, kl_loss = method.train_step(\n",
    "                        self.train_pseudo_base,\n",
    "                        self.base_batch_size,\n",
    "                        classnames=self.pseudo_base_classes,\n",
    "                        accumulation_steps=self.base_accumulation_steps\n",
    "                    )\n",
    "            elif isinstance(method, BaseCoCoOp):\n",
    "                total_loss, acc = method.train_step(\n",
    "                    self.train_pseudo_base,\n",
    "                    self.base_batch_size,\n",
    "                    classnames=self.pseudo_base_classes,\n",
    "                    accumulation_steps=self.base_accumulation_steps\n",
    "                )\n",
    "                kl_loss = None\n",
    "                ce_loss = total_loss\n",
    "\n",
    "            self.logger.log_training_base(\n",
    "                e,\n",
    "                self.optimizer.param_groups[0][\"lr\"],\n",
    "                ce_loss,\n",
    "                acc,\n",
    "                kl_loss,\n",
    "                total_loss,\n",
    "            )\n",
    "            postfix_dict = {\n",
    "                'lr': self.optimizer.param_groups[0][\"lr\"],\n",
    "                'ce_L': ce_loss,\n",
    "                'kl_L': kl_loss,\n",
    "                'pat_c': patience_counter,\n",
    "            }\n",
    "            if e % evaluation_period == 0:\n",
    "                base_val_acc, novel_val_acc = self._evaluate_and_log(e)\n",
    "\n",
    "                score = novel_val_acc\n",
    "\n",
    "                if (score > best_score) or (not self.pat):\n",
    "                    best_score = score\n",
    "                    patience_counter = 0\n",
    "                    torch.save(self.model.state_dict(), best_model_path)\n",
    "                    best_epoch = e  # Save the epoch of the best model\n",
    "                    # Store the best epoch to disk\n",
    "                    with open(best_epoch_path, \"w\") as f:\n",
    "                        f.write(str(best_epoch))\n",
    "                    # When pat=False, we always save the current model state (last epoch)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {e}\")\n",
    "                        break\n",
    "\n",
    "                postfix_dict[\"B_val_acc\"] = base_val_acc\n",
    "                postfix_dict[\"N_val_acc\"] = novel_val_acc\n",
    "                postfix_dict[\"score\"] = score\n",
    "\n",
    "            self.lr_scheduler.step()\n",
    "            pbar.set_postfix(**postfix_dict)\n",
    "            pbar.update(1)\n",
    "            c += 1\n",
    "\n",
    "        return c, best_score, best_epoch\n",
    "\n",
    "    def _train_adversarial_phase(self, start_epoch, best_model_path):\n",
    "        \"\"\"\n",
    "        Train the model adversarially with dynamic lambda scheduling and early stopping.\n",
    "\n",
    "        Args:\n",
    "            start_epoch (int): Starting epoch index.\n",
    "            best_model_path (str): Path to save the best adversarial model.\n",
    "\n",
    "        Returns:\n",
    "            int: Final epoch index after training.\n",
    "        \"\"\"\n",
    "        best_novel_accuracy = 0.0\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "        at_least_one_improving = False\n",
    "        warmup_epochs = self.warmup_lambda_adv\n",
    "        lambda_adv_max = self.lambda_adv\n",
    "        initial_lambda_adv = 0.05\n",
    "        pbar = tqdm(total=self.adv_training_epochs, desc=\"Adversarial Training\")\n",
    "\n",
    "        last_model_state = None  # store last model state\n",
    "\n",
    "        method = self.adversarial_method\n",
    "\n",
    "        for e in range(start_epoch, start_epoch + self.adv_training_epochs):\n",
    "            progress = (e - start_epoch + 1) / warmup_epochs\n",
    "            new_lambda_adv = initial_lambda_adv + (\n",
    "                lambda_adv_max - initial_lambda_adv\n",
    "            ) * min(progress, 1)\n",
    "\n",
    "            method.update_lambda_adv(new_lambda_adv)\n",
    "\n",
    "            if (e-start_epoch) < self.prompt_learner_warmup_epochs:\n",
    "                print(f\"[DEBUG] Prompt learner FROZEN at adv epoch {e-start_epoch}\")\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if \"prompt_learner\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "            elif (e-start_epoch) == self.prompt_learner_warmup_epochs:\n",
    "                print(f\"prompt learner UNFROZEN at adv epoch {e-start_epoch}\")\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if \"prompt_learner\" in name:\n",
    "                        param.requires_grad_(True)\n",
    "                # print the frozen parameters name\n",
    "\n",
    "\n",
    "            if self.using_kl[1]:\n",
    "                total_loss, acc, ce_loss, kl_loss, adv_loss = method.train_step(\n",
    "                    self.train_base,\n",
    "                    self.base_batch_size,\n",
    "                    classnames=self.base_classes,\n",
    "                    accumulation_steps=self.base_accumulation_steps\n",
    "                )\n",
    "            else:\n",
    "                total_loss, acc, ce_loss, adv_loss = method.train_step(\n",
    "                    self.train_base,\n",
    "                    self.adv_batch_size,\n",
    "                    classnames=self.base_classes,\n",
    "                    accumulation_steps=self.adv_accumulation_steps\n",
    "                )\n",
    "                kl_loss = None\n",
    "\n",
    "            self.logger.log_training_adv(\n",
    "                e,\n",
    "                method.lambda_adv,\n",
    "                ce_loss,\n",
    "                acc,\n",
    "                adv_loss,\n",
    "                ce_loss + adv_loss + (kl_loss if kl_loss else 0.0),\n",
    "                kl_loss=kl_loss,\n",
    "            )\n",
    "            if (e-start_epoch) >= self.prompt_learner_warmup_epochs:\n",
    "\n",
    "                # Always update last_model_state to track the current state\n",
    "                last_model_state = deepcopy(self.model.state_dict())\n",
    "\n",
    "                base_val_acc, novel_val_acc = self._evaluate_and_log(\n",
    "                    e,\n",
    "                    is_adv=True,\n",
    "                )\n",
    "                if novel_val_acc > best_novel_accuracy or self.ignore_no_improvement or (not self.pat):\n",
    "                    best_novel_accuracy = novel_val_acc\n",
    "                    print(f\"[DEBUG] Saving model with classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "                    torch.save(self.model.state_dict(), best_model_path)\n",
    "                    at_least_one_improving = True\n",
    "                    patience_counter = 0\n",
    "                    # When pat=False, we always save the current model state (last epoch)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping adversarial at epoch {e}\")\n",
    "                        break\n",
    "                pbar.set_postfix(\n",
    "                    PB_val_acc=base_val_acc,\n",
    "                    PN_val_acc=novel_val_acc,\n",
    "                    ce_L=ce_loss,\n",
    "                    kl_L=kl_loss,\n",
    "                    adv_L=adv_loss,\n",
    "                    lr=self.optimizer.param_groups[0][\"lr\"],\n",
    "                    pat_c=patience_counter,\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                pbar.set_postfix(\n",
    "                    adv_loss=adv_loss,\n",
    "                )\n",
    "            pbar.update(1)\n",
    "\n",
    "        # When pat=False, we want to keep the last model state (no loading from checkpoint)\n",
    "        # When pat=True, we load the best model if there was improvement\n",
    "        if self.pat and ((at_least_one_improving and self.epochs != 0) or self.ignore_no_improvement):\n",
    "            print(f\"[DEBUG] Loading best model state dict after adversarial phase from: {best_model_path}\")\n",
    "            self.model.load_state_dict(torch.load(best_model_path))\n",
    "            print(f\"[DEBUG] Loaded model with classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "            print(\"Loaded best model from adversarial checkpoint (robust, filtered mismatched keys).\")\n",
    "        else:\n",
    "            print(\n",
    "                \"Using model from last adversarial epoch (patience disabled or no improvement).\"\n",
    "            )\n",
    "            if last_model_state is not None:\n",
    "                self.model.load_state_dict(last_model_state)\n",
    "                print(\"Loaded last adversarial model state.\")\n",
    "            else:\n",
    "                # If last_model_state is None (e.g., still in warmup), the model already has the last state\n",
    "                # But to be safe, we can load from the saved checkpoint which should be the last state when pat=False\n",
    "                if not self.pat:\n",
    "                    print(f\"[DEBUG] Loading last model state from checkpoint (patience disabled)\")\n",
    "                    self.model.load_state_dict(torch.load(best_model_path))\n",
    "                    print(\"Loaded last adversarial model state from checkpoint.\")\n",
    "                else:\n",
    "                    print(\"Model already has the last adversarial state (no loading needed).\")\n",
    "\n",
    "        return start_epoch + self.adv_training_epochs\n",
    "\n",
    "    def _evaluate_and_log(self, epoch, is_adv=False):\n",
    "        \"\"\"\n",
    "        Run validation and log results for both base and novel splits.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Current training epoch.\n",
    "            is_adv (bool): Whether evaluation is during adversarial training.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: Accuracy for base and novel classes.\n",
    "        \"\"\"\n",
    "\n",
    "        if is_adv:\n",
    "            metrics_base = self.eval_method.evaluate(\n",
    "                dataset=self.val_base,\n",
    "                classnames=self.base_classes,\n",
    "                desc_add=\" - Base\",\n",
    "            )\n",
    "            base_val_loss = metrics_base[\"loss\"]\n",
    "            base_val_acc = metrics_base[\"accuracy\"]\n",
    "\n",
    "            metrics_novel = self.eval_method.evaluate(\n",
    "                dataset=self.val_novel,\n",
    "                classnames=self.novel_classes,\n",
    "                desc_add=\" - Novel\",\n",
    "            )\n",
    "            novel_val_loss = metrics_novel[\"loss\"]\n",
    "            novel_val_acc = metrics_novel[\"accuracy\"]\n",
    "\n",
    "        else:\n",
    "\n",
    "            metrics_base = self.eval_method.evaluate(\n",
    "                dataset=self.val_base,\n",
    "                classnames=self.base_classes,\n",
    "                desc_add=\" - Base\",\n",
    "            )\n",
    "            base_val_loss = metrics_base[\"loss\"]\n",
    "            base_val_acc = metrics_base[\"accuracy\"]\n",
    "\n",
    "            metrics_novel = self.eval_method.evaluate(\n",
    "                dataset=self.val_novel,\n",
    "                classnames=self.novel_classes,\n",
    "                desc_add=\" - Novel\",\n",
    "            )\n",
    "            novel_val_loss = metrics_novel[\"loss\"]\n",
    "            novel_val_acc = metrics_novel[\"accuracy\"]\n",
    "\n",
    "        self.logger.log_validation(\n",
    "            epoch,\n",
    "            base_val_loss,\n",
    "            base_val_acc,\n",
    "            novel_val_loss,\n",
    "            novel_val_acc,\n",
    "            is_adv=is_adv,\n",
    "        )\n",
    "\n",
    "        return base_val_acc, novel_val_acc\n",
    "\n",
    "    def _log_final_metrics(self, tag, base_acc, novel_acc, step):\n",
    "        \"\"\"\n",
    "        Log final test results to TensorBoard.\n",
    "\n",
    "        Args:\n",
    "            tag (str): Descriptive tag for the log.\n",
    "            base_acc (float): Accuracy on base classes.\n",
    "            novel_acc (float): Accuracy on novel classes.\n",
    "            step (int): Epoch or step index for this log.\n",
    "        \"\"\"\n",
    "        self.logger.log_final_metrics(tag, base_acc, novel_acc, step)\n",
    "\n",
    "    def _lr_lambda(self, current_epoch):\n",
    "        \"\"\"\n",
    "        Learning rate scheduler with cosine annealing and warm-up.\n",
    "\n",
    "        Args:\n",
    "            current_epoch (int): Epoch index.\n",
    "\n",
    "        Returns:\n",
    "            float: Learning rate multiplier.\n",
    "        \"\"\"\n",
    "        if current_epoch < self.warmup_epoch:\n",
    "            return self.warmup_cons_lr / self.optimizer_configs[0].prompt_lr # type: ignore\n",
    "        return 0.5 * (\n",
    "            1\n",
    "            + math.cos(\n",
    "                math.pi\n",
    "                * (current_epoch - self.warmup_epoch)\n",
    "                / (self.max_epoch - self.warmup_epoch + 1e-7)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_evaluation(self, epoch_idx, base=False):\n",
    "        \"\"\"\n",
    "        Run evaluation on the test split for both base and novel classes.\n",
    "\n",
    "        Args:\n",
    "            epoch_idx (int): Epoch index for logging.\n",
    "            base (bool): Whether to evaluate the frozen base CLIP model.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: Base and novel class test accuracy.\n",
    "\n",
    "        model = self.model if not base else self.clip_model\n",
    "        base_accuracy = test_step(model, self.test_base, self.base_classes, self.batch_size, self.device, label=\"test\", base=base)\n",
    "        novel_accuracy = test_step(model, self.test_novel, self.novel_classes, self.batch_size, self.device, label=\"test\", base=base)\n",
    "        \"\"\"\n",
    "        if base:\n",
    "            base_metrics = self.zero_shot_base_classes_test_method.evaluate(\n",
    "                dataset=self.test_base,\n",
    "                desc_add=\" - Base Zero Shot\",\n",
    "\n",
    "            )\n",
    "            novel_metrics = self.zero_shot_novel_classes_test_method.evaluate(\n",
    "                dataset=self.test_novel,\n",
    "                desc_add=\" - Novel Zero Shot\",\n",
    "            )\n",
    "        else:\n",
    "            base_metrics = self.finetuned_test_method.evaluate(\n",
    "                dataset=self.test_base,\n",
    "                classnames=self.base_classes,\n",
    "                desc_add=\" - Base Fine Tuned\",\n",
    "            )\n",
    "            novel_metrics = self.finetuned_test_method.evaluate(\n",
    "                dataset=self.test_novel,\n",
    "                classnames=self.novel_classes,\n",
    "                desc_add=\" - Novel Fine Tuned\",\n",
    "            )\n",
    "\n",
    "        base_accuracy = base_metrics[\"accuracy\"]\n",
    "        novel_accuracy = novel_metrics[\"accuracy\"]\n",
    "        self.logger.log_test_accuracy(epoch_idx, base_accuracy, \"base_classes\")\n",
    "        self.logger.log_test_accuracy(epoch_idx, novel_accuracy, \"novel_classes\")\n",
    "        return base_accuracy, novel_accuracy\n",
    "\n",
    "    def save_model(self, path=\"./bin/cocoop\", prefix=\"\"):\n",
    "        \"\"\"\n",
    "        Save model weights to disk.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory to save the model to.\n",
    "            prefix (str): Filename prefix to distinguish models.\n",
    "        \"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(f\"[DEBUG] Saving model with classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "        with self.model.temporary_classnames([CLASS_NAMES[idx] for idx in self.base_classes]):\n",
    "            torch.save(\n",
    "                self.model.state_dict(), os.path.join(path, f\"{prefix}{self.run_name}.pth\")\n",
    "            )\n",
    "\n",
    "    def save_mlp_adversary(self, path=\"./bin/cocoop\", prefix=\"\"):\n",
    "        \"\"\"\n",
    "        Save the MLP adversary weights to disk.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory to save the MLP adversary model to.\n",
    "            prefix (str): Filename prefix to distinguish models.\n",
    "        \"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch.save(\n",
    "            self.mlp_adversary.state_dict(),\n",
    "            os.path.join(path, f\"{prefix}{self.run_name}_mlp_adversary.pth\"),\n",
    "        )\n",
    "\n",
    "    def get_optimizer(self, model, mlp_adversary, config):\n",
    "        \"\"\"\n",
    "        Build an SGD optimizer with separate learning rates for different parameter groups.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): Main model.\n",
    "            mlp_adversary (torch.nn.Module): Optional adversarial MLP.\n",
    "            config: Optimizer configuration namespace.\n",
    "\n",
    "        Returns:\n",
    "            torch.optim.Optimizer: Configured optimizer.\n",
    "        \"\"\"\n",
    "        params = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.named_parameters()\n",
    "                    if \"prompt_learner\" in n and p.requires_grad\n",
    "                ],\n",
    "                \"lr\": config.prompt_lr,\n",
    "            }\n",
    "        ]\n",
    "        if mlp_adversary is not None:\n",
    "            params.append(\n",
    "                {\n",
    "                    \"params\": mlp_adversary.parameters(),\n",
    "                    \"lr\": config.mlp_lr,\n",
    "                }\n",
    "            )\n",
    "        return torch.optim.SGD(\n",
    "            params, weight_decay=config.weight_decay, momentum=config.momentum\n",
    "        )\n",
    "\n",
    "    def split_by_classes(self, dataset, class_list):\n",
    "        idxs = [i for i, (_, label) in enumerate(dataset) if label in class_list]\n",
    "        return torch.utils.data.Subset(dataset, idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a3ca04",
   "metadata": {
    "id": "73a3ca04"
   },
   "source": [
    "## Training System Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f58a72",
   "metadata": {
    "id": "c5f58a72"
   },
   "source": [
    "### Default training method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bd54f0",
   "metadata": {
    "id": "e4bd54f0"
   },
   "outputs": [],
   "source": [
    "# From: ./training_systems/core/TrainingMethod.py\n",
    "\"\"\"\n",
    " Abstract base class for training methods. it should have forward_backward method to be fulfilled by its children\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any, Callable\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import AverageMeter, ContiguousLabelDataset\n",
    "\n",
    "\n",
    "class TrainingMethod(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for training methods. it should have forward_backward method to be fulfilled by its children\n",
    "\n",
    "    The TrainingMethod class serves as an abstract interface that standardizes the structure of different training strategies.\n",
    "    It defines key methods that all training strategies must implement, such as:\n",
    "     - `get_metrics`: to initialize and return the performance metrics.\n",
    "     - `get_data_loader`: to prepare the DataLoader tailored for the specific training strategy.\n",
    "     - `forward_backward`: to implement the forward and backward passes during training.\n",
    "     - `debug_metrics_to_pbar_args`: to convert debug information for progress bar visualization.\n",
    "     - `training_step_return`: to return summary metrics after a training epoch.\n",
    "     It also provides common functionality like `start_training`, `optimizer_step`, and `train_step` to be reused across concrete training methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: Any, optimizer: Any, title: str, debug) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the training method.\n",
    "\n",
    "        Args:\n",
    "            model (Any): The model to train.\n",
    "            optimizer (Any): The optimizer to use for training.\n",
    "            title (str): Title for identification (e.g., for progress display).\n",
    "            debug (bool): Flag to enable debug mode for extra logging.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.title = title\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self.debug = debug\n",
    "        self.mlp_adversary = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Initialize and return a dictionary of performance metrics.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: Dictionary mapping metric names to metric trackers.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_loader(self, dataset, batch_size) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Create a data loader for the training dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: The training dataset.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch data loader instance.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_backward(\n",
    "            self, sample, batch_idx, metrics: Dict[str, AverageMeter], dataset: ContiguousLabelDataset, accumulation_steps: int = 1, step: int = 0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Execute forward and backward pass, compute loss, and update metrics.\n",
    "\n",
    "        Args:\n",
    "            sample: Current batch sample from data loader.\n",
    "            batch_idx (int): Index of the current batch.\n",
    "            metrics (Dict[str, AverageMeter]): Metric trackers.\n",
    "            dataset (ContiguousLabelDataset): Dataset wrapper for label mapping.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary of current debug metric values.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def debug_metrics_to_pbar_args(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Format debug metrics for display in a progress bar.\n",
    "\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Metrics from the current step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Formatted metrics for tqdm display.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Generate summary metrics after completing a training epoch.\n",
    "\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Collected training metrics.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: List of averaged metric values for reporting.\n",
    "        \"\"\"\n",
    "\n",
    "    def optimizer_step(self) -> None:\n",
    "        \"\"\"\n",
    "        Apply the optimizer's step and zero the gradients.\n",
    "        \"\"\"\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def start_training(self) -> None:\n",
    "        \"\"\"\n",
    "        Set the model in training mode.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "    def train_step(self, dataset, batch_size, classnames, accumulation_steps: int = 1):\n",
    "        \"\"\"\n",
    "        Perform a complete training epoch, including data loading, training, and metric collection.\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset used for training.\n",
    "            batch_size (int): Number of samples per training batch.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Averaged values for each tracked metric after the epoch.\n",
    "        \"\"\"\n",
    "        metrics = self.get_metrics()\n",
    "        self.start_training()\n",
    "        tmp_dataset = ContiguousLabelDataset(dataset, classnames)\n",
    "        dataloader = self.get_data_loader(tmp_dataset, batch_size)\n",
    "        pbar = tqdm(dataloader, desc=f\"Training-{self.title}\", position=1, leave=False)\n",
    "        for batch_idx, sample in enumerate(dataloader):\n",
    "            debug_metrics = self.forward_backward(sample, batch_idx, metrics, tmp_dataset, accumulation_steps=accumulation_steps, step=batch_idx)\n",
    "            pbar.set_postfix(\n",
    "                self.debug_metrics_to_pbar_args(debug_metrics)\n",
    "            )\n",
    "            pbar.update(1)\n",
    "        if accumulation_steps > 1:\n",
    "            if (batch_idx + 1) % accumulation_steps != 0:\n",
    "                if self.mlp_adversary is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        list(self.model.parameters()) + list(self.mlp_adversary.parameters()),\n",
    "                        max_norm=1.0,\n",
    "                        norm_type=2.0,\n",
    "                        error_if_nonfinite=True\n",
    "                    )\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "        return self.training_step_return(metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37be41",
   "metadata": {
    "id": "dc37be41"
   },
   "source": [
    "### DoubleDataset Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9da8f",
   "metadata": {
    "id": "16a9da8f"
   },
   "outputs": [],
   "source": [
    "# From: ./training_systems/core/DoubleDatasetTrainingMethod.py\n",
    "\"\"\"\n",
    " Abstract base class for training methods. it should have forward_backward method to be fulfilled by its children\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any, Callable\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import AverageMeter, ContiguousLabelDataset\n",
    "\n",
    "class DoubleDatasetTrainingMethod:\n",
    "    \"\"\"\n",
    "    Abstract base class for training methods. it should have forward_backward method to be fulfilled by its children\n",
    "\n",
    "    The TrainingMethod class serves as an abstract interface that standardizes the structure of different training strategies.\n",
    "    It defines key methods that all training strategies must implement, such as:\n",
    "     - `get_metrics`: to initialize and return the performance metrics.\n",
    "     - `get_data_loader`: to prepare the DataLoader tailored for the specific training strategy.\n",
    "     - `forward_backward`: to implement the forward and backward passes during training.\n",
    "     - `debug_metrics_to_pbar_args`: to convert debug information for progress bar visualization.\n",
    "     - `training_step_return`: to return summary metrics after a training epoch.\n",
    "     It also provides common functionality like `start_training`, `optimizer_step`, and `train_step` to be reused across concrete training methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: Any, optimizer: Any, title: str, debug) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the training method.\n",
    "\n",
    "        Args:\n",
    "            model (Any): The model to train.\n",
    "            optimizer (Any): The optimizer to use for training.\n",
    "            title (str): Title for identification (e.g., for progress display).\n",
    "            debug (bool): Flag to enable debug mode for extra logging.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.title = title\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self.debug = debug\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Initialize and return a dictionary of performance metrics.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: Dictionary mapping metric names to metric trackers.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_loader1(self, dataset, batch_size) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Create a data loader for the training dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: The training dataset.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch data loader instance.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def get_data_loader2(self, dataset, batch_size) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Create a data loader for the training dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: The training dataset.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch data loader instance.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def debug_metrics_to_pbar_args1(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Format debug metrics for display in a progress bar.\n",
    "\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Metrics from the current step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Formatted metrics for tqdm display.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def debug_metrics_to_pbar_args2(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Format debug metrics for display in a progress bar.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Generate summary metrics after completing a training epoch.\n",
    "\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Collected training metrics.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: List of averaged metric values for reporting.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimizer_step(self) -> None:\n",
    "        \"\"\"\n",
    "        Apply the optimizer's step and zero the gradients.\n",
    "        \"\"\"\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def start_training(self) -> None:\n",
    "        \"\"\"\n",
    "        Set the model in training mode.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_backward1(\n",
    "            self, sample, batch_idx, metrics: Dict[str, AverageMeter], dataset: ContiguousLabelDataset, classes: list[int]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Execute forward and backward pass, compute loss, and update metrics.\n",
    "\n",
    "        Args:\n",
    "            sample: Current batch sample from data loader.\n",
    "            batch_idx (int): Index of the current batch.\n",
    "            metrics (Dict[str, AverageMeter]): Metric trackers.\n",
    "            dataset (ContiguousLabelDataset): Dataset wrapper for label mapping.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary of current debug metric values.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_backward2(\n",
    "            self, sample, batch_idx, metrics: Dict[str, AverageMeter], dataset: ContiguousLabelDataset, classes: list[int]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Execute forward and backward pass, compute loss, and update metrics.\n",
    "        \"\"\"\n",
    "\n",
    "    def double_datasets_train_step(self, dataset1, dataset2, batch_size, names: list[str], classes1, classes2):\n",
    "        assert len(names) == 2, \"Number of names must be 2\"\n",
    "        metrics = self.get_metrics()\n",
    "        self.start_training()\n",
    "        tmp_dataset1 = ContiguousLabelDataset(dataset1, classes1)\n",
    "        tmp_dataset2 = ContiguousLabelDataset(dataset2, classes2)\n",
    "\n",
    "        dataloader1 = self.get_data_loader1(tmp_dataset1, batch_size)\n",
    "        dataloader2 = self.get_data_loader2(tmp_dataset2, batch_size)\n",
    "\n",
    "        pbar = tqdm(dataloader1, desc=f\"Training-{self.title}/{names[0]}\", position=1, leave=False)\n",
    "        for batch_idx, sample in enumerate(dataloader1):\n",
    "            debug_metrics = self.forward_backward1(sample, batch_idx, metrics, tmp_dataset1, classes1)\n",
    "            pbar.set_postfix(\n",
    "                self.debug_metrics_to_pbar_args1(debug_metrics)\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "        pbar = tqdm(dataloader2, desc=f\"Training-{self.title}/{names[1]}\", position=1, leave=False)\n",
    "        for batch_idx, sample in enumerate(dataloader2):\n",
    "            debug_metrics = self.forward_backward2(sample, batch_idx, metrics, tmp_dataset2, classes2)\n",
    "            pbar.set_postfix(\n",
    "                self.debug_metrics_to_pbar_args2(debug_metrics)\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "        return self.training_step_return(metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08fbac",
   "metadata": {
    "id": "2d08fbac"
   },
   "source": [
    "### Evaluation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9902df",
   "metadata": {
    "id": "6e9902df"
   },
   "outputs": [],
   "source": [
    "# From: ./training_systems/core/EvaluationMethod.py\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.metrics import AverageMeter\n",
    "\n",
    "\n",
    "class EvaluationMethod(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for evaluation methods. Standardizes evaluation interface for different strategies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, batch_size: int = 32, device=None):\n",
    "        \"\"\"\n",
    "        Initialize evaluation method.\n",
    "\n",
    "        Args:\n",
    "            model: The model to evaluate.\n",
    "            batch_size (int): Evaluation batch size.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = next(self.model.parameters()).device if device is None else device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, dataset, classnames, desc_add=\"\") -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Perform the evaluation.\n",
    "\n",
    "        Args:\n",
    "            dataset: The dataset to evaluate on.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary with evaluation results (e.g., accuracy, loss).\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ccdfa",
   "metadata": {
    "id": "477ccdfa"
   },
   "source": [
    "## Training Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336eaf35",
   "metadata": {
    "id": "336eaf35"
   },
   "source": [
    "## Base CoCoOp Training\n",
    "\n",
    "**Key Aspects:** Dynamic prompt generation conditioned on image features; cross-entropy loss on base classes.  \n",
    "**Constraints:** Trains only on seen/base classes; no explicit mechanisms for handling novel categories.\n",
    "\n",
    "Base CoCoOp learns context vectors (prompts) dynamically generated by an MLP (called MetaNet), conditioned on visual features extracted from input images. For each training sample, the model constructs prompts for all base classes by combining the learned context vectors with class names, then encodes these prompts using a text encoder.\n",
    "\n",
    "Similarity scores between image and text embeddings are computed and optimized using a cross-entropy loss over the base classes. The training updates both the parameters of MetaNet and the context vectors through backpropagation until convergence.\n",
    "\n",
    "This approach allows the prompts to adapt based on image content, improving recognition on base categories and setting the stage for further improvements like knowledge distillation or adversarial regularization.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Procedure (repeat until convergence):\n",
    "\n",
    "1. Sample an image and its label:  \n",
    "   $$\n",
    "   (\\mathbf{x}, \\hat{y}) \\sim \\mathcal{D}_{\\text{seen}}^{\\text{train}}\n",
    "   $$\n",
    "\n",
    "2. Extract visual features from the image:  \n",
    "   $$\n",
    "   \\mathbf{v} = f(\\mathbf{x})\n",
    "   $$\n",
    "\n",
    "3. Generate context vectors conditioned on visual features:  \n",
    "   $$\n",
    "   [v_1, \\ldots, v_M] = \\text{MetaNet}_\\theta(\\mathbf{v})\n",
    "   $$\n",
    "\n",
    "4. For each base class $c \\in \\mathcal{C}_{\\text{base}}$:  \n",
    "   - Construct the prompt by combining learned context vectors and the class name:  \n",
    "     $$\n",
    "     \\mathbf{p}_c = [p_1 + v_1, \\ldots, p_M + v_M, \\texttt{\"class } c \\texttt{\"}]\n",
    "     $$  \n",
    "   - Encode the prompt into a text embedding:  \n",
    "     $$\n",
    "     \\mathbf{t}_c = g(\\mathbf{p}_c)\n",
    "     $$\n",
    "\n",
    "5. Compute similarity scores between the image feature and each class text embedding:  \n",
    "   $$\n",
    "   \\ell_c = \\text{sim}(\\mathbf{v}, \\mathbf{t}_c)\n",
    "   $$\n",
    "\n",
    "6. Compute cross-entropy loss over the similarity scores with respect to the ground truth label:  \n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{CE}} = \\text{CE}(\\{\\ell_c\\}, \\hat{y})\n",
    "   $$\n",
    "\n",
    "7. Update MetaNet parameters $\\theta$ and context vectors $p_1, \\ldots, p_M$ via backpropagation on $\\mathcal{L}_{\\text{CE}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091c472a",
   "metadata": {
    "id": "091c472a"
   },
   "outputs": [],
   "source": [
    "# From: ./training_systems/training_methods/BaseCoCoOp.py\n",
    "\"\"\"\n",
    "This module defines the BaseCoCoOp training method, a baseline implementation for CoCoOp-based optimization.\n",
    "It includes metric tracking, data loading, and training step execution using standard cross-entropy loss.\n",
    "\"\"\"\n",
    "import random\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from training_systems.core import TrainingMethod\n",
    "# from utils import AverageMeter, ContiguousLabelDataset\n",
    "\n",
    "\n",
    "class BaseCoCoOp(TrainingMethod):\n",
    "    \"\"\"\n",
    "    BaseCoCoOp implements a simple training routine based on cross-entropy loss without adversarial components.\n",
    "    This class inherits from TrainingMethod and provides basic training loop functionalities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: Any,\n",
    "            optimizer: Any,\n",
    "            debug: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__(model, optimizer, \"Base CoCoOp\", debug)\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Initializes and returns the performance metrics used during training.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: A dictionary with average meters for loss and accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"loss_metric\": AverageMeter(),\n",
    "            \"accuracy_metric\": AverageMeter(),\n",
    "        }\n",
    "\n",
    "    def get_data_loader(self, dataset: ContiguousLabelDataset, batch_size: int) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Creates and returns a DataLoader for the given dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (ContiguousLabelDataset): Dataset to load samples from.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch DataLoader configured with shuffle and multiple workers.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "        )\n",
    "\n",
    "    def forward_backward(\n",
    "            self,\n",
    "            sample,\n",
    "            batch_idx,\n",
    "            metrics: Dict[str, AverageMeter],\n",
    "            dataset: ContiguousLabelDataset,\n",
    "            accumulation_steps: int = 1,\n",
    "            step: int = 0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Executes the forward and backward pass for the BaseCoCoOp training method.\n",
    "\n",
    "        Args:\n",
    "            sample (Tuple[Tensor, Tensor]): Batch of input data and targets.\n",
    "            batch_idx (int): Index of the current batch.\n",
    "            metrics (Dict[str, AverageMeter]): Dictionary of metrics to be updated.\n",
    "            dataset (ContiguousLabelDataset): Dataset object (unused in this implementation).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary with current loss and accuracy values.\n",
    "        \"\"\"\n",
    "        # Load data into GPU\n",
    "        inputs, targets = sample\n",
    "        # === Pseudo-base: cross-entropy ===\n",
    "        inputs_base = inputs.to(self.device)\n",
    "        targets_base = targets.to(self.device)\n",
    "\n",
    "        logits_base, loss_ce = self.model(inputs_base, targets_base)\n",
    "        # === Combine losses ===\n",
    "        print(\"SHAPES LOGITS: \",logits_base.shape, targets_base.shape)\n",
    "\n",
    "        loss_ce = loss_ce / accumulation_steps\n",
    "        loss_ce.backward()\n",
    "\n",
    "        # optimizer step every `accumulation_steps` steps\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        self.optimizer_step()\n",
    "        batch_size_total = inputs_base.size(0)\n",
    "\n",
    "        metrics[\"loss_metric\"].update(loss_ce.item(), n=batch_size_total)\n",
    "\n",
    "        _, predicted = logits_base.max(dim=1)\n",
    "        correct = (predicted == targets_base).sum().item()\n",
    "        total = targets_base.size(0)\n",
    "        metrics[\"accuracy_metric\"].update(correct, n=total, raw=True)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss_ce.item(),\n",
    "            \"accuracy\": correct / targets_base.size(0),\n",
    "        }\n",
    "\n",
    "    def debug_metrics_to_pbar_args(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Passes debug metrics directly to be displayed in the progress bar.\n",
    "\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Metrics from the current step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Unmodified metrics suitable for display.\n",
    "        \"\"\"\n",
    "        return debug_metrics\n",
    "\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Extracts and returns the average loss and accuracy metrics after a training step.\n",
    "\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Dictionary containing tracked metrics.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: A list containing the average loss and accuracy values.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            metrics[\"loss_metric\"].avg,\n",
    "            metrics[\"accuracy_metric\"].avg,\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85546b4d",
   "metadata": {
    "id": "85546b4d"
   },
   "source": [
    "### MLP Adversarial Training\n",
    "\n",
    "One of our proposed modifications incorporates an adversarial training scheme applied to the MLP prompt learner within CoCoOp. The goal is to encourage the MLP to produce features invariant to predefined image clusters, thereby improving robustness.\n",
    "\n",
    "#### Loss Formulation  \n",
    "The total CoCoOp training loss combines the original cross-entropy classification loss and an adversarial loss weighted by a hyperparameter $\\gamma$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{CE}} + \\gamma \\cdot (-\\mathcal{L}_{\\text{MLP-adv}})\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}_{\\text{CE}}$ is the base CoCoOp cross-entropy loss, and $\\mathcal{L}_{\\text{MLP-adv}}$ is the adversarial loss from the MLP. The adversarial component is implemented using a Gradient Reversal Layer (GRL) [4], effectively maximizing $\\mathcal{L}_{\\text{MLP-adv}}$ during CoCoOp training.\n",
    "\n",
    "The MLP itself is trained to minimize $\\mathcal{L}_{\\text{MLP-adv}}$, which corresponds to a classification loss that predicts the cluster membership of each input image.\n",
    "\n",
    "#### Cluster Definition\n",
    "\n",
    "Clusters are precomputed before training and represent groupings of classes used to guide adversarial training in the MLP prompt learner (see [Clustering](#clustering)). Different clustering strategies and parameters can be employed depending on the experiment.\n",
    "\n",
    "During training, the MLP aims to classify the cluster membership of each input image, with the number of clusters and loss type (binary or categorical cross-entropy) adjusted accordingly. These clusters help encourage the prompt learner to produce representations invariant to certain groupings, potentially improving robustness and generalization.\n",
    "\n",
    "#### Intuition  \n",
    "The most useful information we tried to exploit is: all the latent-space representations of the images in a category are different from those for images in a different category.\n",
    "\n",
    "We adversarially train the CoCoOp prompt learner against a cluster-based MLP classifier to prevent it from encoding cluster-specific features. This forces the model to focus on broader, more transferable representations.\n",
    "\n",
    "Our belief is that if the prompt learner degrades the discriminator's accuracy, it also reduces the influence of shared semantic traits across clusters' categories, leading to logits that are less biased by intra-domain similarities. This is key in our setting, where classes are visually similar and overfitting to fine-grained base class cues harms generalization to novel ones.\n",
    "\n",
    "[4]:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f272c4",
   "metadata": {
    "id": "13f272c4"
   },
   "source": [
    "#### Adversarial Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f2600",
   "metadata": {
    "id": "c07f2600"
   },
   "outputs": [],
   "source": [
    "# From: ./training_systems/training_methods/Adversarial.py\n",
    "\"\"\"\n",
    "This module implements the Adversarial training method, which incorporates a gradient reversal layer\n",
    "and an adversarial MLP to encourage domain-invariant feature learning.\n",
    "\"\"\"\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from training_systems.core import TrainingMethod\n",
    "# from utils import AverageMeter, ContiguousLabelDataset, CLASS_NAMES\n",
    "# from model.cocoop.mlp_adversary import AdversarialMLP, GradientReversalLayer\n",
    "\n",
    "\n",
    "class Adversarial(TrainingMethod):\n",
    "    \"\"\"\n",
    "    Adversarial training method using a Gradient Reversal Layer and an MLP adversary.\n",
    "\n",
    "    Attributes:\n",
    "        cls_cluster_dict (Dict[int, Any]): Maps class indices to cluster labels.\n",
    "        grl (GradientReversalLayer): The gradient reversal layer instance.\n",
    "        mlp_adversary (AdversarialMLP): The adversarial MLP used to confuse cluster prediction.\n",
    "        lambda_adv (float): Weight of the adversarial loss term.\n",
    "        debug (bool): If True, print debug information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: Any,\n",
    "            optimizer: Any,\n",
    "            cls_cluster_dict: Dict[int, Any],\n",
    "            grl: GradientReversalLayer,\n",
    "            mlp_adversary: AdversarialMLP,\n",
    "            lambda_adv,\n",
    "            tmp_classes: list,\n",
    "            debug: bool = False,\n",
    "            gaussian_noise: float = 0.0,\n",
    "            use_bias_ctx: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (Any): The main model being trained.\n",
    "            optimizer (Any): Optimizer for updating model parameters.\n",
    "            cls_cluster_dict (Dict[int, Any]): Mapping from class labels to clusters.\n",
    "            grl (GradientReversalLayer): The gradient reversal layer.\n",
    "            mlp_adversary (AdversarialMLP): The adversarial network module.\n",
    "            lambda_adv (float): Weight for the adversarial loss.\n",
    "            debug (bool, optional): Enables debug mode. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__(model, optimizer, \"Adv.\", debug)\n",
    "        self.cls_cluster_dict = cls_cluster_dict\n",
    "        self.grl = grl\n",
    "        self.mlp_adversary = mlp_adversary\n",
    "        self.lambda_adv = lambda_adv\n",
    "        self.tmp_classes = tmp_classes\n",
    "        self.gaussian_noise = gaussian_noise\n",
    "        self.use_bias_ctx = use_bias_ctx\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: Dictionary containing initialized metrics for training.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"total_loss_metric\": AverageMeter(),\n",
    "            \"ce_loss_metric\": AverageMeter(),\n",
    "            \"adv_loss_metric\": AverageMeter(),\n",
    "            \"accuracy_metric\": AverageMeter(),\n",
    "        }\n",
    "\n",
    "    def get_data_loader(self, dataset: ContiguousLabelDataset, batch_size: int) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (ContiguousLabelDataset): Dataset to be used.\n",
    "            batch_size (int): Size of each batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: Configured PyTorch DataLoader instance.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "        )\n",
    "\n",
    "    def forward_backward(\n",
    "            self,\n",
    "            sample,\n",
    "            batch_idx,\n",
    "            metrics: Dict[str, AverageMeter],\n",
    "            dataset: ContiguousLabelDataset,\n",
    "            accumulation_steps: int = 1,\n",
    "            step: int = 0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Executes the forward and backward pass.\n",
    "\n",
    "        Args:\n",
    "            sample (Tuple[Tensor, Tensor]): Batch of input data and targets.\n",
    "            batch_idx (int): Index of the current batch.\n",
    "            metrics (Dict[str, AverageMeter]): Dictionary of metrics to be updated.\n",
    "            dataset (ContiguousLabelDataset): Dataset for cluster label lookup.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary with loss and accuracy metrics.\n",
    "        \"\"\"\n",
    "        # Load data into GPU\n",
    "        inputs, targets = sample\n",
    "        inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "        targets_real_category = [dataset.idx2cat[c.item()] for c in targets]\n",
    "        cluster_target = [int(self.cls_cluster_dict[int(tl)]) for tl in targets_real_category]\n",
    "        cluster_target = torch.tensor(\n",
    "            cluster_target,\n",
    "            device=targets.device,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # Use all tmp_classes for adversarial phase\n",
    "        with self.model.temporary_classnames([CLASS_NAMES[idx] for idx in self.tmp_classes]):\n",
    "            logits, ce_loss, img_features, ctx, bias, avg_txt_features, selected_text_features = self.model(inputs, targets, get_image_features=True)\n",
    "\n",
    "            if self.gaussian_noise > 0:\n",
    "                noise = torch.randn_like(logits) * self.gaussian_noise\n",
    "                noisy_logits = logits + noise\n",
    "            else:\n",
    "                noisy_logits = logits\n",
    "\n",
    "            if self.use_bias_ctx:\n",
    "                if ctx.shape[0] == 1:\n",
    "                    ctx = ctx.expand(bias.shape[0], -1, -1)\n",
    "\n",
    "                ctx_shifted = ctx + bias  # shape: [B, L, D]\n",
    "                # concat = ctx_shifted.view(ctx_shifted.size(0), -1).to(dtype=torch.float32)\n",
    "                concat = torch.cat([selected_text_features, ctx_shifted.mean(dim=1)], dim=1).to(dtype=torch.float32)\n",
    "\n",
    "            else:\n",
    "                concat = torch.cat([avg_txt_features, noisy_logits], dim=1).to(dtype=torch.float32)\n",
    "            # print(f\"concat shape: {concat.shape}, bias shape: {bias.shape}, ctx shape: {ctx.shape}, avg_txt_features shape: {avg_txt_features.shape}, logits shape: {logits.shape}\")\n",
    "            reversed_concat = self.grl(concat)\n",
    "            # print(f\"reversed_concat shape: {reversed_concat.shape}\")\n",
    "            cluster_logits = self.mlp_adversary(reversed_concat)\n",
    "\n",
    "            if cluster_logits.shape[1] == 1:\n",
    "                # Binary classification\n",
    "                cluster_logits = cluster_logits.squeeze(-1)\n",
    "                loss_adv = F.binary_cross_entropy_with_logits(cluster_logits, cluster_target.float())\n",
    "            else:\n",
    "                # Multi-class classification\n",
    "                loss_adv = F.cross_entropy(cluster_logits, cluster_target.long())\n",
    "\n",
    "            # Skip adversarial update if prompt learner is frozen\n",
    "            if any(p.requires_grad for p in self.model.prompt_learner.parameters()):\n",
    "                ce_grads = self.get_grads(ce_loss)\n",
    "            else:\n",
    "                ce_grads = None  # Or torch.zeros_like(...), depending on downstream use\n",
    "            total_loss = ce_loss + self.lambda_adv * loss_adv\n",
    "\n",
    "            total_loss = total_loss / accumulation_steps\n",
    "            total_loss.backward()\n",
    "            # print(f\"step: {step}, total_loss: {total_loss.item():.4f}, accumulation_steps: {accumulation_steps}, \")\n",
    "            # --- accumulate grads and update ---\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(self.model.parameters()) + list(self.mlp_adversary.parameters()),\n",
    "                    max_norm=1.0,\n",
    "                    norm_type=2.0,\n",
    "                    error_if_nonfinite=True\n",
    "                )\n",
    "                self.optimizer_step()\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "            if batch_idx < 3 and self.debug:  # Log only a few batches for performance\n",
    "                print(f\"[Batch {batch_idx}] CE Loss: {ce_loss.item():.4f} | \"\n",
    "                      f\"Adv Loss: {loss_adv.item():.4f} | \"\n",
    "                      f\"Total Loss: {total_loss.item():.4f} | \"\n",
    "                      f\"lambda_adv: {self.lambda_adv:.4f}\")\n",
    "            batch_size = inputs.shape[0]\n",
    "            metrics[\"total_loss_metric\"].update(total_loss.item(), n=batch_size)\n",
    "            metrics[\"ce_loss_metric\"].update(ce_loss.item(), n=batch_size)\n",
    "            metrics[\"adv_loss_metric\"].update(loss_adv.item(), n=batch_size)\n",
    "            _, predicted = logits.max(dim=1)\n",
    "            correct = predicted.eq(targets).sum().item()\n",
    "            metrics[\"accuracy_metric\"].update(correct, n=batch_size, raw=True)\n",
    "            return {\n",
    "                \"total_loss\": total_loss.item(),\n",
    "                \"ce_loss\": ce_loss.item(),\n",
    "                \"adv_loss\": loss_adv.item(),\n",
    "                \"accuracy\": correct / batch_size,\n",
    "            }\n",
    "\n",
    "    def print_grads_norms(self, bce_grads, ce_grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bce_grads (Dict[str, Tensor]): Gradients from BCE loss.\n",
    "            ce_grads (Dict[str, Tensor]): Gradients from CE loss.\n",
    "        \"\"\"\n",
    "        for name in ce_grads:\n",
    "            ce_norm = ce_grads[name].norm().item()\n",
    "            bce_norm = bce_grads[name].norm().item()\n",
    "            print(f\"{name}: CE grad norm = {ce_norm:.4e}, BCE grad norm = {bce_norm:.4e}\")\n",
    "\n",
    "    def get_grads(self, loss):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loss (Tensor): Loss tensor to backpropagate.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Tensor]: Dictionary of gradients.\n",
    "        \"\"\"\n",
    "        loss.backward(retain_graph=True)\n",
    "        ce_grads = {}\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.grad is not None and \"prompt_learner\" in name:\n",
    "                ce_grads[name] = p.grad.detach().clone()\n",
    "        # --- Zero gradients ---\n",
    "        self.optimizer.zero_grad()\n",
    "        return ce_grads\n",
    "\n",
    "    def debug_metrics_to_pbar_args(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Metrics from current training step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Same metrics, passed to progress bar.\n",
    "        \"\"\"\n",
    "        return debug_metrics\n",
    "\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Collected training metrics.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Average values of total, accuracy, CE, and adversarial losses.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            metrics[\"total_loss_metric\"].avg,\n",
    "            metrics[\"accuracy_metric\"].avg,\n",
    "            metrics[\"ce_loss_metric\"].avg,\n",
    "            metrics[\"adv_loss_metric\"].avg,\n",
    "        ]\n",
    "\n",
    "    def update_lambda_adv(self, lambda_adv) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lambda_adv (float): New value to set for lambda_adv.\n",
    "        \"\"\"\n",
    "        self.lambda_adv = lambda_adv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92064a70",
   "metadata": {
    "id": "92064a70"
   },
   "source": [
    "#### MLP Adversarial + GRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c0760",
   "metadata": {
    "id": "187c0760"
   },
   "outputs": [],
   "source": [
    "# From: ./model/cocoop/mlp_adversary.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block with a linear layer, layer normalization, ReLU activation, dropout, and optional identity or projection shortcut.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim_in, dim_out)\n",
    "        self.norm = nn.LayerNorm(dim_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.residual = (\n",
    "            nn.Identity() if dim_in == dim_out else nn.Linear(dim_in, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the residual block with normalization, activation, dropout, and residual connection.\n",
    "        \"\"\"\n",
    "        out = self.linear(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop(out)\n",
    "        return out + self.residual(x)\n",
    "\n",
    "\n",
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Implements a gradient reversal layer as a custom autograd function, useful in adversarial training.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        \"\"\"\n",
    "        Forward pass that returns the input as-is and stores the lambda factor for use in the backward pass.\n",
    "        \"\"\"\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass that reverses the gradient by multiplying it by -lambda.\n",
    "        \"\"\"\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for the GradientReversalFunction to integrate it into a standard nn.Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_=1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Applies the gradient reversal function with the stored lambda parameter.\n",
    "        \"\"\"\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "\n",
    "\n",
    "class AdversarialMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron with optional bias context support, designed for adversarial learning.\n",
    "    Uses residual blocks for intermediate layers and configurable final output dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, opt, output_dim=1, use_bias_ctx=False, n_ctx=4):\n",
    "        \"\"\"\n",
    "        Initializes the adversarial MLP with given structure and optional bias context input.\n",
    "        Builds the network using residual blocks and applies Xavier initialization to weights.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dims = opt.hidden_structure\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        if use_bias_ctx:\n",
    "            # Add a bias context layer if specified\n",
    "            layers.append(nn.Linear(512*2, hidden_dims[0]))\n",
    "            layers.append(nn.ReLU())\n",
    "            for in_dim, out_dim in zip(hidden_dims[:-1], hidden_dims[1:]):\n",
    "                layers.append(ResidualBlock(in_dim, out_dim))\n",
    "        else:\n",
    "            for in_dim, out_dim in zip(hidden_dims[:-1], hidden_dims[1:]):\n",
    "                layers.append(ResidualBlock(in_dim, out_dim))\n",
    "\n",
    "        # Final output layer with configurable output_dim\n",
    "        layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        self.model.apply(self.init_weights.__get__(self, AdversarialMLP))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the adversarial MLP model.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass and applies a sigmoid activation to the output.\n",
    "        Squeezes output if it's a single dimension.\n",
    "        \"\"\"\n",
    "        out = self.forward(x)\n",
    "        return torch.sigmoid(out).squeeze(-1) if out.shape[-1] == 1 else torch.sigmoid(out)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        \"\"\"\n",
    "        Initializes weights of linear layers using Xavier uniform distribution and biases to zero.\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5069aa",
   "metadata": {
    "id": "fa5069aa"
   },
   "source": [
    "#### Clustering\n",
    "To support adversarial training and related experiments, we utilize multiple methods to assign categories into clusters. These cluster assignments are designed to partition classes based on semantic, visual, or arbitrary criteria, allowing flexible experimentation.\n",
    "\n",
    "- **Default Clustering:** The simplest form, grouping classes into base (seen) and novel (unseen) categories.\n",
    "\n",
    "- **Visual Feature Clustering:** Extracting CLIP visual embeddings for each base class, aggregating features across training samples, then projecting into a lower-dimensional space using PCA (preserving 95% variance). We compute cosine distances between class features and apply Agglomerative Clustering to group visually similar categories. This can be extended to produce more than two clusters, enabling the MLP discriminator to differentiate among multiple groups.\n",
    "\n",
    "- **Random Clustering:** Synthetic cluster assignments are generated by randomly partitioning classes using various distributions (uniform, fully random, or sequential). This tests the adversarial module‚Äôs robustness under arbitrary partitions and noise in the clustering criterion.\n",
    "\n",
    "##### Consistency Across Datasets\n",
    "Cluster assignments are consistently applied to both training and validation datasets.\n",
    "\n",
    "##### Caching Mechanism\n",
    "To improve efficiency, cluster assignments (both as indices and category names) are cached. If cached clusters exist for a given configuration, they are loaded automatically; otherwise, they are computed and saved for reuse.\n",
    "\n",
    "##### Configurability via YAML Files\n",
    "All clustering parameters, including the number of clusters, assignment criteria, and caching options, are configurable via the experiment `.yaml` files. This modular setup facilitates systematic evaluation of how different clusterings affect adversarial training and overall model generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3ccce8",
   "metadata": {
    "id": "fc3ccce8"
   },
   "outputs": [],
   "source": [
    "# From: ./utils/clustering.py\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# from utils.datasets import get_data, base_novel_categories, split_data, CLASS_NAMES\n",
    "import clip\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter, deque\n",
    "\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "def create_cluster_from_ordered_list(ordered_categories, split_ratio):\n",
    "    \"\"\"\n",
    "    Converts a list of categories into a cluster dict using split_ratio.\n",
    "    \"\"\"\n",
    "    n = len(ordered_categories)\n",
    "    n_zeros = int(n * split_ratio)\n",
    "    return {\n",
    "        cat: 0 if i < n_zeros else 1\n",
    "        for i, cat in enumerate(ordered_categories)\n",
    "    }\n",
    "\n",
    "def rotating_cluster_generator_shift(categories, split_ratio, steps=1, seed=None):\n",
    "    \"\"\"\n",
    "    Yields clusters by cyclically rotating the category list.\n",
    "\n",
    "    Args:\n",
    "        categories (list): List of category identifiers.\n",
    "        split_ratio (float): Ratio of cluster 0 elements.\n",
    "        seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Yields:\n",
    "        dict: Cluster mapping.\n",
    "    \"\"\"\n",
    "    cat_list = list(categories)\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    random.shuffle(cat_list)\n",
    "    cat_deque = deque(cat_list)\n",
    "\n",
    "    while True:\n",
    "        cluster = create_cluster_from_ordered_list(cat_deque, split_ratio)\n",
    "        cat0 = [cat for cat, c in cluster.items() if c == 0]\n",
    "        cat1 = [cat for cat, c in cluster.items() if c == 1]\n",
    "        yield cluster, cat0, cat1\n",
    "        cat_deque.rotate(-steps)  # rotate left\n",
    "\n",
    "def cluster_categories(device, cnn, n_clusters=2, variance=0.95, data_dir=\"../data\"):\n",
    "    \"\"\"\n",
    "    Clusters base classes using visual features extracted from a CLIP model. Applies PCA to reduce dimensionality\n",
    "    and Agglomerative Clustering on cosine distances to group the categories.\n",
    "\n",
    "    Args:\n",
    "        device (torch.device): The device to run computations on (CPU/GPU).\n",
    "        cnn (str): CLIP model architecture name (e.g., \"ViT-B/32\").\n",
    "        n_clusters (int): Number of clusters to generate.\n",
    "        variance (float): Variance ratio to preserve during PCA.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[int, int], Dict[str, int]]: Two dictionaries mapping class indices and class names to cluster IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize clip model with ViT\n",
    "    clip_model, _ = clip.load(cnn)\n",
    "    clip_model = clip_model.to(device)\n",
    "    resolution = clip_model.visual.input_resolution\n",
    "    train_set, _, _ = get_data(data_dir=data_dir, resolution=resolution)\n",
    "\n",
    "    # split classes into base and novel\n",
    "    base_classes, _ = base_novel_categories(train_set)\n",
    "\n",
    "    # split the three datasets\n",
    "    train_base, _ = split_data(train_set, base_classes)\n",
    "\n",
    "    class_feature = {}\n",
    "    with torch.no_grad():\n",
    "        for c in tqdm(base_classes, desc=\"Processing classes\"):\n",
    "            imgs_c = []\n",
    "            # Create a DataLoader to iterate over the dataset properly\n",
    "            dataloader = torch.utils.data.DataLoader(train_base, batch_size=1, shuffle=False)\n",
    "            for img, label in dataloader:\n",
    "                if label.item() == c:\n",
    "                    imgs_c.append(img.squeeze(0))\n",
    "            features = [\n",
    "                clip_model.encode_image(img.unsqueeze(0).to(device)).cpu().numpy()\n",
    "                for img in imgs_c\n",
    "            ]\n",
    "            class_feature[c] = np.mean(features, axis=0)\n",
    "\n",
    "    # class_ft_array = np.array([class_feature[c][0] for c in base_classes])\n",
    "\n",
    "    cat2idx = {}\n",
    "    idx2cat = {}\n",
    "    class_ft_array = []\n",
    "    for i, c in enumerate(base_classes):\n",
    "        cat2idx[c] = i\n",
    "        idx2cat[i] = c\n",
    "        class_ft_array.append(class_feature[c][0])\n",
    "\n",
    "    pca = PCA(n_components=variance)\n",
    "    X_reduced = pca.fit_transform(class_ft_array)\n",
    "\n",
    "    print(\n",
    "        f\"Reduced feature shape: {X_reduced.shape}, Variance explained: {pca.explained_variance_ratio_.sum()}\"\n",
    "    )\n",
    "\n",
    "    cosine_dist = cosine_distances(X_reduced)\n",
    "    # Step 5: Agglomerative clustering\n",
    "    agglo = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters, metric=\"precomputed\", linkage=\"average\"\n",
    "    )\n",
    "    cluster_labels = agglo.fit_predict(cosine_dist)\n",
    "\n",
    "    cluster_labels = {idx2cat[i]: cluster for i, cluster in enumerate(cluster_labels)}\n",
    "\n",
    "    cluster_labels_text = {\n",
    "        CLASS_NAMES[base_class]: int(cluster)\n",
    "        for base_class, cluster in enumerate(cluster_labels)\n",
    "    }\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return cluster_labels, cluster_labels_text\n",
    "\n",
    "\n",
    "def random_clustering(\n",
    "    n_cluster,\n",
    "    seed=42,\n",
    "    data_dir=\"../data\",\n",
    "    distribution=\"uniform\",\n",
    "    split_ratio=0.7,  # Only for bipartite\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates random cluster assignments for a given number of clusters.\n",
    "\n",
    "    Args:\n",
    "        n_cluster (int): Number of clusters to generate.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        data_dir (str): Directory where the dataset is stored.\n",
    "        distribution (str): Distribution type for cluster assignment. Options are \"uniform\", \"random\", \"sequential\", or \"bipartite\".\n",
    "        split_ratio (float): Percentage of classes in the larger cluster (only used for \"bipartite\").\n",
    "\n",
    "    Returns:\n",
    "        Tuple[\n",
    "            Dict[int, int],        # class_id -> cluster_id\n",
    "            Dict[str, int],        # class_name -> cluster_id\n",
    "            List[int],             # class_ids in cluster 0\n",
    "            List[int],             # class_ids in cluster 1\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    train_set, _, _ = get_data(data_dir=data_dir)\n",
    "    base_classes, _ = base_novel_categories(train_set)\n",
    "\n",
    "    cluster_labels = {}\n",
    "\n",
    "    if distribution == \"uniform\":\n",
    "        shuffled = np.random.permutation(base_classes)\n",
    "        for i, cls in enumerate(shuffled):\n",
    "            cluster_id = i % n_cluster\n",
    "            cluster_labels[cls] = cluster_id\n",
    "\n",
    "    elif distribution == \"random\":\n",
    "        for cls in base_classes:\n",
    "            cluster_id = np.random.choice(range(n_cluster))\n",
    "            cluster_labels[cls] = cluster_id\n",
    "\n",
    "    elif distribution == \"sequential\":\n",
    "        for i, cls in enumerate(base_classes):\n",
    "            cluster_id = i % n_cluster\n",
    "            cluster_labels[cls] = cluster_id\n",
    "\n",
    "    cluster_labels_text = {\n",
    "        CLASS_NAMES[cls]: int(cluster_labels[cls])\n",
    "        for cls in cluster_labels\n",
    "    }\n",
    "\n",
    "    cluster_dict_int = {int(k): v for k, v in cluster_labels.items()}\n",
    "\n",
    "    return cluster_dict_int, cluster_labels_text\n",
    "\n",
    "\n",
    "def conditional_clustering(n_cluster, variance, cnn, device, data_dir=\"../data\"):\n",
    "    \"\"\"\n",
    "    Loads existing cluster labels from disk if available, otherwise computes and saves new cluster assignments.\n",
    "\n",
    "    Args:\n",
    "        n_cluster (int): Number of clusters to generate.\n",
    "        variance (float): Variance ratio to preserve during PCA.\n",
    "        cnn (str): CLIP model architecture name (used for naming output files).\n",
    "        device (torch.device): The device to run computations on.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[int, int], Dict[str, int]]: Dictionaries for integer-labeled and text-labeled cluster assignments.\n",
    "    \"\"\"\n",
    "    cnn_sanitized = cnn.replace(\"/\", \"_\")\n",
    "    save_dir = f\"clustering_split/cluster_labels_{n_cluster}_{variance}_{cnn_sanitized}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    int_categories_path = os.path.join(save_dir, \"int_categories.pkl\")\n",
    "    text_categories_path = os.path.join(save_dir, \"text_categories.pkl\")\n",
    "\n",
    "\n",
    "    # Commented this lines because they are not used in the Jupyter version, they are meant to save and load cluster labels\n",
    "\n",
    "    # if os.path.exists(int_categories_path) and os.path.exists(text_categories_path):\n",
    "    #     print(\"üü© CLUSTERS FILES FOUND. Loading existing cluster labels...\")\n",
    "    #     with open(int_categories_path, \"rb\") as f:\n",
    "    #         cluster_labels = pickle.load(f)\n",
    "    #         cluster_dict_int = {int(k): v for k, v in cluster_labels.items()}\n",
    "    #     with open(text_categories_path, \"rb\") as f:\n",
    "    #         cluster_labels_text = pickle.load(f)\n",
    "\n",
    "    # else:\n",
    "    print(\"üüß NO CLUSTERS FILES FOUND (jupyter version without saves). Creating cluster labels...\")\n",
    "    # cluster the base classes\n",
    "    cluster_labels, cluster_labels_text = cluster_categories(\n",
    "        device, n_clusters=n_cluster, variance=variance, cnn=cnn, data_dir=data_dir\n",
    "    )\n",
    "    cluster_dict_int = {int(k): v for k, v in cluster_labels.items()}\n",
    "\n",
    "    ## Commented this lines because they are not used in the Jupyter version, they are meant to save and load cluster labels\n",
    "\n",
    "    # with open(int_categories_path, \"wb\") as f:\n",
    "    #     pickle.dump(cluster_labels, f)\n",
    "    # with open(text_categories_path, \"wb\") as f:\n",
    "    #     pickle.dump(cluster_labels_text, f)\n",
    "    # Count samples in each cluster\n",
    "    cluster_counts = Counter(cluster_dict_int.values())\n",
    "    for cluster_id in range(n_cluster):\n",
    "        print(f\"Cluster {cluster_id} count: {cluster_counts.get(cluster_id, 0)}\")\n",
    "\n",
    "    return cluster_dict_int, cluster_labels_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e8f54c",
   "metadata": {
    "id": "f5e8f54c"
   },
   "source": [
    "### KL Divergence Teacher Guidance (v1)\n",
    "\n",
    "In this modification, we introduce a form of teacher-student training where CLIP's zero-shot predictions [^3] are used as soft targets to guide CoCoOp. This is motivated by the observation that CLIP-zero-shot can outperform CoCoOp on certain datasets like Oxford Flowers for novel classes, which is also our study case, and the fact that both models use the same CLIP backbone ensures architectural consistency.\n",
    "\n",
    "#### Loss Formulation  \n",
    "The training objective augments the standard CoCoOp cross-entropy loss with a Kullback‚ÄìLeibler (KL) divergence term between CoCoOp‚Äôs output and CLIP-zero-shot predictions:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{CE}} + \\delta \\cdot \\mathcal{L}_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}_{\\text{KL}}$ is the KL divergence between the softmax outputs of CoCoOp and the CLIP zero-shot classifier, and $\\delta$ is a scaling hyperparameter.\n",
    "\n",
    "#### Batch Splitting Strategy  \n",
    "To apply the KL loss meaningfully, we split each training batch into two subsets:\n",
    "\n",
    "- **Pseudo-base:** Samples belonging to 70% of the categories in the batch are used to compute the standard cross-entropy loss.\n",
    "- **Pseudo-novel:** Samples belonging to the remaining 30% of the categories in the batch are used to compute the KL divergence loss.\n",
    "\n",
    "This split is randomized at each batch, allowing CoCoOp to be trained with a blend of direct supervision and knowledge distillation from CLIP zero-shot predictions [^3].\n",
    "\n",
    "#### Intuition  \n",
    "The KL divergence term encourages CoCoOp to preserve the generalization strengths of CLIP‚Äôs zero-shot model, particularly for novel or underrepresented classes. By mimicking CLIP's soft predictions on part of the data, the prompt learner is gently nudged toward representations that maintain strong performance outside the training distribution.\n",
    "\n",
    "\n",
    "\n",
    "## KL Regularization (v1)\n",
    "\n",
    "**Key Aspects:** Pseudo-novel splitting within batch; loss fusion in a single backward pass.  \n",
    "**Constraints:** No memory separation; subject to class imbalance due to random sampling.\n",
    "\n",
    "This variant augments CoCoOp training with a KL divergence term between the model‚Äôs predictions and frozen CLIP zero-shot outputs. It uses a single dataloader that returns mini-batches sampled from  \n",
    "$$\n",
    "\\mathcal{D}_{\\text{seen}}^{\\text{train}}\n",
    "$$\n",
    "with each batch split into pseudo-base and pseudo-novel subsets inside the collate function (typically at a fixed 70/30 ratio).\n",
    "\n",
    "Both losses ‚Äî cross-entropy on the pseudo-base and KL divergence on the pseudo-novel ‚Äî are computed jointly and combined before the backward pass. This enables efficient optimization but introduces stochasticity in class composition and potential imbalance in the pseudo-novel subset. These effects can reduce training stability and reproducibility compared to more structured variants.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Procedure (repeat until convergence):\n",
    "\n",
    "1. Sample an image batch:  \n",
    "   $$\n",
    "   \\mathbf{X}_{\\text{b}} \\sim \\mathcal{D}_{\\text{seen}}^{\\text{train}}\n",
    "   $$\n",
    "\n",
    "2. Divide the batch into pseudo-novel and pseudo-base subsets:  \n",
    "   $$\n",
    "   (\\mathbf{x}_{\\text{bn}}, y_{\\text{bn}}),\\; (\\mathbf{x}_{\\text{bb}}, y_{\\text{bb}}) \\sim \\mathbf{X}_{\\text{b}}\n",
    "   $$\n",
    "\n",
    "3. Compute CoCoOp cross-entropy loss on the pseudo-base subset:  \n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{CE}} = \\text{CE} \\left( \\text{CoCoOp}_{\\theta, p_1, \\ldots, p_M}(\\mathbf{x}_{\\text{bb}}), y_{\\text{bb}} \\right)\n",
    "   $$\n",
    "\n",
    "4. Compute CoCoOp softmax predictions on the pseudo-novel subset:  \n",
    "   $$\n",
    "   y_{\\text{student}} = \\text{CoCoOp}_{\\theta, p_1, \\ldots, p_M}(\\mathbf{x}_{\\text{bn}})\n",
    "   $$\n",
    "\n",
    "5. Compute frozen CLIP softmax predictions on the pseudo-novel subset:  \n",
    "   $$\n",
    "   y_{\\text{teacher}} = \\text{CLIP}(\\mathbf{x}_{\\text{bn}})\n",
    "   $$\n",
    "\n",
    "6. Compute student-teacher KL divergence:  \n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{KL}} = D_{\\text{KL}} \\left[ y_{\\text{teacher}} \\| y_{\\text{student}} \\right]\n",
    "   $$\n",
    "\n",
    "7. Compute total loss combining cross-entropy and weighted KL divergence:  \n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{CE}} + \\lambda_{\\text{KL}} \\cdot \\mathcal{L}_{\\text{KL}}\n",
    "   $$\n",
    "\n",
    "8. Update parameters $\\theta$ and context vectors $p_1, \\ldots, p_M$ via backpropagation on $\\mathcal{L}_{\\text{total}}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7d7b47",
   "metadata": {
    "id": "ef7d7b47"
   },
   "outputs": [],
   "source": [
    "# From: ./training_systems/training_methods/KLCoCoOp.py\n",
    "\"\"\"\n",
    "This module defines the KLCoCoOp training method, which combines standard cross-entropy loss with KL divergence\n",
    "between a model's predictions and a frozen teacher (e.g., CLIP) to enhance generalization to novel classes.\n",
    "\"\"\"\n",
    "import random\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from training_systems.core.TrainingMethod import TrainingMethod\n",
    "\n",
    "# from utils import AverageMeter,ContiguousLabelDataset, get_kl_loss\n",
    "# from utils.datasets import CLASS_NAMES\n",
    "\n",
    "\n",
    "class KLCoCoOp(TrainingMethod):\n",
    "    \"\"\"\n",
    "    KLCoCoOp training method combining cross-entropy classification on pseudo-base samples and KL divergence\n",
    "    loss on pseudo-novel samples. It encourages transferability and generalization by mixing base and novel categories.\n",
    "\n",
    "    Attributes:\n",
    "        lambda_kl (float): Weight for the KL divergence loss component.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: Any,\n",
    "            optimizer: Any,\n",
    "            lambda_kl,\n",
    "            debug: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__(model, optimizer, \"Base CoCoOp + KL\", debug)\n",
    "        self.lambda_kl = lambda_kl\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Initializes training metrics including total, cross-entropy, KL divergence losses, and classification accuracy.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: Dictionary of metric names mapped to their respective AverageMeter instances.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"total_loss_metric\": AverageMeter(),\n",
    "            \"ce_loss_metric\": AverageMeter(),\n",
    "            \"kl_loss_metric\": AverageMeter(),\n",
    "            \"accuracy_metric\": AverageMeter(),\n",
    "        }\n",
    "\n",
    "    def get_data_loader(self, dataset: ContiguousLabelDataset, batch_size: int) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns a DataLoader that splits each batch into pseudo-base and pseudo-novel subsets.\n",
    "\n",
    "        Args:\n",
    "            dataset (ContiguousLabelDataset): The dataset used for training.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch DataLoader with a custom collate function to separate base and novel samples.\n",
    "        \"\"\"\n",
    "\n",
    "        def custom_collate(batch):\n",
    "            base_samples = []\n",
    "            novel_samples = []\n",
    "            targets_in_batch = list(set([target for _, target in batch]))\n",
    "            random.shuffle(targets_in_batch)\n",
    "            split_idx = int(0.7 * len(targets_in_batch))\n",
    "            pseudo_base_ids = targets_in_batch[:split_idx]\n",
    "            pseudo_novel_ids = targets_in_batch[split_idx:]\n",
    "            for img, label in batch:\n",
    "                if label in pseudo_base_ids:\n",
    "                    base_samples.append((img, label))\n",
    "                elif label in pseudo_novel_ids:\n",
    "                    novel_samples.append((img, label))\n",
    "            return base_samples, novel_samples\n",
    "\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            collate_fn=custom_collate,\n",
    "        )\n",
    "\n",
    "    def forward_backward(\n",
    "            self,\n",
    "            sample,\n",
    "            batch_idx,\n",
    "            metrics: Dict[str, AverageMeter],\n",
    "            dataset: ContiguousLabelDataset,\n",
    "            accumulation_steps: int = 1,\n",
    "            step: int = 0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Performs forward and backward passes, computing CE loss on pseudo-base and KL loss on pseudo-novel samples.\n",
    "\n",
    "        Args:\n",
    "            sample (Tuple[List[Tuple[Tensor, int]], List[Tuple[Tensor, int]]]): Tuple of pseudo-base and pseudo-novel batches.\n",
    "            batch_idx (int): Current batch index during training.\n",
    "            metrics (Dict[str, AverageMeter]): Dictionary to update with the training metrics.\n",
    "            dataset (ContiguousLabelDataset): Dataset object used for KL divergence lookup.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Scalar values of total loss, CE loss, KL loss, and CE accuracy.\n",
    "        \"\"\"\n",
    "        # Load data into GPU\n",
    "        base_batch, novel_batch = sample\n",
    "\n",
    "        if not base_batch or not novel_batch:\n",
    "            return {\n",
    "                metric: val.avg\n",
    "                for metric, val in metrics.items()\n",
    "            }\n",
    "\n",
    "        # === Pseudo-base: cross-entropy ===\n",
    "        inputs_base = torch.stack([img for img, _ in base_batch]).to(self.device)\n",
    "        targets_base = torch.tensor([lbl for _, lbl in base_batch]).to(self.device)\n",
    "\n",
    "\n",
    "        categories_base_tensor = [dataset.idx2cat[int(c.item())] for c in list(set(targets_base))]\n",
    "        remapped_class_names = [ CLASS_NAMES[ c ] for c in categories_base_tensor ]\n",
    "\n",
    "        target_remapping = {cat:idx for idx, cat in enumerate(categories_base_tensor)}\n",
    "        target_original = [dataset.idx2cat[int(c.item())] for c in targets_base]\n",
    "        target_remapped = torch.tensor([target_remapping[c] for c in target_original]).to(self.device)\n",
    "\n",
    "        with self.model.temporary_classnames(remapped_class_names):\n",
    "            self.model.train()\n",
    "            logits_base, loss_ce = self.model(inputs_base, target_remapped)\n",
    "\n",
    "        # === Pseudo-novel: KL divergence with frozen CLIP ===\n",
    "        self.model.eval()  # needed to disable dropout etc.\n",
    "        inputs_novel = torch.stack([img for img, _ in novel_batch]).to(self.device)\n",
    "        targets_novel = [lbl for _, lbl in novel_batch]\n",
    "\n",
    "        kl_loss = get_kl_loss(self.device, inputs_novel, self.model, targets_novel, dataset)\n",
    "\n",
    "        # === Combine losses ===\n",
    "        total_loss = loss_ce + self.lambda_kl * kl_loss\n",
    "\n",
    "        total_loss = total_loss / accumulation_steps\n",
    "        total_loss.backward()\n",
    "\n",
    "        # optimizer step every `accumulation_steps` steps\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        batch_size_total = inputs_base.size(0) + inputs_novel.size(0)\n",
    "\n",
    "        metrics[\"total_loss_metric\"].update(total_loss.item(), n=batch_size_total)\n",
    "        metrics[\"ce_loss_metric\"].update(loss_ce.item(), n=inputs_base.size(0))\n",
    "        metrics[\"kl_loss_metric\"].update(kl_loss.item(), n=inputs_novel.size(0))\n",
    "\n",
    "        _, predicted = logits_base.max(dim=1)\n",
    "        correct = (predicted == target_remapped).sum().item()\n",
    "        total = target_remapped.size(0)\n",
    "        metrics[\"accuracy_metric\"].update(correct, n=total, raw=True)\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"ce_loss\": loss_ce.item(),\n",
    "            \"ce_accuracy\": correct / target_remapped.size(0),\n",
    "            \"kl_loss\": kl_loss.item(),\n",
    "        }\n",
    "\n",
    "    def debug_metrics_to_pbar_args(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Prepares debug metrics for visualization in progress bars or logs.\n",
    "\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Dictionary of debug metrics from the current step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Same metrics for direct display.\n",
    "        \"\"\"\n",
    "        return debug_metrics\n",
    "\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Returns the average values of tracked metrics after a training step.\n",
    "\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Metric dictionary to extract averages from.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Averages of total loss, accuracy, CE loss, and KL loss.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            metrics[\"total_loss_metric\"].avg,\n",
    "            metrics[\"accuracy_metric\"].avg,\n",
    "            metrics[\"ce_loss_metric\"].avg,\n",
    "            metrics[\"kl_loss_metric\"].avg,\n",
    "        ]\n",
    "\n",
    "    def update_lambda_kl(self, lambda_kl):\n",
    "        self.lambda_kl = lambda_kl\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26edd2e9",
   "metadata": {
    "id": "26edd2e9"
   },
   "source": [
    "#### KL(v1) Loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b9bba",
   "metadata": {
    "id": "a56b9bba"
   },
   "outputs": [],
   "source": [
    "# From: ./utils/kl.py\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "# from utils.datasets import CLASS_NAMES\n",
    "\n",
    "\n",
    "def get_kl_loss(device, inputs_novel, model, targets_novel, tmp_dataset):\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between the student model's predictions and the CLIP model's predictions\n",
    "    for a batch of novel class images.\n",
    "\n",
    "    Args:\n",
    "        device (torch.device): The device (CPU or CUDA) to perform computation on.\n",
    "        inputs_novel (Tensor): A batch of input images from novel classes.\n",
    "        model (nn.Module): The student model that includes a CLIP backbone and prompt learner.\n",
    "        targets_novel (List[int]): Target labels corresponding to the novel class inputs.\n",
    "        tmp_dataset (ContiguousLabelDataset): Dataset wrapper with label-to-category mappings.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A scalar tensor representing the KL divergence loss.\n",
    "    \"\"\"\n",
    "    targets_novel_tensor = torch.tensor(targets_novel).to(device) if isinstance(targets_novel, list) else targets_novel\n",
    "    categories_novel_tensor = [tmp_dataset.idx2cat[c.item()] for c in list(set(targets_novel_tensor))]\n",
    "    # print(f\"input novel shape: {inputs_novel.shape} novel base: {targets_novel_tensor.shape}\")\n",
    "    with torch.no_grad():\n",
    "        image_features_clip = model.clip_model.encode_image(inputs_novel)\n",
    "        image_features_clip = image_features_clip / image_features_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "        text_inputs = clip.tokenize(\n",
    "            [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories_novel_tensor]\n",
    "        ).to(device)\n",
    "\n",
    "        text_features_clip = model.clip_model.encode_text(text_inputs)\n",
    "        text_features_clip = text_features_clip / text_features_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        clip_logits = image_features_clip @ text_features_clip.T\n",
    "\n",
    "    remapped_class_names = [ CLASS_NAMES[ c ] for c in categories_novel_tensor ]\n",
    "\n",
    "    # targets_novel_tensor contains contiguous indices (0, 1, 2, ...)\n",
    "    # categories_novel_tensor should be the original class labels for the current batch\n",
    "\n",
    "    # No need to remap targets; they are already correct\n",
    "    target_remapping = {cat:idx for idx, cat in enumerate(categories_novel_tensor)}\n",
    "    target_original = [tmp_dataset.idx2cat[c.item()] for c in targets_novel_tensor]\n",
    "    target_remapped = torch.tensor([target_remapping[c] for c in target_original]).to(device)\n",
    "\n",
    "    with model.temporary_classnames(remapped_class_names):\n",
    "        model.train()\n",
    "        student_logits, student_loss = model(inputs_novel, target_remapped)  # [B, num_classes]\n",
    "        kl_loss = torch.nn.functional.kl_div(\n",
    "            torch.nn.functional.log_softmax(student_logits, dim=-1),\n",
    "            torch.nn.functional.softmax(clip_logits, dim=-1),\n",
    "            reduction=\"batchmean\"\n",
    "        )\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19c9d46",
   "metadata": {
    "id": "c19c9d46"
   },
   "source": [
    "### KL Divergence with Rotating Class Splits (v2)\n",
    "\n",
    "This variant extends the KLv1 method by introducing a two-stage training strategy that separates the dataset into two disjoint subsets and optimizes them sequentially. The goal is to better mimic a base/novel split and train CoCoOp with a curriculum-like structure using CLIP zero-shot outputs as guidance [^3].\n",
    "\n",
    "#### Split-Based Training  \n",
    "We partition the base training dataset into two subsets:\n",
    "\n",
    "- **pseudo-base-klv2 ($\\mathcal{D}^\\text{training}_\\text{seen}$):** Used for supervised learning with standard cross-entropy loss.\n",
    "- **pseudo-novel-klv2 ($\\mathcal{D}^\\text{training}_\\text{unseen}$):** Used for knowledge distillation via KL divergence from CLIP zero-shot predictions.\n",
    "\n",
    "During each epoch, the optimizer first performs updates on pseudo-base-klv2 using only the cross-entropy loss. After completing this pass, the model is trained on pseudo-novel-klv2 using only the KL divergence loss:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{CE-step}} = \\mathcal{L}_{\\text{CE}}, \\quad \\mathcal{L}_{\\text{KL-step}} = \\delta \\cdot \\mathcal{L}_{\\text{KL}}\n",
    "$$\n",
    "\n",
    "We deliberately avoid combining CE and KL in a single backward pass, as it leads to higher memory consumption. This separation simplifies optimization but may introduce gradient interference issues since both losses are not co-optimized in a shared context. Nevertheless, in practice, this sequential training proved more stable under memory constraints.\n",
    "\n",
    "#### Rotating Class Assignments  \n",
    "To prevent the model from overfitting to a fixed base/novel class partition, we periodically reshuffle the class assignments across epochs using a cyclic rotation strategy. The base class list is shuffled and split using a predefined ratio (e.g., 70% pseudo-base, 30% pseudo-novel), and then rotated after each remixing step.\n",
    "\n",
    "The number of rotation steps is calculated as:\n",
    "\n",
    "$$\n",
    "\\texttt{rotation\\_steps} = |\\mathcal{C}_\\text{unseen}| = \\left\\lfloor |\\mathcal{C}_\\text{base}| \\cdot (1 - \\texttt{pseudo\\_base\\_ratio}) \\right\\rfloor\n",
    "$$\n",
    "\n",
    "This ensures that after each rotation:\n",
    "\n",
    "$$\n",
    "\\mathcal{C}'_\\text{unseen} \\cap \\mathcal{C}_\\text{unseen} = \\emptyset\n",
    "$$\n",
    "\n",
    "where $\\mathcal{C}'_\\text{unseen}$ are the new `pseudo-novel-klv2` classes after rotation.\n",
    "\n",
    "#### Intuition  \n",
    "This strategy gradually exposes the model to different subsets of classes as pseudo-novel categories, encouraging it to generalize beyond any fixed training split. By rotating which classes are treated as \"novel\" across epochs, the model is repeatedly challenged to adapt to new class combinations. This setup mimics the conditions of generalization to unseen classes, which is the core goal of CoCoOp.\n",
    "\n",
    "The KL divergence loss acts as a soft guidance signal from CLIP‚Äôs zero-shot predictions [^3], helping the model stay aligned with more generalizable representations. Meanwhile, the rotation strategy prevents overfitting to a specific partition, ensuring balanced coverage and making training more robust over time.\n",
    "\n",
    "[^1]: Radford, A., et al. (2021). *Learning Transferable Visual Models From Natural Language Supervision*. [arXiv:2103.00020](https://arxiv.org/abs/2103.00020)\n",
    "\n",
    "\n",
    "### KL (v2) - Implementation\n",
    "\n",
    "**Key Aspects:** Dual dataloaders; controlled class remixing with rotation; label remapping for KL.  \n",
    "**Constraints:** Increased implementation complexity; KL and CE losses optimized separately.\n",
    "\n",
    "This version improves control over the pseudo-base and pseudo-novel composition by using two distinct dataloaders. One loader provides samples for cross-entropy loss, while the other supplies examples for KL divergence. A rotation-based mechanism (implemented via a `deque`) periodically shifts the set of classes assigned to the pseudo-novel split, ensuring coverage over time.\n",
    "\n",
    "To ensure consistent KL computation, pseudo-novel labels are remapped to contiguous indices, and the corresponding CLIP logits are computed on-the-fly. Since CE and KL losses operate on independent batches, gradients are accumulated in two separate backward passes per step. This setup allows better balancing, interpretability, and reproducibility at the cost of increased computational and memory overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Procedure (repeat until convergence):\n",
    "\n",
    "1. **Phase 1: Standard CoCoOp training on seen batch**\n",
    "\n",
    "   Sample batch:  \n",
    "   $$\n",
    "   (\\mathbf{x}_{\\text{bb}}, \\mathbf{y}_{\\text{bb}}) \\sim \\mathcal{D}_{\\text{seen}}^{\\text{train}}\n",
    "   $$\n",
    "\n",
    "   Apply base CoCoOp training procedure (see previous section) on $(\\mathbf{x}_{\\text{bb}}, \\mathbf{y}_{\\text{bb}})$.\n",
    "\n",
    "2. **Phase 2: KL divergence on unseen batch**\n",
    "\n",
    "   Sample batch:  \n",
    "   $$\n",
    "   (\\mathbf{x}_{\\text{bn}}, \\mathbf{y}_{\\text{bn}}) \\sim \\mathcal{D}_{\\text{unseen}}^{\\text{train}}\n",
    "   $$\n",
    "\n",
    "   Compute frozen CLIP teacher logits:  \n",
    "   $$\n",
    "   y_{\\text{teacher}} = \\text{CLIP}(\\mathbf{x}_{\\text{bn}})\n",
    "   $$\n",
    "\n",
    "   Compute CoCoOp student logits:  \n",
    "   $$\n",
    "   y_{\\text{student}} = \\text{CoCoOp}_{\\theta, p_1, \\ldots, p_M}(\\mathbf{x}_{\\text{bn}})\n",
    "   $$\n",
    "\n",
    "   Compute KL divergence loss:  \n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{KL}} = D_{\\text{KL}} \\left( y_{\\text{teacher}} \\| y_{\\text{student}} \\right)\n",
    "   $$\n",
    "\n",
    "   Apply weighting factor:  \n",
    "   $$\n",
    "   \\mathcal{L}_{\\text{KL}} \\leftarrow \\lambda_{\\text{KL}} \\cdot \\mathcal{L}_{\\text{KL}}\n",
    "   $$\n",
    "\n",
    "   Update parameters $\\theta$ and context vectors $p_1, \\ldots, p_M$ via backpropagation on $\\mathcal{L}_{\\text{KL}}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f61b81",
   "metadata": {
    "id": "73f61b81"
   },
   "outputs": [],
   "source": [
    "# From: ./training_systems/training_methods/KLCoCoOpV2.py\n",
    "\"\"\"\n",
    "This module defines the KLCoCoOp training method, which combines standard cross-entropy loss with KL divergence\n",
    "between a model's predictions and a frozen teacher (e.g., CLIP) to enhance generalization to novel classes.\n",
    "\"\"\"\n",
    "import random\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# from training_systems.core import DoubleDatasetTrainingMethod\n",
    "\n",
    "# from utils import AverageMeter,CLASS_NAMES, ContiguousLabelDataset, get_kl_loss\n",
    "\n",
    "\n",
    "class KLCoCoOpV2(DoubleDatasetTrainingMethod):\n",
    "    \"\"\"\n",
    "    KLCoCoOp training method combining cross-entropy classification on pseudo-base samples and KL divergence\n",
    "    loss on pseudo-novel samples. It encourages transferability and generalization by mixing base and novel categories.\n",
    "\n",
    "    Attributes:\n",
    "        lambda_kl (float): Weight for the KL divergence loss component.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: Any,\n",
    "            optimizer: Any,\n",
    "            lambda_kl,\n",
    "            debug: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(model, optimizer, \"Base CoCoOp + KL\", debug)\n",
    "        self.lambda_kl = lambda_kl\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] Initialized with lambda_kl={self.lambda_kl}, device={self.device}\")\n",
    "\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Initializes training metrics including total, cross-entropy, KL divergence losses, and classification accuracy.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: Dictionary of metric names mapped to their respective AverageMeter instances.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(\"[KLCoCoOpV2] Initializing metrics.\")\n",
    "        return {\n",
    "            \"ce_loss_metric\": AverageMeter(),\n",
    "            \"kl_loss_metric\": AverageMeter(),\n",
    "            \"ce_accuracy_metric\": AverageMeter(),\n",
    "        }\n",
    "\n",
    "    def get_data_loader1(self, pseudo_base: ContiguousLabelDataset, batch_size: int) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns a DataLoader that splits each batch into pseudo-base and pseudo-novel subsets.\n",
    "\n",
    "        Args:\n",
    "            dataset (ContiguousLabelDataset): The dataset used for training.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch DataLoader with a custom collate function to separate base and novel samples.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] Creating DataLoader1 for pseudo_base with batch_size={batch_size}.\")\n",
    "        return DataLoader(\n",
    "            pseudo_base,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "        )\n",
    "\n",
    "    def get_data_loader2(self, pseudo_novel_dataset: ContiguousLabelDataset, batch_size: int) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns a DataLoader that splits each batch into pseudo-base and pseudo-novel subsets.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] Creating DataLoader2 for pseudo_novel with batch_size={batch_size}.\")\n",
    "        return DataLoader(pseudo_novel_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "\n",
    "    def forward_backward1(\n",
    "            self,\n",
    "            sample,\n",
    "            batch_idx,\n",
    "            metrics: Dict[str, AverageMeter],\n",
    "            dataset: ContiguousLabelDataset,\n",
    "            classes: list[int]\n",
    "    ) -> Dict[str, float]:\n",
    "        if self.debug and batch_idx < 3:\n",
    "            print(f\"[KLCoCoOpV2] forward_backward1: batch_idx={batch_idx}\")\n",
    "            print(f\"[KLCoCoOpV2] Sample type: {type(sample)}\")\n",
    "        # Load data into GPU\n",
    "        inputs, targets = sample\n",
    "        # === Pseudo-base: cross-entropy ===\n",
    "        inputs_base = inputs.to(self.device)\n",
    "        targets_base = targets.to(self.device)\n",
    "        # Use remapped class names to match the remapped labels\n",
    "        # classes contains original category indices, but we need to map them to 0-based indices\n",
    "        # The ContiguousLabelDataset remaps labels to 0-based indices, so we need class names in the same order\n",
    "        # Use the remapped indices to get class names in the correct order\n",
    "        remapped_class_names = [ CLASS_NAMES[ dataset.idx2cat[i] ] for i in range(len(dataset.idx2cat)) ]\n",
    "        assert set(classes) == set(dataset.idx2cat.values()), \\\n",
    "            f\"Split classes ({classes}) != dataset labels ({list(dataset.idx2cat.values())})\"\n",
    "        with self.model.temporary_classnames(remapped_class_names):\n",
    "            logits_base, loss_ce = self.model(inputs_base, targets_base)\n",
    "            # === Combine losses ===\n",
    "            if self.debug and batch_idx < 3:\n",
    "                print(f\"[KLCoCoOpV2] LOGITS shape: {logits_base.shape}, TARGETS shape: {targets_base.shape}\")\n",
    "                print(f\"[KLCoCoOpV2] CE loss: {loss_ce.item()}\")\n",
    "                print(f\"[KLCoCoOpV2] Remapped class names: {remapped_class_names}\")\n",
    "                print(f\"[KLCoCoOpV2] Original classes: {classes}\")\n",
    "                print(f\"[KLCoCoOpV2] Targets: {targets_base}\")\n",
    "                print(f\"[KLCoCoOpV2] Logits max: {logits_base.max(dim=1)[1]}\")\n",
    "                print(f\"[KLCoCoOpV2] Dataset cat2idx: {dataset.cat2idx}\")\n",
    "                print(f\"[KLCoCoOpV2] Dataset idx2cat: {dataset.idx2cat}\")\n",
    "                print(f\"[KLCoCoOpV2] Expected class names order: {[CLASS_NAMES[dataset.idx2cat[i]] for i in range(len(classes))]}\")\n",
    "                print(f\"[KLCoCoOpV2] Model class names: {[CLASS_NAMES[c] for c in classes]}\")\n",
    "            loss_ce.backward()\n",
    "\n",
    "            self.optimizer_step()\n",
    "            batch_size_total = inputs_base.size(0)\n",
    "\n",
    "            metrics[\"ce_loss_metric\"].update(loss_ce.item(), n=batch_size_total)\n",
    "\n",
    "            _, predicted = logits_base.max(dim=1)\n",
    "            correct = (predicted == targets_base).sum().item()\n",
    "            total = targets_base.size(0)\n",
    "            metrics[\"ce_accuracy_metric\"].update(correct, n=total, raw=True)\n",
    "\n",
    "            if self.debug and batch_idx < 3:\n",
    "                print(f\"[KLCoCoOpV2] Batch accuracy: {correct}/{total} = {correct/total if total > 0 else 0}\")\n",
    "        return {\n",
    "            \"ce_loss\": loss_ce.item(),\n",
    "            \"accuracy\": correct / targets_base.size(0),\n",
    "        }\n",
    "\n",
    "    def forward_backward2(\n",
    "            self,\n",
    "            sample,\n",
    "            batch_idx,\n",
    "            metrics: Dict[str, AverageMeter],\n",
    "            dataset: ContiguousLabelDataset,\n",
    "            classes: list[int]\n",
    "    ) -> Dict[str, float]:\n",
    "        # Use remapped class names to match the remapped labels\n",
    "        # classes contains original category indices, but we need to map them to 0-based indices\n",
    "        # The ContiguousLabelDataset remaps labels to 0-based indices, so we need class names in the same order\n",
    "        # Use the remapped indices to get class names in the correct order\n",
    "        remapped_class_names = [ CLASS_NAMES[ dataset.idx2cat[i] ] for i in range(len(dataset.idx2cat)) ]\n",
    "        pseudo_novel_class_names = remapped_class_names\n",
    "        if self.debug and batch_idx < 3:\n",
    "            print(f\"[KLCoCoOpV2] forward_backward2: batch_idx={batch_idx}\")\n",
    "            print(f\"[KLCoCoOpV2] Sample type: {type(sample)}; Sample len: {len(sample) if hasattr(sample, '__len__') else 'N/A'}\")\n",
    "        # Load data into GPU\n",
    "        inputs_novel, targets_novel = sample\n",
    "        # === Pseudo-novel: KL divergence with frozen CLIP ===\n",
    "        inputs_novel = inputs_novel.to(self.device)\n",
    "        targets_novel = targets_novel.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features_clip = self.model.clip_model.encode_image(inputs_novel)\n",
    "            image_features_clip = image_features_clip / image_features_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            #category_idxs = [dataset.idx2cat[c.item()] for c in list(set(targets_novel))] # type: ignore\n",
    "\n",
    "            text_inputs = clip.tokenize(\n",
    "                [f\"a photo of a {cn}, a type of flower.\" for cn in pseudo_novel_class_names]\n",
    "            ).to(self.device)\n",
    "\n",
    "            text_features_clip = self.model.clip_model.encode_text(text_inputs)\n",
    "            text_features_clip = text_features_clip / text_features_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            clip_logits = image_features_clip @ text_features_clip.T\n",
    "\n",
    "\n",
    "        self.model.train()\n",
    "        with self.model.temporary_classnames(remapped_class_names):\n",
    "            student_logits, student_loss = self.model(inputs_novel, targets_novel)\n",
    "            kl_loss = torch.nn.functional.kl_div(\n",
    "                torch.nn.functional.log_softmax(student_logits, dim=-1),\n",
    "                torch.nn.functional.softmax(clip_logits, dim=-1),\n",
    "                reduction=\"batchmean\"\n",
    "            )\n",
    "\n",
    "        # === Combine losses ===\n",
    "            kl_loss = self.lambda_kl * kl_loss\n",
    "\n",
    "            if self.debug and batch_idx < 3:\n",
    "                print(f\"[KLCoCoOpV2] KL loss (weighted): {kl_loss.item()}\")\n",
    "            kl_loss.backward()\n",
    "\n",
    "            self.optimizer_step()\n",
    "\n",
    "            metrics[\"kl_loss_metric\"].update(kl_loss.item(), n=inputs_novel.size(0))\n",
    "\n",
    "        return {\n",
    "            \"kl_loss\": kl_loss.item(),\n",
    "        }\n",
    "\n",
    "    def debug_metrics_to_pbar_args1(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Prepares debug metrics for visualization in progress bars or logs.\n",
    "\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Dictionary of debug metrics from the current step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Same metrics for direct display.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] debug_metrics_to_pbar_args1: {debug_metrics}\")\n",
    "        return debug_metrics\n",
    "\n",
    "    def debug_metrics_to_pbar_args2(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Prepares debug metrics for visualization in progress bars or logs.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] debug_metrics_to_pbar_args2: {debug_metrics}\")\n",
    "        return debug_metrics\n",
    "\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Returns the average values of tracked metrics after a training step.\n",
    "\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Metric dictionary to extract averages from.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Averages of total loss, accuracy, CE loss, and KL loss.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] training_step_return: KL={metrics['kl_loss_metric'].avg}, CE={metrics['ce_loss_metric'].avg}, Acc={metrics['ce_accuracy_metric'].avg}\")\n",
    "        return [\n",
    "            metrics[\"kl_loss_metric\"].avg,\n",
    "            metrics[\"ce_loss_metric\"].avg,\n",
    "            metrics[\"ce_accuracy_metric\"].avg,\n",
    "        ]\n",
    "\n",
    "    def update_lambda_kl(self, lambda_kl):\n",
    "        \"\"\"\n",
    "        Update the lambda_kl value used for KL loss weighting.\n",
    "\n",
    "        Args:\n",
    "            lambda_kl (float): New lambda_kl value.\n",
    "        \"\"\"\n",
    "        self.lambda_kl = lambda_kl\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f85c2b",
   "metadata": {
    "id": "e2f85c2b"
   },
   "source": [
    "## Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685fb012",
   "metadata": {
    "id": "685fb012"
   },
   "source": [
    "### Eval Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc16ada",
   "metadata": {
    "id": "1bc16ada"
   },
   "outputs": [],
   "source": [
    "# From: ./training_systems/evaluation_methods/EvalStep.py\n",
    "\n",
    "from typing import Dict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# from training_systems.core import EvaluationMethod\n",
    "# from utils import ContiguousLabelDataset, CLASS_NAMES, AverageMeter\n",
    "\n",
    "\n",
    "class EvalStep(EvaluationMethod):\n",
    "    \"\"\"\n",
    "    Generic evaluation step for models that support temporary class name modification.\n",
    "    \"\"\"\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataset, classnames, desc_add=\"\") -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate model performance on the provided dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset for evaluation.\n",
    "            new_classnames (list, optional): New class names to apply temporarily.\n",
    "            desc_add (str): Suffix to append to tqdm description.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary containing average loss and accuracy.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        loss_meter = AverageMeter()\n",
    "        accuracy_meter = AverageMeter()\n",
    "        tmp_dataset = ContiguousLabelDataset(dataset, classnames)\n",
    "        dataloader = DataLoader(tmp_dataset, batch_size=self.batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "        remapped_classnames = [ CLASS_NAMES[ tmp_dataset.idx2cat[i] ] for i in range(len(tmp_dataset.idx2cat)) ]\n",
    "        with self.model.temporary_classnames(remapped_classnames):\n",
    "            self.walk(loss_meter, accuracy_meter, dataloader, desc_add)\n",
    "\n",
    "        return {\"loss\": loss_meter.avg, \"accuracy\": accuracy_meter.avg}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def walk(self, loss_meter, accuracy_meter, dataloader, desc_add=\"\"):\n",
    "        \"\"\"\n",
    "        Perform the evaluation loop over the dataset.\n",
    "\n",
    "        Args:\n",
    "            loss_meter: Tracks average loss.\n",
    "            accuracy_meter: Tracks average accuracy.\n",
    "            dataloader: DataLoader to iterate over.\n",
    "            desc_add (str): Additional string to append to tqdm description.\n",
    "        \"\"\"\n",
    "        for images, targets in tqdm(dataloader, desc=\"Validation\" + desc_add, position=1, leave=False):\n",
    "            images = images.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            logits = self.model(images)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            correct = (predictions == targets).sum().item()\n",
    "            loss_meter.update(loss.item(), n=targets.size(0))\n",
    "            accuracy_meter.update(correct, n=targets.size(0), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68beea03",
   "metadata": {
    "id": "68beea03"
   },
   "source": [
    "### Zero-Shot Test Step + Finetuned Test Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5916ced9",
   "metadata": {
    "id": "5916ced9"
   },
   "outputs": [],
   "source": [
    "# From: ./training_systems/evaluation_methods/TestSteps.py\n",
    "from typing import Dict\n",
    "\n",
    "import clip\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# from training_systems.core import EvaluationMethod\n",
    "# from utils import ContiguousLabelDataset, CLASS_NAMES, AverageMeter\n",
    "\n",
    "\n",
    "class ZeroShotTestStep(EvaluationMethod):\n",
    "    \"\"\"\n",
    "    Evaluation method for models that have been fine-tuned (e.g., CoCoOp or adversarial models).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, batch_size, categories):\n",
    "        super().__init__(model, batch_size)\n",
    "        self.categories = categories\n",
    "        # here we apply the standard CLIP template used for oxford flowers to all categories\n",
    "        text_inputs = clip.tokenize(\n",
    "            [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in self.categories]\n",
    "        ).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            # we can encode the text features once as they are shared for all images\n",
    "            # therefore we do it outside the evaluation loop\n",
    "            self.text_features = self.model.encode_text(text_inputs)\n",
    "            # and here we normalize them (standard pratice with CLIP)\n",
    "            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataset, desc_add=\"\") -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "        accuracy_meter = AverageMeter()\n",
    "        tmp_dataset = ContiguousLabelDataset(dataset, self.categories)\n",
    "        dataloader = DataLoader(tmp_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "        # Remap labels into a contiguous set starting from zero\n",
    "        self.walk(dataloader, accuracy_meter, desc_add)\n",
    "\n",
    "        return {\"accuracy\": accuracy_meter.avg}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def walk(self, dataloader, accuracy_meter, desc_add):\n",
    "        pbar = tqdm(total=len(dataloader), desc=\"Test (Zero Shots) \" + desc_add, position=1, leave=False)\n",
    "        for image, target in dataloader:\n",
    "            # base categories range from 0 to 50, while novel ones from 51 to 101\n",
    "            # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "            # Map targets in contiguous set starting from zero\n",
    "            # Labels needs to be .long() in pytorch\n",
    "\n",
    "            image = image.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "\n",
    "            # forward image through CLIP image encoder\n",
    "            image_features = self.model.encode_image(image)\n",
    "            # and normalize\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "            predicted_class = (image_features @ self.text_features.T).argmax(dim=-1)\n",
    "            # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "\n",
    "            correct = (predicted_class == target).sum().item()\n",
    "            accuracy_meter.update(correct, n=target.size(0), raw=True)\n",
    "            pbar.set_postfix({\n",
    "                \"accuracy\" : accuracy_meter.avg\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "class FineTunedTestStep(EvaluationMethod):\n",
    "    \"\"\"\n",
    "    Evaluation method for the f rozen base CLIP model.\n",
    "    \"\"\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataset, classnames: list[int], desc_add=\"\") -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "        accuracy_meter = AverageMeter()\n",
    "        tmp_dataset = ContiguousLabelDataset(dataset, classnames)\n",
    "        dataloader = DataLoader(tmp_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "        remapped_class_names = [CLASS_NAMES[tmp_dataset.idx2cat[i]] for i in range(len(tmp_dataset.idx2cat))]\n",
    "        with self.model.temporary_classnames(remapped_class_names):\n",
    "            pbar = tqdm(total=len(dataloader), desc=\"Test (Finetuned) \" + desc_add, position=1, leave=False)\n",
    "            for images, targets in dataloader:\n",
    "                images = images.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                logits = self.model(images)\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                correct = (predictions == targets).sum().item()\n",
    "                accuracy_meter.update(correct, n=targets.size(0), raw=True)\n",
    "                pbar.set_postfix({\n",
    "                    \"accuracy\": accuracy_meter.avg\n",
    "                })\n",
    "                pbar.update(1)\n",
    "\n",
    "        return {\"accuracy\": accuracy_meter.avg}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb965458",
   "metadata": {
    "id": "bb965458"
   },
   "source": [
    "# Utils & Generics\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset used for this project is **Oxford 102 Flowers**, a fine-grained classification dataset consisting of 102 flower categories. The dataset was predefined by the course instructors as a common benchmark for all groups participating in the project.\n",
    "\n",
    "To simulate a zero-shot generalization setting, the dataset is split into two disjoint subsets:\n",
    "\n",
    "- **Base classes** ($\\mathcal{C}_\\text{base}$): 51 classes used during training.  \n",
    "- **Novel classes** ($\\mathcal{C}_\\text{novel}$): 51 classes held out for evaluation only.\n",
    "\n",
    "Following the standard few-shot protocol, we use only **10 shots per class** for training and validation. No additional data augmentation or unlabeled data is used beyond this few-shot setup.\n",
    "\n",
    "In certain methods, we simulate exposure to novel-like conditions by creating **pseudo-novel** ($\\mathcal{D}_\\text{unseen}$) subsets within the base classes, split dynamically or statically depending on the method (e.g., KLv2). The pseudo-novel ratio determines how many base classes are treated as novel at a time.\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "To promote generalization to novel classes and reduce overfitting to spurious image components such as background or object shape, we apply data augmentation during training. Specifically, we replicate the augmentation pipeline used in the original CoCoOp implementation [^3], as defined in its configuration files.\n",
    "\n",
    "The augmentation includes a random resized crop with bicubic interpolation, random horizontal flip, and normalization using CLIP-specific mean and standard deviation values:\n",
    "\n",
    "```yaml\n",
    "INPUT:\n",
    "  SIZE: (224, 224)\n",
    "  INTERPOLATION: \"bicubic\"\n",
    "  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]\n",
    "  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]\n",
    "  TRANSFORMS: [\"random_resized_crop\", \"random_flip\", \"normalize\"]\n",
    "```\n",
    "\n",
    "This version uses proper triple backticks for the YAML block and no escaping, so it will render cleanly in any Markdown environment. Let me know if you're targeting a specific renderer (like Jupyter or Hugo).\n",
    "\n",
    "### *Seen* and *Unseen* Split\n",
    "\n",
    "#### Motivation\n",
    "\n",
    "As part of the zero-shot learning framework, it is critical to respect the constraint that **no data from the novel classes** ($\\mathcal{D}_\\text{novel}$) is used in any form of training or model selection. This includes:\n",
    "\n",
    "- Updating model parameters using novel data.\n",
    "- Using novel data for validation, early stopping, or loss evaluation during training.\n",
    "\n",
    "Allowing access to $\\mathcal{D}_\\text{novel}$ during training would compromise the core assumption of zero-shot learning: generalization to unseen categories without direct supervision.\n",
    "\n",
    "**However**, in this work, we occasionally report performance on *val_novel* for the purpose of monitoring how different techniques affect generalization to novel classes. This evaluation is performed strictly *post hoc*, with no model updates or checkpoint selection based on novel data. This use is consistent with a case-study setup, where understanding the behavior of novel-class performance under various constraints is central to the investigation.\n",
    "\n",
    "#### Split\n",
    "\n",
    "Whatever method-pipeline is chosen, at the beginning of training we split base categories into $b$ seen and $\\texttt{total categories} - b$ unseen categories, where:\n",
    "\n",
    "$$\n",
    "b = \\texttt{total categories} \\cdot \\texttt{seen ratio}\n",
    "$$\n",
    "\n",
    "with $0.7 < \\texttt{seen ratio} < 0.9$ to preserve training quality.\n",
    "\n",
    "The formal structure is defined as:\n",
    "\n",
    "$$\n",
    "\\langle \\mathcal{C}_\\text{seen}, \\mathcal{C}_\\text{unseen} \\rangle = \\mathcal{C}_\\text{base}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_\\text{base} = \\{ \\mathbf{x}_i \\mid y_i \\in \\mathcal{C}_\\text{base} \\} = \\mathcal{D}_\\text{seen} \\cup \\mathcal{D}_\\text{unseen}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_\\text{seen} = \\{ \\mathbf{x}_i \\mid y_i \\in \\mathcal{C}_\\text{seen} \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_\\text{unseen} = \\{ \\mathbf{x}_i \\mid y_i \\in \\mathcal{C}_\\text{unseen} \\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_\\text{seen} \\cap \\mathcal{D}_\\text{unseen} = \\emptyset\n",
    "$$\n",
    "\n",
    "The roles of $\\mathcal{D}_\\text{seen}$ and $\\mathcal{D}_\\text{unseen}$ vary depending on the training method employed. Below, we summarize how each method leverages the data split:\n",
    "\n",
    "- **Base CoCoOp**:  \n",
    "  - $\\mathcal{D}_\\text{seen}$ is used to update model parameters via cross-entropy loss.  \n",
    "  - $\\mathcal{D}_\\text{unseen}$ is used only for validation (\"novel\" accuracy tracking).  \n",
    "  - No knowledge distillation or gradients from $\\mathcal{D}_\\text{unseen}$ flow into the model.\n",
    "\n",
    "- **CoCoOp + KL (v1)**:  \n",
    "  - A batch from $\\mathcal{D}_\\text{seen}$ is split into pseudo-base/pseudo-novel samples.  \n",
    "  - All samples contribute to both CE and KL loss within the batch.  \n",
    "  - $\\mathcal{D}_\\text{unseen}$ is only used for validation.\n",
    "\n",
    "- **CoCoOp + KL (v2)**:  \n",
    "  - $\\mathcal{D}_\\text{seen}$ is used for standard CoCoOp training with cross-entropy.  \n",
    "  - $\\mathcal{D}_\\text{unseen}$ is used to compute KL divergence from frozen CLIP logits.  \n",
    "  - $\\mathcal{D}_\\text{seen}$ and $\\mathcal{D}_\\text{unseen}$ are rotated epoch-by-epoch (see [Section](#subsec:KLV2design)).  \n",
    "  - Both sets contribute to training updates.\n",
    "\n",
    "- **Adversarial Training**:  \n",
    "  - The full dataset $\\mathcal{D}_\\text{base} = \\mathcal{D}_\\text{seen} \\cup \\mathcal{D}_\\text{unseen}$ is used.  \n",
    "  - All samples contribute via cross-entropy and adversarial losses.\n",
    "\n",
    "---\n",
    "\n",
    "**Summary:**  \n",
    "$\\mathcal{D}_\\text{unseen}$ is used *only for validation* in **Base CoCoOp** and **KL(v1)**. It becomes active in **KL(v2)** through KL regularization and is fully used in **Adversarial Training**.  \n",
    "In code, the terms `pseudo_base` and `pseudo_novel` correspond to $\\mathcal{C}_\\text{seen}$ and $\\mathcal{C}_\\text{unseen}$ respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb10ca19",
   "metadata": {
    "id": "bb10ca19"
   },
   "source": [
    "### ContiguousLabelDataset + Dataset's utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd4cd9",
   "metadata": {
    "id": "c1fd4cd9"
   },
   "outputs": [],
   "source": [
    "# From: ./utils/datasets.py\n",
    "import clip\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Subset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "class ContiguousLabelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that remaps arbitrary class labels to contiguous integers starting from 0.\n",
    "\n",
    "    This is useful for classification tasks where models expect class indices to be in a 0-based contiguous range.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (Dataset): The original dataset to wrap.\n",
    "        cat2idx (Dict[Any, int]): Mapping from original class labels to contiguous integer indices.\n",
    "        idx2cat (Dict[int, Any]): Reverse mapping from indices back to original class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_order: list[int]):\n",
    "        self.dataset = dataset\n",
    "        # force the mapping to use your custom order\n",
    "        self.cat2idx = { cat: idx for idx, cat in enumerate(class_order) }\n",
    "        self.idx2cat = { idx: cat for cat, idx in self.cat2idx.items() }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]\n",
    "        mapped_label = self.cat2idx[label]\n",
    "        return image, mapped_label\n",
    "\n",
    "\n",
    "# --- Data Augmentation Pipeline ---\n",
    "def build_default_transform(resolution=224):\n",
    "    \"\"\"\n",
    "    Builds the default data augmentation pipeline as specified:\n",
    "    - RandomResizedCrop with bicubic interpolation\n",
    "    - RandomHorizontalFlip\n",
    "    - Normalize with given mean and std\n",
    "    \"\"\"\n",
    "    return T.Compose([\n",
    "        T.RandomResizedCrop(resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "    ])\n",
    "\n",
    "\n",
    "def build_eval_transform(resolution=224):\n",
    "    \"\"\"\n",
    "    Builds the evaluation transform pipeline (no random augmentations):\n",
    "    - Resize to resolution\n",
    "    - CenterCrop to resolution\n",
    "    - Normalize with given mean and std\n",
    "    \"\"\"\n",
    "    return T.Compose([\n",
    "        T.Resize(resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(resolution),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_data(data_dir=\"./data\", train_transform=None, eval_transform=None, resolution=224):\n",
    "    \"\"\"\n",
    "    Loads the Flowers102 dataset from torchvision, returning separate splits for training, validation, and testing.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be downloaded/stored. Defaults to \"./data\".\n",
    "        train_transform (torchvision.transforms.Compose or None): Transformations to apply to training data.\n",
    "        eval_transform (torchvision.transforms.Compose or None): Transformations to apply to validation/test data.\n",
    "        resolution (int): Image resolution for transforms (default 224).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (train, val, test) of Flowers102 dataset splits.\n",
    "    \"\"\"\n",
    "    if train_transform is None:\n",
    "        train_transform = build_default_transform(resolution)\n",
    "    if eval_transform is None:\n",
    "        eval_transform = build_eval_transform(resolution)\n",
    "\n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=train_transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=eval_transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=eval_transform)\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def base_novel_categories(dataset):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "\n",
    "def get_labels(dataset):\n",
    "    \"\"\"\n",
    "    Recursively retrieve labels from dataset or nested Subset.\n",
    "    Assumes the base dataset has a `_labels` attribute.\n",
    "    \"\"\"\n",
    "    if hasattr(dataset, '_labels'):\n",
    "        return dataset._labels\n",
    "    elif isinstance(dataset, Subset):\n",
    "        parent_labels = get_labels(dataset.dataset)\n",
    "        return [parent_labels[i] for i in dataset.indices]\n",
    "    else:\n",
    "        raise AttributeError(\"Dataset does not have _labels or is not a Subset of a dataset with _labels.\")\n",
    "\n",
    "def split_data(dataset, base_classes):\n",
    "    \"\"\"\n",
    "    Splits the dataset into base and novel subsets based on base_classes.\n",
    "    Works even if the input dataset is already a Subset.\n",
    "\n",
    "    Args:\n",
    "        dataset: PyTorch Dataset or Subset\n",
    "        base_classes (List[int]): List of class indices considered as base.\n",
    "\n",
    "    Returns:\n",
    "        base_dataset (Subset): Subset containing samples from base classes.\n",
    "        novel_dataset (Subset): Subset containing samples from novel classes.\n",
    "    \"\"\"\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    labels = get_labels(dataset)\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    for sample_id, label in enumerate(labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    base_dataset = Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = Subset(dataset, novel_categories_samples)\n",
    "\n",
    "    return base_dataset, novel_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969fc3b6",
   "metadata": {
    "id": "969fc3b6"
   },
   "source": [
    "## Metrics\n",
    "\n",
    "To evaluate training dynamics and final performance, we track a comprehensive set of metrics using TensorBoard. These metrics are recorded for both training and validation phases and are logged for every configuration run.\n",
    "\n",
    "#### Loss Curves  \n",
    "For each method, we log the evolution of the main losses on both training and validation sets:\n",
    "\n",
    "- **Cross-Entropy Loss**  \n",
    "- **KL Divergence Loss** (when applicable, e.g., KLv1 or KLv2)  \n",
    "- **Adversarial Loss** (when adversarial training with the MLP discriminator is active)\n",
    "\n",
    "#### Accuracy Tracking  \n",
    "We monitor accuracy on both training and validation datasets, according to the structure of the current method:\n",
    "\n",
    "- **Validation Accuracy:** Includes base, novel, pseudo-base, and pseudo-novel subsets depending on the method (e.g., pseudo-novel for KLv2, base-only for CE-only baselines).  \n",
    "- **Training Accuracy:** Captured for the subset used in each training phase (e.g., pseudo-base for CE step).\n",
    "\n",
    "#### Final Evaluation  \n",
    "At the end of training, we report test accuracy on:\n",
    "\n",
    "- **Base Classes:** Accuracy on the 51 seen classes used during training.  \n",
    "- **Novel Classes:** Accuracy on the 51 unseen classes to assess generalization.  \n",
    "- **Harmonic Mean:** The harmonic mean of base and novel accuracy is computed to summarize the trade-off between performance on seen and unseen classes:\n",
    "\n",
    "$$\n",
    "H = \\frac{2 \\cdot \\text{Acc}_\\text{base} \\cdot \\text{Acc}_\\text{novel}}{\\text{Acc}_\\text{base} + \\text{Acc}_\\text{novel}}\n",
    "$$\n",
    "\n",
    "#### Experiment Configuration  \n",
    "All experiment settings, including hyperparameters such as learning rate, loss weights, number of epochs, architectural dimensions, and optimizer choices, are stored and logged from the YAML configuration file associated with each run. This allows for consistent tracking and comparison across experiments.\n",
    "\n",
    "These metrics provide both detailed insights into learning progress and a clear summary of final generalization performance across different configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f8b05e",
   "metadata": {
    "id": "51f8b05e"
   },
   "outputs": [],
   "source": [
    "# From: ./utils/metrics.py\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Compute and store the average and current value.\n",
    "\n",
    "    Examples::\n",
    "        >>> # 1. Initialize a meter to record loss\n",
    "        >>> losses = AverageMeter()\n",
    "        >>> # 2. Update meter after every mini-batch update\n",
    "        >>> losses.update(loss_value, batch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ema=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ema (bool, optional): apply exponential moving average.\n",
    "        \"\"\"\n",
    "        self.ema = ema\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1, raw=False):\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            val = val.item()\n",
    "\n",
    "        self.val = val\n",
    "        if raw:\n",
    "            self.sum += val\n",
    "        else:\n",
    "            self.sum += val * n\n",
    "\n",
    "        self.count += n\n",
    "\n",
    "        if self.ema:\n",
    "            self.avg = self.avg * 0.9 + self.val * 0.1\n",
    "        else:\n",
    "            self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class MetricMeter:\n",
    "    \"\"\"Store the average and current value for a set of metrics.\n",
    "\n",
    "    Examples::\n",
    "        >>> # 1. Create an instance of MetricMeter\n",
    "        >>> metric = MetricMeter()\n",
    "        >>> # 2. Update using a dictionary as input\n",
    "        >>> input_dict = {'loss_1': value_1, 'loss_2': value_2}\n",
    "        >>> metric.update(input_dict)\n",
    "        >>> # 3. Convert to string and print\n",
    "        >>> print(str(metric))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, delimiter=\" \"):\n",
    "        self.meters = defaultdict(AverageMeter)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, input_dict):\n",
    "        if input_dict is None:\n",
    "            return\n",
    "\n",
    "        if not isinstance(input_dict, dict):\n",
    "            raise TypeError(\n",
    "                \"Input to MetricMeter.update() must be a dictionary\"\n",
    "            )\n",
    "\n",
    "        for k, v in input_dict.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __str__(self):\n",
    "        output_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            output_str.append(f\"{name} {meter.val:.4f} ({meter.avg:.4f})\")\n",
    "        return self.delimiter.join(output_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d22d92",
   "metadata": {
    "id": "63d22d92"
   },
   "source": [
    "#### Tensorboard Logger + CSV Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9ed9f",
   "metadata": {
    "id": "85a9ed9f"
   },
   "outputs": [],
   "source": [
    "# From: ./utils/tensor_board_logger.py\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "\n",
    "def harmonic_mean(a, b):\n",
    "    return 2 * (a * b) / (a + b)\n",
    "\n",
    "\n",
    "class CSVLogger:\n",
    "    \"\"\"\n",
    "    A simple logger that writes training metrics to a CSV file.\n",
    "\n",
    "    Attributes:\n",
    "        filename (str): The path to the output CSV file.\n",
    "        file (file object): Open file handle for writing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"\n",
    "        Initializes the CSV logger and creates the output directory and file.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Path to the CSV file for logging.\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        os.makedirs(\n",
    "            os.path.dirname(filename), exist_ok=True\n",
    "        )  # Create folder if it doesn't exist\n",
    "        self.file = open(filename, \"w\")\n",
    "        self.file.write(\"epoch,base_acc,novel_acc,harmonic_mean\\n\")\n",
    "\n",
    "    def log(self, epoch, base_acc, novel_acc):\n",
    "        \"\"\"\n",
    "        Logs a training epoch's results including harmonic mean to the CSV file.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): The epoch number.\n",
    "            base_acc (float): Accuracy on base classes.\n",
    "            novel_acc (float): Accuracy on novel classes.\n",
    "        \"\"\"\n",
    "        hm = harmonic_mean(base_acc, novel_acc)\n",
    "        self.file.write(f\"{epoch},{base_acc},{novel_acc},{hm}\\n\")\n",
    "        self.file.flush()\n",
    "        print(\n",
    "            f\"Logged: epoch {epoch}, base_acc {base_acc}, novel_acc {novel_acc}, harmonic_mean {hm}\"\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the CSV file.\n",
    "        \"\"\"\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "class BaseAndNovelMetrics:\n",
    "    \"\"\"\n",
    "    Tracks base and novel accuracy values and their harmonic mean across epochs.\n",
    "\n",
    "    Attributes:\n",
    "        tmp (List[Tuple[int, float, float, float]]): Logged metrics per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tmp = []\n",
    "\n",
    "    def update(self, epoch, base_acc, novel_acc):\n",
    "        \"\"\"\n",
    "        Records a new set of metrics for a specific epoch.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Epoch number.\n",
    "            base_acc (float): Accuracy on base classes.\n",
    "            novel_acc (float): Accuracy on novel classes.\n",
    "        \"\"\"\n",
    "        self.tmp.append(\n",
    "            (epoch, base_acc, novel_acc, harmonic_mean(base_acc, novel_acc))\n",
    "        )\n",
    "\n",
    "    def get_metrics(self):\n",
    "        \"\"\"\n",
    "        Retrieves the collected metrics.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[int, float, float, float]] or None: Logged metrics or None if empty.\n",
    "        \"\"\"\n",
    "        if len(self.tmp) == 0:\n",
    "            return None\n",
    "        return self.tmp\n",
    "\n",
    "\n",
    "class TensorboardLogger:\n",
    "    \"\"\"\n",
    "    Handles logging of training and evaluation metrics to TensorBoard and CSV.\n",
    "\n",
    "    Attributes:\n",
    "        writer (SummaryWriter): TensorBoard writer instance.\n",
    "        csv_logger (CSVLogger): CSV logger instance for persistent metric storage.\n",
    "        hparams (dict): Hyperparameters dictionary.\n",
    "        base_and_novel_metrics (BaseAndNovelMetrics): Metric tracker.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, writer: SummaryWriter):\n",
    "        \"\"\"\n",
    "        Initializes the TensorboardLogger.\n",
    "\n",
    "        Args:\n",
    "            writer (SummaryWriter): TensorBoard writer instance.\n",
    "        \"\"\"\n",
    "        self.writer = writer\n",
    "        self.csv_logger = CSVLogger(f\"{writer.log_dir}/metrics.csv\")\n",
    "        self.hparams = None\n",
    "        self.base_and_novel_metrics = BaseAndNovelMetrics()\n",
    "\n",
    "    def log_hparams(self, hparams: dict):\n",
    "        \"\"\"\n",
    "        Stores hyperparameters for later logging.\n",
    "\n",
    "        Args:\n",
    "            hparams (dict): Hyperparameters dictionary.\n",
    "        \"\"\"\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def log_training_base(self, epoch, lr, ce_loss, acc, kl_loss, total_loss):\n",
    "        \"\"\"\n",
    "        Logs training metrics for the base training phase.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Current epoch.\n",
    "            lr (float): Learning rate.\n",
    "            ce_loss (float): Cross entropy loss.\n",
    "            acc (float): Accuracy.\n",
    "            kl_loss (float or None): KL divergence loss, optional.\n",
    "            total_loss (float): Total loss.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(\"learning_rate\", lr, epoch)\n",
    "        self.writer.add_scalar(\"train_base/ce_loss\", ce_loss, epoch)\n",
    "        self.writer.add_scalar(\"train_base/ce_accuracy\", acc, epoch)\n",
    "        if kl_loss is not None:\n",
    "            self.writer.add_scalar(\"train_base/kl_loss\", kl_loss, epoch)\n",
    "        self.writer.add_scalar(\"train_base/total_loss\", total_loss, epoch)\n",
    "\n",
    "    def log_training_adv(\n",
    "        self, epoch, lambda_adv, ce_loss, acc, adv_loss, total_loss, kl_loss=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs training metrics for the adversarial training phase.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Current epoch.\n",
    "            lambda_adv (float): Adversarial loss weight.\n",
    "            ce_loss (float): Cross entropy loss.\n",
    "            acc (float): Accuracy.\n",
    "            adv_loss (float): Adversarial loss.\n",
    "            total_loss (float): Total loss.\n",
    "            kl_loss (float or None): KL divergence loss, optional.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(\"lambda_adv\", lambda_adv, epoch)\n",
    "        self.writer.add_scalar(\"train_adv/ce_loss\", ce_loss, epoch)\n",
    "        self.writer.add_scalar(\"train_adv/ce_accuracy\", acc, epoch)\n",
    "        self.writer.add_scalar(\"train_adv/mlp_loss\", adv_loss, epoch)\n",
    "        if kl_loss is not None:\n",
    "            self.writer.add_scalar(\"train_adv/kl_loss\", kl_loss, epoch)\n",
    "        self.writer.add_scalar(\"train_adv/total_loss\", total_loss, epoch)\n",
    "\n",
    "    def log_validation(\n",
    "        self, epoch, base_loss, base_acc, novel_loss, novel_acc, is_adv=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs validation metrics.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Current epoch.\n",
    "            base_loss (float): Loss on base classes.\n",
    "            base_acc (float): Accuracy on base classes.\n",
    "            novel_loss (float): Loss on novel classes.\n",
    "            novel_acc (float): Accuracy on novel classes.\n",
    "            is_adv (bool): Whether validation is adversarial.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(f\"validation_base/loss\", base_loss, epoch)\n",
    "        self.writer.add_scalar(f\"validation_base/accuracy\", base_acc, epoch)\n",
    "        self.writer.add_scalar(f\"validation_novel/loss\", novel_loss, epoch)\n",
    "        self.writer.add_scalar(f\"validation_novel/accuracy\", novel_acc, epoch)\n",
    "\n",
    "        prefix = \"validation_adv\" if is_adv else \"validation_ce\"\n",
    "        self.writer.add_scalar(f\"{prefix}_base/loss\", base_loss, epoch)\n",
    "        self.writer.add_scalar(f\"{prefix}_base/accuracy\", base_acc, epoch)\n",
    "        self.writer.add_scalar(f\"{prefix}_novel/loss\", novel_loss, epoch)\n",
    "        self.writer.add_scalar(f\"{prefix}_novel/accuracy\", novel_acc, epoch)\n",
    "\n",
    "    def log_final_metrics(self, tag, base_acc, novel_acc, step):\n",
    "        \"\"\"\n",
    "        Logs final accuracy metrics and updates CSV and metric tracker.\n",
    "\n",
    "        Args:\n",
    "            tag (str): Tag name for TensorBoard.\n",
    "            base_acc (float): Accuracy on base classes.\n",
    "            novel_acc (float): Accuracy on novel classes.\n",
    "            step (int): Training step or epoch index.\n",
    "        \"\"\"\n",
    "        harmonic = harmonic_mean(base_acc, novel_acc)\n",
    "        self.writer.add_scalars(\n",
    "            tag,\n",
    "            {\n",
    "                \"Harmonic Mean\": harmonic,\n",
    "                \"Base Accuracy\": base_acc,\n",
    "                \"Novel Accuracy\": novel_acc,\n",
    "            },\n",
    "            global_step=step + 1,\n",
    "        )\n",
    "\n",
    "        self.csv_logger.log(step + 1, base_acc, novel_acc)\n",
    "\n",
    "        if self.hparams is not None:\n",
    "            self.base_and_novel_metrics.update(step + 1, base_acc, novel_acc)\n",
    "\n",
    "        self.writer.flush()\n",
    "\n",
    "    def log_test_accuracy(self, step, acc, label):\n",
    "        \"\"\"\n",
    "        Logs test accuracy for a given label.\n",
    "\n",
    "        Args:\n",
    "            step (int): Step or epoch index.\n",
    "            acc (float): Accuracy value.\n",
    "            label (str): Label name for the accuracy metric.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(f\"{label}/accuracy\", acc, step)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the logger, writes hyperparameters and final metrics if available.\n",
    "        \"\"\"\n",
    "        metrics = self.base_and_novel_metrics.get_metrics() or []\n",
    "\n",
    "        \"\"\"\n",
    "        metric_dict = {\n",
    "            \"base_acc_after_base\": metrics[0][1] if metrics else 0,\n",
    "            \"novel_acc_after_base\": metrics[0][2] if metrics else 0,\n",
    "            \"harmonic_mean_after_base\": metrics[0][3] if metrics else 0,\n",
    "            \"base_acc_after_adv\": metrics[1][1] if metrics else 0,\n",
    "            \"novel_acc_after_adv\": metrics[1][2] if metrics else 0,\n",
    "            \"harmonic_mean_after_adv\": metrics[1][3] if metrics else 0,\n",
    "        }\"\"\"\n",
    "\n",
    "        if self.hparams is not None and metrics:\n",
    "\n",
    "            tmp = {}\n",
    "            # Set the prefix based on whether metrics are from base phase (index 0) or adversarial phase (index 1)\n",
    "            for idx, m in enumerate(metrics):\n",
    "                prefix = \"after_base\" if idx == 0 else \"after_adv\"\n",
    "                tmp[f\"epoch_{prefix}\"] = m[0]\n",
    "                tmp[f\"base_acc_{prefix}\"] = m[1]\n",
    "                tmp[f\"novel_acc_{prefix}\"] = m[2]\n",
    "                tmp[f\"harmonic_mean_{prefix}\"] = m[3]\n",
    "\n",
    "            self.writer.add_hparams(\n",
    "                hparam_dict=self.hparams,\n",
    "                metric_dict=tmp,\n",
    "            )\n",
    "        self.writer.close()\n",
    "        self.csv_logger.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bb9fc2",
   "metadata": {
    "id": "c9bb9fc2"
   },
   "source": [
    "## Training Coop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f3938d",
   "metadata": {
    "id": "f7f3938d"
   },
   "outputs": [],
   "source": [
    "# From: ./utils/training_coop.py\n",
    "\"\"\"\n",
    "This module provides training, evaluation, and testing functions for the CoOp model and standard CLIP evaluation,\n",
    "including fine-tuning and zero-shot classification on image datasets.\n",
    "\"\"\"\n",
    "from clip.model import CLIP\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "\n",
    "# from model.coop.custom_clip import CustomCLIPCoOp\n",
    "# from utils.datasets import ContiguousLabelDataset, CLASS_NAMES\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_step(model, dataset, cost_function, new_classnames, batch_size=32, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataset using cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate.\n",
    "        dataset (Dataset): Dataset to evaluate on.\n",
    "        cost_function (Callable): Loss function to use.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "        device (str): Computation device (\"cuda\" or \"cpu\").\n",
    "        new_classnames (List[int] or None): Optional list of class indices to temporarily substitute for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Average loss and accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    tmp_dataset = ContiguousLabelDataset(dataset, new_classnames)\n",
    "    dataloader = DataLoader(tmp_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if new_classnames is not None:\n",
    "        new_classnames = [CLASS_NAMES[c] for c in new_classnames]\n",
    "        with model.temporary_classnames(new_classnames):\n",
    "            correct, total, total_loss = walk_the_dataset(correct, cost_function, dataloader, device, model, total,\n",
    "                                                          total_loss)\n",
    "\n",
    "    else:\n",
    "        correct, total, total_loss = walk_the_dataset(correct, cost_function, dataloader, device, model, total,\n",
    "                                                      total_loss)\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def walk_the_dataset(correct, cost_function, dataloader, device, model, total, total_loss):\n",
    "    \"\"\"\n",
    "    Iterates over the dataset and computes cumulative loss and accuracy.\n",
    "\n",
    "    Args:\n",
    "        correct (int): Running count of correct predictions.\n",
    "        cost_function (Callable): Loss function used.\n",
    "        dataloader (DataLoader): DataLoader for the dataset.\n",
    "        device (str): Computation device.\n",
    "        model (nn.Module): Model being evaluated.\n",
    "        total (int): Total number of samples evaluated so far.\n",
    "        total_loss (float): Accumulated loss value.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int, float]: Updated correct, total, and total_loss.\n",
    "    \"\"\"\n",
    "    for images, targets in tqdm(dataloader, desc=\"Validation\", position=1, leave=False):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        loss, logits = model(images, targets)\n",
    "\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    return correct, total, total_loss\n",
    "\n",
    "\n",
    "def training_step(model: CustomCLIPCoOp, dataset, optimizer, batch_size, classnames, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Performs one full training epoch for the CoOp model.\n",
    "\n",
    "    Args:\n",
    "        model (CustomCLIPCoOp): The model to train.\n",
    "        dataset (Dataset): Dataset to train on.\n",
    "        optimizer (Optimizer): Optimizer used for updating model parameters.\n",
    "        batch_size (int): Batch size for training.\n",
    "        device (str): Computation device.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Average training loss and accuracy.\n",
    "    \"\"\"\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to training mode\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    tmp_dataset = ContiguousLabelDataset(dataset, classnames)\n",
    "    dataloader = DataLoader(tmp_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", position=1, leave=False)\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        # Load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # debug if inputs and targets are taken correctly by the dataloader\n",
    "        #print(inputs.shape)\n",
    "        #print(targets.shape)\n",
    "        # Forward pass + loss computation\n",
    "        loss, logits = model(inputs, targets)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(\"‚ö†Ô∏è NaN loss encountered!\")\n",
    "            #print(\"Logits:\", logits)\n",
    "            print(\"Targets:\", targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Fetch prediction and loss value\n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item() * inputs.shape[0]\n",
    "        _, predicted = logits.max(dim=1)  # max() returns (maximum_value, index_of_maximum_value)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_postfix(train_loss=loss.item(), train_acc=cumulative_accuracy / samples )\n",
    "        pbar.update(1)\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_step(model, dataset, batch_size, device, categories, label=\"test\", base=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model using either fine-tuned or base (zero-shot) strategy.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to test.\n",
    "        dataset (Dataset): Dataset for testing.\n",
    "        batch_size (int): Batch size for testing.\n",
    "        device (str): Device used for computation.\n",
    "        label (str): Label for the progress bar.\n",
    "        base (bool): Whether to use zero-shot CLIP instead of the fine-tuned model.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy score.\n",
    "    \"\"\"\n",
    "    if not base:\n",
    "        return finetuned_test_step(model, dataset, batch_size, device, categories, label)\n",
    "    else:\n",
    "        return base_test_step(model, dataset, categories, batch_size, device, label)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def finetuned_test_step(model: CustomCLIPCoOp, dataset, batch_size, device, categories, label=\"test\"):\n",
    "    \"\"\"\n",
    "    Evaluates a fine-tuned CustomCLIPCoOp model on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (CustomCLIPCoOp): Fine-tuned CoOp model.\n",
    "        dataset (Dataset): Dataset for evaluation.\n",
    "        batch_size (int): Batch size.\n",
    "        device (str): Computation device.\n",
    "        label (str): Label for progress display.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    tmp_dataset = ContiguousLabelDataset(dataset, categories)\n",
    "    dataloader = DataLoader(tmp_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, targets in tqdm(dataloader, desc=label):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()  # we don't want gradients\n",
    "def base_test_step(model: CLIP, dataset, categories, batch_size, device, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluates a zero-shot CLIP model using cosine similarity between image and text embeddings.\n",
    "\n",
    "    Args:\n",
    "        model (CLIP): Pretrained CLIP model.\n",
    "        dataset (Dataset): Dataset to evaluate.\n",
    "        categories (List[int]): List of category indices to evaluate.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "        device (str): Computation device.\n",
    "        label (str): Optional label for progress bar.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of zero-shot CLIP classification.\n",
    "    \"\"\"\n",
    "    # let's set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
    "    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
    "    text_inputs = clip.tokenize(\n",
    "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
    "    ).to(device)\n",
    "\n",
    "    # we can encode the text features once as they are shared for all images\n",
    "    # therefore we do it outside the evaluation loop\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    # and here we normalize them (standard pratice with CLIP)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        # base categories range from 0 to 50, while novel ones from 51 to 101\n",
    "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "        # Map targets in contiguous set starting from zero\n",
    "        # Labels needs to be .long() in pytorch\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward image through CLIP image encoder\n",
    "        image_features = model.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84132fb5",
   "metadata": {
    "id": "84132fb5"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56db17",
   "metadata": {
    "id": "9b56db17"
   },
   "source": [
    "### Main (Disabled for Jupyter Execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30364506",
   "metadata": {
    "id": "30364506"
   },
   "outputs": [],
   "source": [
    "# From: ./main.py\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "# from training_systems.cocoop import CoCoOpSystem\n",
    "# from training_systems.coop import CoOpSystem\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--device', default=os.getenv(\"DEVICE\", \"cuda:0\"))\n",
    "    parser.add_argument('--run_name', default=f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    parser.add_argument('--using_coop', default=False, type=lambda x: x.lower() in (\"1\", \"true\", \"yes\", \"true\"))\n",
    "    parser.add_argument('--config', default=\"train_config.yaml\")\n",
    "    parser.add_argument('--debug', default=True, type=lambda x: x.lower() in (\"1\", \"true\", \"yes\", \"true\"))\n",
    "    return parser.parse_args()\n",
    "\n",
    "####\n",
    "#Disabled for jupyter notebook execution\n",
    "####\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Parse command-line arguments\n",
    "#     # Assign parsed arguments to variables\n",
    "#     # Display which device is being used\n",
    "#     # Handle MPS backend by setting default tensor type to float32\n",
    "#     # Indicate whether CoOp or CoCoOp is used for training\n",
    "#     # Load training configuration from YAML file\n",
    "\n",
    "#     # Initialize and train using CoOpSystem if specified in arguments\n",
    "#     # Initialize and train using CoCoOpSystem otherwise\n",
    "#     args = parse_args()\n",
    "\n",
    "#     device = args.device\n",
    "#     run_name = args.run_name\n",
    "#     debug = args.debug\n",
    "#     use_coop = args.using_coop\n",
    "\n",
    "#     print(f\"Using device: {device}\")\n",
    "\n",
    "#     if torch.backends.mps.is_available():\n",
    "#         print(\"\\u26a0\\ufe0f Forcing float32 due to MPS limitations\")\n",
    "#         torch.set_default_dtype(torch.float32)\n",
    "\n",
    "#     print(f\"Using {'CoOp' if use_coop else 'CoCoOp'} for training\")\n",
    "\n",
    "#     # Load hyperparameters from YAML\n",
    "#     with open(args.config, \"r\") as file:\n",
    "#         config = yaml.safe_load(file)\n",
    "\n",
    "#     if use_coop:\n",
    "#         coop_cfg = config['coop']\n",
    "#         train_sys = CoOpSystem(\n",
    "#             device=device,\n",
    "#             run_name=run_name,\n",
    "#             **coop_cfg\n",
    "#         )\n",
    "#     else:\n",
    "#         cocoop_cfg = config['cocoop']\n",
    "#         train_sys = CoCoOpSystem(\n",
    "#             device=device,\n",
    "#             run_name=run_name,\n",
    "#             debug=debug,\n",
    "#             hparams_file=args.config,\n",
    "#             **cocoop_cfg\n",
    "#         )\n",
    "\n",
    "#     train_sys.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb798f3",
   "metadata": {
    "id": "ddb798f3"
   },
   "source": [
    "### Jupyter Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187ae66",
   "metadata": {
    "id": "2187ae66",
    "outputId": "ba0fffd9-e95a-4e86-9a9c-8032cd55e2e3"
   },
   "outputs": [],
   "source": [
    "# Jupyter new Main\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    runs_yaml = [\"test_kl+adv\"]\n",
    "\n",
    "    ## converted to a list of dictionaries for compatibility from the yaml file\n",
    "\n",
    "    runs_config = [{\n",
    "        \"coop\": {\n",
    "            \"batch_size\": 10,\n",
    "            \"learning_rate\": 0.002,\n",
    "            \"weight_decay\": 0.0001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"epochs\": 20,\n",
    "            \"n_ctx\": 8,\n",
    "            \"ctx_init\": \"\",\n",
    "            \"class_token_position\": \"end\",\n",
    "            \"csc\": False\n",
    "        },\n",
    "        \"cocoop\": {\n",
    "            \"report\": True,\n",
    "            \"pat\": False,\n",
    "            \"cnn_model\": \"ViT-B/16\",\n",
    "            \"test_batch_size\": 10,\n",
    "            \"train_base_checkpoint_path\": None, # Cant have outside files in the jupyter\n",
    "            \"optimizer_configs\": [\n",
    "                {\n",
    "                    \"prompt_lr\": 0.002,\n",
    "                    \"weight_decay\": 0.0001,\n",
    "                    \"momentum\": 0.9\n",
    "                },\n",
    "                {\n",
    "                    \"prompt_lr\": 0.002,\n",
    "                    \"mlp_lr\": 0.005,\n",
    "                    \"weight_decay\": 0.0005,\n",
    "                    \"momentum\": 0.8\n",
    "                }\n",
    "            ],\n",
    "            \"skip_tests\": [True, True, False],\n",
    "            \"prompt_learner_opt\": {\n",
    "                \"n_ctx\": 4,\n",
    "                \"ctx_init\": \"\",\n",
    "                \"class_token_position\": \"end\",\n",
    "                \"csc\": False\n",
    "            },\n",
    "            \"kl_loss_opt\": {\n",
    "                \"lambda_kl\": [0.1, 0.1],\n",
    "                \"using_kl\": [True, False]\n",
    "            },\n",
    "            \"adv_training_opt\": {\n",
    "                \"adv_training_epochs\": 1,\n",
    "                \"batch_size\": 10,\n",
    "                \"warmup_lambda_adv\": 1,\n",
    "                \"lambda_adv\": 3,\n",
    "                \"grl_lambda\": 1,\n",
    "                \"mlp_opt\": {\n",
    "                    \"hidden_structure\": [563, 256, 128]\n",
    "                },\n",
    "                \"prompt_learner_warmup_epochs\": 3,\n",
    "                \"ignore_no_improvement\": True,\n",
    "                \"use_bias_ctx\": True\n",
    "            },\n",
    "            \"base_training_opt\": {\n",
    "                \"epochs\": 1,\n",
    "                \"batch_size\": 10,\n",
    "                \"warmup_epoch\": 0,\n",
    "                \"warmup_cons_lr\": 1e-5\n",
    "            },\n",
    "            \"clustering_opt\": {\n",
    "                \"n_clusters\": 4,\n",
    "                \"variance\": 0.95,\n",
    "                \"vision_encoder\": \"ViT-B/32\",\n",
    "                \"clustering_type\": \"semantic\"\n",
    "            }\n",
    "            }\n",
    "        },]\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    debug = False\n",
    "    use_coop = False\n",
    "\n",
    "    for run_id, run_yaml in enumerate(runs_yaml):\n",
    "        run_name = \"jupyter_notebook_\"+ run_yaml+ \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        if torch.backends.mps.is_available():\n",
    "            print(\"\\u26a0\\ufe0f Forcing float32 due to MPS limitations\")\n",
    "            torch.set_default_dtype(torch.float32)\n",
    "\n",
    "        print(f\"Using {'CoOp' if use_coop else 'CoCoOp'} for training\")\n",
    "\n",
    "        config = runs_config[run_id] if isinstance(runs_config, list) else runs_config\n",
    "\n",
    "        if use_coop:\n",
    "            coop_cfg = config['coop']\n",
    "            train_sys = CoOpSystem(\n",
    "                device=device,\n",
    "                run_name=run_name,\n",
    "                **coop_cfg\n",
    "            )\n",
    "        else:\n",
    "            cocoop_cfg = config['cocoop']\n",
    "            train_sys = CoCoOpSystem(\n",
    "                device=device,\n",
    "                run_name=run_name,\n",
    "                debug=debug,\n",
    "                hparams_file=run_yaml,\n",
    "                **cocoop_cfg\n",
    "            )\n",
    "        print(f\"\\nTraining system initialized with run name: {train_sys.run_name}\")\n",
    "        train_sys.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VEdlp6oswrNs",
   "metadata": {
    "id": "VEdlp6oswrNs"
   },
   "source": [
    "# Charts and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06f1327",
   "metadata": {},
   "source": [
    "## Main results'table\n",
    "\n",
    "| **Method**                 | **KL V1** (Œª_kl) | **KL V2** (Œª_kl) | **Adversarial** | **n_ctx** | **Base** | **Harmonic Mean** | **Novel** | **Notes**                                                                 |\n",
    "|---------------------------|------------------|------------------|------------------|----------|---------|-------------------|----------|------------------------------------------------------------------------------|\n",
    "| CLIP zero-shot            |                  |                  |                  | 4        | 72.08   | 74.83             | **77.80** | Reported in CoCoOp paper [2]                                               |\n",
    "| CoCoOp                    |                  |                  |                  | 4        | **94.87** | **81.71**         | 71.75    | Reported in CoCoOp paper [2]                                               |\n",
    "|                           | ‚úÖ               |                  |                  | 4        | 63.69   | 67.62             | 72.06    | KLv1 baseline, 4 context tokens.                                           |\n",
    "|                           |                  | ‚úÖ               |                  | 4        | 75.66   | 75.33             | 75.00    | KLv2 with Œª_kl = 0.3, rotation period of 3 epochs.                         |\n",
    "|                           |                  |                  | ‚úÖ               | 4        | **87.26** | 75.54             | 66.59    | Adversarial-only training 4 clusters.                                     |\n",
    "|                           |                  | ‚úÖ               |                  | 4        | 77.23   | 75.68             | 74.18    | KLv2 with Œª_kl = 0.1, rotation every 4 epochs.                             |\n",
    "|                           |                  | ‚úÖ               |                  | 8        | 75.62   | 73.83             | 72.12    | KLv2 with 8 context tokens.                                                |\n",
    "|                           |                  | ‚úÖ               | ‚úÖ               | 4        | 79.01   | 77.20             | **75.46** | Adversary input: `ctx + bias` (`ctx_shifted`).                             |\n",
    "|                           |                  | ‚úÖ               | ‚úÖ               | 4        | 77.80   | 76.20             | 74.67    | Adversary input: `selected_text_feature + ctx_shifted.mean()`.             |\n",
    "| Custom CoCoOp             |                  | ‚úÖ               | ‚úÖ               | 4        | 85.32   | **79.18**         | 73.86    | Adversary input: `avg_text_features + masked logits`.                      |\n",
    "\n",
    "[2]: Yaroslav Ganin and Victor Lempitsky. *Unsupervised Domain Adaptation by Backpropagation*. 2015. arXiv: [1409.7495](https://arxiv.org/abs/1409.7495) [stat.ML]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b6624",
   "metadata": {},
   "source": [
    "## Part with Graph Plotting \n",
    "### repo clone is required, run the block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea211b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the repo for accessing logs of the experiments, needed for visualization of the graphs\n",
    "!git clone https://github.com/bacobax/DeepL-project /content/deepl_project\n",
    "import sys\n",
    "sys.path.insert(0, '/content/deepl_project')\n",
    "%cd /content/deepl_project\n",
    "# %pip install -r /content/deepl_project/req_clean.txt    \n",
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9064924b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WsX9oEKKw7a8",
   "metadata": {
    "id": "WsX9oEKKw7a8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "def plot_tensorboard_scalars_grid(log_dirs: dict, scalar_names: list, max_cols: int = 2):\n",
    "    \"\"\"\n",
    "    Plots TensorBoard scalar values from multiple runs in a grid layout.\n",
    "\n",
    "    Args:\n",
    "        log_dirs (dict): Mapping from label name (str) to log directory path (str).\n",
    "        scalar_names (list): List of scalar tag names to plot (str).\n",
    "        max_cols (int): Maximum number of columns in the grid layout.\n",
    "    \"\"\"\n",
    "    num_scalars = len(scalar_names)\n",
    "    cols = min(max_cols, num_scalars)\n",
    "    rows = math.ceil(num_scalars / cols)\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(6 * cols, 4 * rows))\n",
    "    axs = axs.flatten() if num_scalars > 1 else [axs]\n",
    "\n",
    "    for i, scalar_name in enumerate(scalar_names):\n",
    "        ax = axs[i]\n",
    "        found = False\n",
    "\n",
    "        for label, log_dir in log_dirs.items():\n",
    "            if not os.path.exists(log_dir):\n",
    "                print(f\"[WARN] Directory not found: {log_dir}. Skipping '{label}'.\")\n",
    "                continue\n",
    "\n",
    "            event_acc = EventAccumulator(log_dir)\n",
    "            try:\n",
    "                event_acc.Reload()\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to load {log_dir}: {e}\")\n",
    "                continue\n",
    "\n",
    "            if scalar_name not in event_acc.Tags().get('scalars', []):\n",
    "                print(f\"[INFO] Scalar '{scalar_name}' not found in '{label}'.\")\n",
    "                continue\n",
    "\n",
    "            events = event_acc.Scalars(scalar_name)\n",
    "            steps = [e.step for e in events]\n",
    "            values = [e.value for e in events]\n",
    "\n",
    "            ax.plot(steps, values, label=label)\n",
    "            found = True\n",
    "\n",
    "        ax.set_title(scalar_name)\n",
    "        ax.set_xlabel(\"Step\")\n",
    "        ax.set_ylabel(\"Value\")\n",
    "        ax.grid(True)\n",
    "        if found:\n",
    "            ax.legend()\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, \"Scalar not found\", ha='center', va='center')\n",
    "            ax.set_axis_off()\n",
    "\n",
    "    # Turn off any unused axes\n",
    "    for j in range(len(scalar_names), len(axs)):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    fig.suptitle(\"TensorBoard Scalars Comparison\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daDF5duew8C6",
   "metadata": {
    "id": "daDF5duew8C6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "\n",
    "def plot_novel_base_hmean_bar_chart(log_dirs: dict):\n",
    "    \"\"\"\n",
    "    Creates a bar chart comparing 'novel_classes/accuracy', 'base_classes/accuracy',\n",
    "    and their harmonic mean for multiple TensorBoard runs.\n",
    "\n",
    "    Args:\n",
    "        log_dirs (dict): Mapping from label name (str) to log directory path (str).\n",
    "    \"\"\"\n",
    "    novel_values = []\n",
    "    base_values = []\n",
    "    hmean_values = []\n",
    "    run_labels = []\n",
    "\n",
    "    for label, log_dir in log_dirs.items():\n",
    "        if not os.path.exists(log_dir):\n",
    "            print(f\"[WARN] Directory not found: {log_dir}. Skipping '{label}'.\")\n",
    "            continue\n",
    "\n",
    "        event_acc = EventAccumulator(log_dir)\n",
    "        try:\n",
    "            event_acc.Reload()\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Failed to load {log_dir}: {e}\")\n",
    "            continue\n",
    "\n",
    "        tags = event_acc.Tags().get('scalars', [])\n",
    "        if 'novel_classes/accuracy' not in tags or 'base_classes/accuracy' not in tags:\n",
    "            print(f\"[INFO] Missing scalars in '{label}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        novel_events = event_acc.Scalars('novel_classes/accuracy')\n",
    "        base_events = event_acc.Scalars('base_classes/accuracy')\n",
    "\n",
    "        novel = novel_events[-1].value if novel_events else 0\n",
    "        base = base_events[-1].value if base_events else 0\n",
    "\n",
    "        # Harmonic mean: 2 * (base * novel) / (base + novel)\n",
    "        hmean = 2 * base * novel / (base + novel) if (base + novel) != 0 else 0\n",
    "\n",
    "        novel_values.append(novel)\n",
    "        base_values.append(base)\n",
    "        hmean_values.append(hmean)\n",
    "        run_labels.append(label)\n",
    "\n",
    "    # Plotting\n",
    "    x = np.arange(len(run_labels))\n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(x - width, base_values, width, label='Base Accuracy')\n",
    "    ax.bar(x,         novel_values, width, label='Novel Accuracy')\n",
    "    ax.bar(x + width, hmean_values, width, label='Harmonic Mean')\n",
    "\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title('Base / Novel / Harmonic Mean Accuracy per Run')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(run_labels, rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cCCtdM7MxA6Q",
   "metadata": {
    "id": "cCCtdM7MxA6Q"
   },
   "source": [
    "## Training metrics for most relevant runs in the non-adversarial training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EOcYpWhjw96e",
   "metadata": {
    "id": "EOcYpWhjw96e"
   },
   "outputs": [],
   "source": [
    "run_names = {\n",
    "    'kl v2 kl0.3' : './runs/report_no_pat/from_yaml_base_kl_v2_80_20_kl_03_rot_period_3_4_ctx_balanced_20250720_134756',\n",
    "    'kl v2 kl0.1' :  './runs/report_no_pat/from_yaml_base_kl_v2_80_20_kl_01_rot_period_4_4_ctx_base_acc_20250720_125637',\n",
    "    'kl v1' : './runs/report_no_pat/from_yaml_base_kl_v1_4_ctx_20250720_073108',\n",
    "    'kl v2 rotation period 6 ep' : './runs/report_no_pat/from_yaml_base_kl_v2_80_20_kl_03_rot_period_rel_4_ctx_novel_acc_20250720_133801',\n",
    "    'kl v2 kl0.1 8 ctx' : './runs/report_no_pat/from_yaml_base_kl_v2_80_20_kl_01_rot_period_4_8_ctx_20250719_125847',\n",
    "}\n",
    "scalar_names = [\n",
    "\n",
    "    'validation_novel/accuracy',\n",
    "    'validation_base/accuracy',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ArWfWjPLxDJA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ArWfWjPLxDJA",
    "outputId": "cec278ea-c006-4fd7-d04c-39827fa64d01"
   },
   "outputs": [],
   "source": [
    "plot_tensorboard_scalars_grid(run_names, scalar_names, max_cols=2)\n",
    "plot_novel_base_hmean_bar_chart(run_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zHDXDANExHkK",
   "metadata": {
    "id": "zHDXDANExHkK"
   },
   "source": [
    "### KL Divergence Training (KLv1 vs. KLv2)\n",
    "\n",
    "**KLv2** clearly outperforms **KLv1** in both base and novel accuracy, thanks to its staged curriculum. In KLv2, a full epoch is first used to train on seen classes with cross-entropy (CE), then another full epoch for distillation on unseen classes using the KL loss. This ensures sufficient class diversity and a stable signal for both objectives.\n",
    "\n",
    "In contrast, KLv1 splits pseudo-base and pseudo-novel classes within each mini-batch. This can result in limited or imbalanced class distributions, especially when batch sizes are small, weakening the effectiveness of KL training. Additionally, KLv1 tends to overfit after 4‚Äì5 epochs, leading to degradation in novel accuracy. KLv2 is more robust, converging quickly and tolerating longer training schedules.\n",
    "\n",
    "### Rotation Periods\n",
    "\n",
    "We evaluated several schedules for rotating the seen/unseen (pseudo-base/pseudo-novel) class split: every 1, 3, 4, 6 epochs, and random. While all configurations yield comparable results (harmonic mean between 0.71‚Äì0.76), rotating every 3‚Äì4 epochs performs best overall. Rotation every epoch is too frequent, not allowing enough time to adapt to a fixed pseudo-novel set. On the other hand, rotation every 6 epochs can lead to higher novel accuracy than base, suggesting underfitting on base classes.\n",
    "\n",
    "### KL Weight\n",
    "\n",
    "Adjusting the KL loss weight (`Œª_KL`) impacts the balance between base and novel performance. A value of **0.3** favors novel accuracy but may reduce base performance due to gradient conflict between CE and KL objectives. A lower value like **0.1** improves base accuracy and often yields a higher harmonic mean. We found `Œª_KL = 0.3` to offer a reasonable trade-off.\n",
    "\n",
    "### Rotation Periods Comparison\n",
    "\n",
    "We tested various rotation intervals for changing the seen/unseen class splits:\n",
    "\n",
    "- every epoch  \n",
    "- every 3 epochs  \n",
    "- every 4 epochs  \n",
    "- every 6 epochs  \n",
    "- random rotation  \n",
    "\n",
    "All approaches produced similar harmonic means (between **0.707** and **0.757**), but the best results were achieved when rotating every **3 or 4 epochs**.\n",
    "\n",
    "- Rotating every epoch is too frequent, limiting time for the model to distill zero-shot knowledge.  \n",
    "- Rotating every 6 epochs may cause underfitting on base classes, as the model focuses too long on a fixed unseen set.  \n",
    "- Interestingly, rotating every 6 epochs tends to result in higher novel accuracy than base accuracy.\n",
    "\n",
    "### `Œª_KL = 0.3` vs `Œª_KL = 0.1`\n",
    "\n",
    "Increasing `Œª_KL` emphasizes the objective of mimicking CLIP‚Äôs zero-shot predictions on unseen classes, which may lead to decreased performance on base classes.\n",
    "\n",
    "This is likely because gradients from the CE loss and KL loss can be orthogonal, reducing alignment during optimization.\n",
    "\n",
    "- `Œª_KL = 0.3` strikes the best balance between base and novel accuracy.  \n",
    "- `Œª_KL = 0.1` achieves higher base accuracy and harmonic mean compared to the `Œª = 0.3` setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZkPdhO6BxKer",
   "metadata": {
    "id": "ZkPdhO6BxKer"
   },
   "source": [
    "## Training metrics for most relevant mixed adversarial/pretrain+adversarial runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2FQmQRmHxE1M",
   "metadata": {
    "id": "2FQmQRmHxE1M"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Niente -> avg_text_feat + logits\n",
    "Noise-> avg_text_feat + noisy logits\n",
    "bias -> selected_text_feat + ctx_shifted.mean()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "BIAS FATTO LA NOTTE TRA IL 20 e 21 era diverso e non funzionava, mi serve solo per dire che non funziona\n",
    "bias -> ctx_shifted (as ctx_shifted = ctx+bias)\n",
    "\"\"\"\n",
    "run_names = {\n",
    "\n",
    "    'l-adv3.0+avg_txt_ft+noisy_logits':'./runs/report_no_pat/from_yaml_kl_pretrain_adv_4_ctx_4_clusters_adv_3_noise_20250720_215302',\n",
    "    'l-adv1.0+avg_txt_ft+logits':'./runs/report_no_pat/from_yaml_kl_pretrain_adv_4_ctx_4_clusters_20250720_164121',\n",
    "    'l-adv3.0+post_metanet_ctx' : './runs/report_no_pat/from_yaml_kl_pretrain_adv_4_ctx_4_clusters_adv_3_bias_20250721_004545',\n",
    "    'l-adv3.0+avg_post_metanet_ctx+selected_txt_ft':'./runs/report_no_pat/from_yaml_kl_pretrain_adv_4_ctx_4_clusters_adv_3_bias_20250721_130018',\n",
    "    'all adv 4 clusters' : './runs/report_no_pat/from_yaml_all_adv_4_ctx_4_clusters_20250721_104046',\n",
    "}\n",
    "scalar_names = [\n",
    "\n",
    "    'validation_novel/accuracy',\n",
    "    'validation_base/accuracy',\n",
    "    'train_adv/mlp_loss',\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stKCF10qxN6w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "stKCF10qxN6w",
    "outputId": "e8172cad-332d-4b71-d80d-75447b44f357"
   },
   "outputs": [],
   "source": [
    "plot_tensorboard_scalars_grid(run_names, scalar_names, max_cols=2)\n",
    "plot_novel_base_hmean_bar_chart(run_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uEiEWKjSxVUE",
   "metadata": {
    "id": "uEiEWKjSxVUE"
   },
   "source": [
    "## Adversarial Training\n",
    "\n",
    "Adversarial training, when applied **after KLv2**, significantly improves **test base accuracy** (up to 0.91) while maintaining **competitive test novel performance** (e.g., 0.71).  \n",
    "However, when used **in isolation**, it tends to **converge more slowly** and exhibits **instability**.  \n",
    "Using a **higher value of** `Œª_adv` (e.g., 3 vs. 1) improves the **stability of novel accuracy** across epochs, particularly in **longer training schedules** (e.g., 60+ epochs).\n",
    "\n",
    "### Discriminator's Input Impact\n",
    "\n",
    "- **With logits:**  \n",
    "  Feeding logits directly to the discriminator can be considered a form of \"cheating\", as the model may simply learn the mapping `recognized_category ‚Üí cluster`, bypassing any visual feature representation.  \n",
    "  To address this, **Gaussian noise** is added to the logits:\n",
    "  - Without noise: overfitting to base classes often occurs.\n",
    "  - With noise: novel accuracy remains slightly more stable, but the model still overfits to base classes.\n",
    "\n",
    "- **Without logits:**  \n",
    "  Learning meaningful cluster mappings from alternative features is challenging.  \n",
    "  We tested feeding the **MetaNet-biased prompt context** directly:\n",
    "  - Direct input resulted in poor performance (accuracy ‚âà 1.33 ‚âà random).\n",
    "  - Best results occurred when averaging the **MetaNet-biased prompt** with the **text feature of the predicted category** (from softmax output).  \n",
    "\n",
    "  This allowed the discriminator to learn category ‚Üí cluster mappings *without relying on logits*.\n",
    "\n",
    "Nevertheless, performance gains were marginal and comparable to setups where the discriminator did not learn at all.  \n",
    "This suggests that **any update to the prompt learner weights** may degrade the discriminator‚Äôs effectiveness similarly to exposing it directly to logits.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Training the model to convergence using KLv2 (`Œª = 0.3`) plus adversarial learning (4 clusters, `Œª_adv = 1.0`) results in:\n",
    "\n",
    "- Base accuracy ‚âà 0.90  \n",
    "- Novel accuracy ‚âà 0.71  \n",
    "\n",
    "These results align with CoCoOp paper benchmarks, with this configuration yielding the **highest harmonic mean**.\n",
    "\n",
    "---\n",
    "\n",
    "*Open question:*  \n",
    "If updates to prompt learner weights impair discriminator performance,  \n",
    "**Could such degradation lead to improved generalization on novel categories?**  \n",
    "Our adversarial learning hypothesis suggests: **Yes**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lI-htd6pxWjv",
   "metadata": {
    "id": "lI-htd6pxWjv"
   },
   "source": [
    "## Overall View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RkG8osrNxPIP",
   "metadata": {
    "id": "RkG8osrNxPIP"
   },
   "outputs": [],
   "source": [
    "run_names = {\n",
    "    'kl v2 kl0.3' : './runs/report_no_pat/from_yaml_base_kl_v2_80_20_kl_03_rot_period_3_4_ctx_balanced_20250720_134756',\n",
    "    'kl v2 kl0.1' :  './runs/report_no_pat/from_yaml_base_kl_v2_80_20_kl_01_rot_period_4_4_ctx_base_acc_20250720_125637',\n",
    "    'kl v1' : './runs/report_no_pat/from_yaml_base_kl_v1_4_ctx_20250720_073108',\n",
    "    'kl v2 rotation period 6 ep' : './runs/report_no_pat/from_yaml_base_kl_v2_80_20_kl_03_rot_period_rel_4_ctx_novel_acc_20250720_133801',\n",
    "    'kl v2 kl0.1 8 ctx' : './runs/report_no_pat/from_yaml_base_kl_v2_80_20_kl_01_rot_period_4_8_ctx_20250719_125847',\n",
    "    'l-adv3.0+avg_txt_ft+noisy_logits':'./runs/report_no_pat/from_yaml_kl_pretrain_adv_4_ctx_4_clusters_adv_3_noise_20250720_215302',\n",
    "    'l-adv1.0+avg_txt_ft+logits':'./runs/report_no_pat/from_yaml_kl_pretrain_adv_4_ctx_4_clusters_20250720_164121',\n",
    "    'l-adv3.0+post_metanet_ctx' : './runs/report_no_pat/from_yaml_kl_pretrain_adv_4_ctx_4_clusters_adv_3_bias_20250721_004545',\n",
    "    'l-adv3.0+avg_post_metanet_ctx+selected_txt_ft':'./runs/report_no_pat/from_yaml_kl_pretrain_adv_4_ctx_4_clusters_adv_3_bias_20250721_130018',\n",
    "    'all adv 4 clusters' : './runs/report_no_pat/from_yaml_all_adv_4_ctx_4_clusters_20250721_104046',\n",
    "}\n",
    "scalar_names = [\n",
    "    'validation_novel/accuracy',\n",
    "    'validation_base/accuracy',\n",
    "    'train_adv/mlp_loss',\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kQ7EH_tGxRQJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "kQ7EH_tGxRQJ",
    "outputId": "4418abed-469c-491e-e8c1-ed5652ef880c"
   },
   "outputs": [],
   "source": [
    "plot_tensorboard_scalars_grid(run_names, scalar_names, max_cols=2)\n",
    "plot_novel_base_hmean_bar_chart(run_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc1bc0531bb49e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Impact of Prompt Length\n",
    "Prompt length influences generalization. Shorter prompts ($n_{\\text{ctx}} = 4$) outperform longer ones ($n_{\\text{ctx}} = 8$), particularly on novel classes. Longer prompts offer more capacity but tend to overfit faster.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f7c5957d18f33",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this project, we explored ways to improve CoCoOp, a method that builds on CLIP to help generalize better in zero-shot image classification tasks. Our focus was on two main strategies: a curriculum learning method based on KL divergence, and a second training phase that uses an adversarial MLP network.\n",
    "\n",
    "We developed a two-stage training approach (which we called **KLv2**) that slowly shifts supervision from base classes to pseudo-novel ones. Compared to a simpler, single-stage version (**KLv1**), KLv2 led to better accuracy and faster, more stable training.\n",
    "\n",
    "We also experimented with **adversarial training**, where we used a small MLP network to predict cluster labels. While this didn‚Äôt boost novel class performance on its own, it worked well as a **regularizer when used after KLv2**, improving base class accuracy and helping prevent overfitting.\n",
    "\n",
    "Throughout the project, we looked closely at how different design choices‚Äîlike the **length of the context prompt** or the **type of features fed into the MLP**‚Äîaffect the results. We also confirmed that using **data augmentation similar to what CLIP was trained on** helped improve generalization to novel classes.\n",
    "\n",
    "**Overall**, this project showed how thoughtful training strategies and architectural tweaks can make prompt learning systems like CoCoOp more robust. The combination of curriculum learning and regularization turned out to be especially effective for balancing performance between base and novel classes.\n",
    "\n",
    "## Future Work\n",
    "\n",
    "In future work, we aim to **enhance the adversarial training component** by replacing the current MLP with a **lightweight transformer architecture**. This would allow better modeling of the interactions between context tokens and potentially improve the adversary‚Äôs effectiveness. Additionally, we consider **dividing the available training data into specialized subsets** to enable targeted training strategies.\n",
    "\n",
    "One promising direction is to **incorporate contrastive loss functions** on the prompt embeddings. Contrastive learning encourages the model to bring similar class prompts closer in the feature space while pushing apart dissimilar ones. This approach could help improve the **representation quality and generalization ability**, especially for novel classes, by explicitly enforcing class-discriminative embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e663efd0319c1baa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## References\n",
    "\n",
    "[1^] Yaroslav Ganin and Victor Lempitsky. *Unsupervised Domain Adaptation by Backpropagation*. 2015. arXiv: [1409.7495](https://arxiv.org/abs/1409.7495) [stat.ML].  \n",
    "[2^] Alec Radford et al. *Learning Transferable Visual Models From Natural Language Supervision*. 2021. arXiv: [2103.00020](https://arxiv.org/abs/2103.00020) [cs.CV].  \n",
    "[3^] Kaiyang Zhou et al. *Conditional Prompt Learning for Vision-Language Models*. 2022. arXiv: [2203.05557](https://arxiv.org/abs/2203.05557) [cs.CV].  \n",
    "[4^] Kaiyang Zhou et al. ‚ÄúLearning to Prompt for Vision-Language Models‚Äù. In: *CoRR* abs/2109.01134 (2021). arXiv: [2109.01134](https://arxiv.org/abs/2109.01134)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
