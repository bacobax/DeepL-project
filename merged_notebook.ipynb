{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9338c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f183b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./.ipynb_checkpoints/training_system-checkpoint.py\n",
    "from easydict import EasyDict\n",
    "\n",
    "# from model.cocoop.custom_clip import CustomCLIP\n",
    "# from utils.datasets import  get_data, base_novel_categories, split_data, CLASS_NAMES\n",
    "# from utils.debugging import test_step, training_step, eval_step\n",
    "import clip\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import SGD\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CoCoOpSystem:\n",
    "    def __init__(self,\n",
    "                 batch_size=16,\n",
    "                 num_classes=10,\n",
    "                 device=\"cuda:0\",\n",
    "                 learning_rate=0.002,\n",
    "                 weight_decay=0.0005,\n",
    "                 momentum=0.9,\n",
    "                 epochs=2,\n",
    "                 run_name=\"exp1\",\n",
    "                 n_ctx=4,\n",
    "                 ctx_init=\"\",\n",
    "                 class_token_position=\"end\",\n",
    "                 csc=False,\n",
    "                 ):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.epochs = epochs\n",
    "        self.run_name = run_name\n",
    "        self.n_ctx = n_ctx\n",
    "        self.ctx_init = ctx_init\n",
    "        self.class_token_position = class_token_position\n",
    "        self.csc = csc\n",
    "\n",
    "        # Create a logger for the experiment\n",
    "        self.writer = SummaryWriter(log_dir=f\"runs/{run_name}\")\n",
    "\n",
    "        # Get dataloaders\n",
    "\n",
    "        self.clip_model, preprocess = clip.load(\"RN50\")\n",
    "        self.train_set, self.val_set, self.test_set = get_data(transform=preprocess)\n",
    "\n",
    "        # split classes into base and novel\n",
    "        self.base_classes, self.novel_classes = base_novel_categories(self.train_set)\n",
    "\n",
    "        # split the three datasets\n",
    "        self.train_base, _ = split_data(self.train_set, self.base_classes)\n",
    "        self.val_base, _ = split_data(self.val_set, self.base_classes)\n",
    "        self.test_base, self.test_novel = split_data(self.test_set, self.base_classes)\n",
    "\n",
    "        #self.classnames, _ = embed_dataset_classnames(dataset_name, preprocess=preprocess, model=clip_model)\n",
    "\n",
    "        resolution = self.clip_model.visual.input_resolution\n",
    "\n",
    "        cfg = EasyDict()\n",
    "        # Training configuration\n",
    "        cfg.TRAINER = EasyDict()\n",
    "        cfg.TRAINER.COCOOP = EasyDict()\n",
    "        cfg.TRAINER.COCOOP.N_CTX = self.n_ctx  # Number of context tokens\n",
    "        cfg.TRAINER.COCOOP.CTX_INIT = self.ctx_init  # Leave empty for random initialization\n",
    "        cfg.TRAINER.COCOOP.PREC = \"fp16\"  # Precision for meta network\n",
    "        cfg.INPUT = EasyDict()\n",
    "        cfg.INPUT.SIZE = [resolution, resolution]  # Must match CLIP model's input resolution\n",
    "\n",
    "        # Instantiate the network and move it to the chosen device (GPU)\n",
    "        self.model = CustomCLIP(\n",
    "            classnames=[CLASS_NAMES[idx] for idx in self.base_classes],\n",
    "            cfg=cfg,\n",
    "            clip_model=self.clip_model,\n",
    "        ).to(device)\n",
    "\n",
    "        print(\"Turning off gradients in both the image and the text encoder\")\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"prompt_learner\" not in name:\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        print(f\"Total parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        print(f\"Total trainable parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "        self.optimizer = self.get_optimizer(self.learning_rate, self.weight_decay, self.momentum)\n",
    "        self.cost_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self):\n",
    "        print(\"Before training:\")\n",
    "        self.compute_evaluation(-1, base=True)\n",
    "        # For each epoch, train the network and then compute evaluation results\n",
    "        for e in range(self.epochs):\n",
    "            train_loss, train_accuracy = training_step(\n",
    "                model=self.model,\n",
    "                dataset=self.train_base,\n",
    "                optimizer=self.optimizer,\n",
    "                batch_size=self.batch_size,\n",
    "                device=self.device,\n",
    "            )\n",
    "            val_loss, val_accuracy = eval_step(\n",
    "                model=self.model,\n",
    "                dataset=self.val_base,\n",
    "                cost_function=self.cost_function,\n",
    "                device=self.device,\n",
    "                batch_size=self.batch_size,\n",
    "            )\n",
    "\n",
    "            self.log_values(e, train_loss, train_accuracy, \"train\")\n",
    "            self.log_values(e, val_loss, val_accuracy, \"validation\")\n",
    "\n",
    "        print(\"After training:\")\n",
    "        self.compute_evaluation(self.epochs)\n",
    "        self.writer.close()\n",
    "\n",
    "    def compute_evaluation(self, epoch_idx, base=False):\n",
    "        base_accuracy = test_step(self.model if not base else self.clip_model, self.test_base, self.base_classes, self.batch_size, self.device, label=\"test\", base=base)\n",
    "        novel_accuracy = test_step(self.model if not base else self.clip_model, self.test_novel, self.novel_classes, self.batch_size, self.device, label=\"test\", base=base)\n",
    "        # Log to TensorBoard\n",
    "        self.log_value(epoch_idx,  base_accuracy, \"base_classes\")\n",
    "        self.log_value(epoch_idx,  novel_accuracy, \"novel_classes\")\n",
    "\n",
    "        return base_accuracy, novel_accuracy\n",
    "\n",
    "    def get_optimizer(self, lr, wd, momentum):\n",
    "        optimizer = SGD([\n",
    "            {\n",
    "                \"params\": self.model.parameters()\n",
    "            }\n",
    "        ], lr=lr, weight_decay=wd, momentum=momentum)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def log_value(self, step,  accuracy, prefix):\n",
    "        self.writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "    def log_values(self, step, loss, accuracy, prefix):\n",
    "        self.writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "        self.writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1acf685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./main.py\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "# from training_systems.cocoop import CoCoOpSystem\n",
    "# from training_systems.coop import CoOpSystem\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--device', default=os.getenv(\"DEVICE\", \"cuda:0\"))\n",
    "    parser.add_argument('--run_name', default=f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    parser.add_argument('--using_coop', default=False, type=lambda x: x.lower() in (\"1\", \"true\", \"yes\", \"true\"))\n",
    "    parser.add_argument('--config', default=\"train_config.yaml\")\n",
    "    parser.add_argument('--debug', default=True, type=lambda x: x.lower() in (\"1\", \"true\", \"yes\", \"true\"))\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Parse command-line arguments\n",
    "#     # Assign parsed arguments to variables\n",
    "#     # Display which device is being used\n",
    "#     # Handle MPS backend by setting default tensor type to float32\n",
    "#     # Indicate whether CoOp or CoCoOp is used for training\n",
    "#     # Load training configuration from YAML file\n",
    "\n",
    "#     # Initialize and train using CoOpSystem if specified in arguments\n",
    "#     # Initialize and train using CoCoOpSystem otherwise\n",
    "#     args = parse_args()\n",
    "\n",
    "#     device = args.device\n",
    "#     run_name = args.run_name\n",
    "#     debug = args.debug\n",
    "#     use_coop = args.using_coop\n",
    "\n",
    "#     print(f\"Using device: {device}\")\n",
    "\n",
    "#     if torch.backends.mps.is_available():\n",
    "#         print(\"\\u26a0\\ufe0f Forcing float32 due to MPS limitations\")\n",
    "#         torch.set_default_dtype(torch.float32)\n",
    "\n",
    "#     print(f\"Using {'CoOp' if use_coop else 'CoCoOp'} for training\")\n",
    "\n",
    "#     # Load hyperparameters from YAML\n",
    "#     with open(args.config, \"r\") as file:\n",
    "#         config = yaml.safe_load(file)\n",
    "\n",
    "#     if use_coop:\n",
    "#         coop_cfg = config['coop']\n",
    "#         train_sys = CoOpSystem(\n",
    "#             device=device,\n",
    "#             run_name=run_name,\n",
    "#             **coop_cfg\n",
    "#         )\n",
    "#     else:\n",
    "#         cocoop_cfg = config['cocoop']\n",
    "#         train_sys = CoCoOpSystem(\n",
    "#             device=device,\n",
    "#             run_name=run_name,\n",
    "#             debug=debug,\n",
    "#             hparams_file=args.config,\n",
    "#             **cocoop_cfg\n",
    "#         )\n",
    "\n",
    "#     train_sys.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b34d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./model/cocoop/custom_clip.py\n",
    "import os.path as osp\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "# from model.cocoop.prompt_learner import PromptLearner\n",
    "# from model.cocoop.mlp_adversary import GradientReversalLayer, AdversarialMLP\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "from easydict import EasyDict\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "\n",
    "# class TextEncoder(nn.Module):\n",
    "#     \"\"\"\n",
    "#     TextEncoder wraps CLIP's internal transformer-based text encoder.\n",
    "#     It encodes tokenized prompts with learned positional embeddings and projects them to the shared feature space.\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     Initialize the TextEncoder by extracting relevant CLIP transformer components.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, clip_model):\n",
    "#         super().__init__()\n",
    "#         self.transformer = clip_model.transformer\n",
    "#         self.positional_embedding = clip_model.positional_embedding\n",
    "#         self.ln_final = clip_model.ln_final\n",
    "#         self.text_projection = clip_model.text_projection\n",
    "#         self.dtype = clip_model.dtype\n",
    "#         print(f\"self.dtype={self.dtype}\")\n",
    "\n",
    "#     \"\"\"\n",
    "#     Encode a batch of prompts into text features using the transformer and positional embeddings.\n",
    "#     Returns the end-of-text (EOT) token representations projected into the feature space.\n",
    "#     \"\"\"\n",
    "#     def forward(self, prompts, tokenized_prompts):\n",
    "#         x = prompts + self.positional_embedding.type(self.dtype)\n",
    "#         x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "#         x = self.transformer(x.to(dtype=torch.float32))\n",
    "#         x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "#         x = self.ln_final(x).type(self.dtype)\n",
    "\n",
    "#         # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "#         # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "#         x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "class CustomCLIP(nn.Module):\n",
    "    \"\"\"\n",
    "    CustomCLIP is a modified CLIP wrapper supporting CoCoOp prompt learning and adversarial training extensions.\n",
    "    It enables image-text matching via learned prompts and exposes additional methods for class manipulation.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    Initialize the CustomCLIP model with the given configuration, classnames, and base CLIP model.\n",
    "    Sets up the image and text encoders and prepares the prompt learner.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = PromptLearner(cfg, classnames, clip_model)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.dtype = clip_model.dtype\n",
    "        self.clip_model = clip_model\n",
    "        self.cfg = cfg\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Updates the prompt learner with a new list of classnames and resets its tokenized prompts.\n",
    "    \"\"\"\n",
    "    def change_classnames(self, new_classnames):\n",
    "        temp_prompt_learner = PromptLearner(\n",
    "            cfg=self.cfg,\n",
    "            classnames=new_classnames,\n",
    "            clip_model=self.clip_model\n",
    "        )\n",
    "\n",
    "        self.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.token_prefix = temp_prompt_learner.token_prefix\n",
    "        self.prompt_learner.token_suffix = temp_prompt_learner.token_suffix\n",
    "        self.prompt_learner.n_cls = len(new_classnames)\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    A context manager that temporarily sets new classnames for inference, restoring the original ones afterward.\n",
    "    \"\"\"\n",
    "    @contextmanager\n",
    "    def temporary_classnames(self, new_classnames):\n",
    "        # --- Save original state ---\n",
    "        original_classnames = self.prompt_learner.n_cls\n",
    "        original_tokenized_prompts = self.tokenized_prompts\n",
    "        original_token_prefix = self.prompt_learner.token_prefix\n",
    "        original_token_suffix = self.prompt_learner.token_suffix\n",
    "\n",
    "        # --- Apply temporary state ---\n",
    "        temp_prompt_learner = PromptLearner(\n",
    "            cfg=self.cfg,\n",
    "            classnames=new_classnames,\n",
    "            clip_model=self.clip_model\n",
    "        )\n",
    "\n",
    "        self.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.token_prefix = temp_prompt_learner.token_prefix\n",
    "        self.prompt_learner.token_suffix = temp_prompt_learner.token_suffix\n",
    "        self.prompt_learner.n_cls = len(new_classnames)\n",
    "\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            # --- Restore original state ---\n",
    "            self.tokenized_prompts = original_tokenized_prompts\n",
    "            self.prompt_learner.tokenized_prompts = original_tokenized_prompts\n",
    "            self.prompt_learner.token_prefix = original_token_prefix\n",
    "            self.prompt_learner.token_suffix = original_token_suffix\n",
    "            self.prompt_learner.n_cls = original_classnames\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Performs a forward pass through the model, computing similarity logits between image features and prompt-conditioned text features.\n",
    "    Optionally returns loss, image features, and intermediate representations during training.\n",
    "    \"\"\"\n",
    "    def forward(self, image, label=None, get_image_features=False):\n",
    "        # tokenized_prompts: [num_classes, context_length] (e.g., [10, 77])\n",
    "        tokenized_prompts = self.tokenized_prompts\n",
    "\n",
    "        # logit_scale: scalar (e.g., initialized as a learnable parameter like torch.tensor(1/0.07).log())\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        # image: [B, 3, H, W]\n",
    "        # image_features: [B, D] where D = transformer width (e.g., 512 for ViT-B/32)\n",
    "        #print(f\"image device: {image.device} | image encoder device: {next(self.image_encoder.parameters()).device}\")\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        if image_features.isnan().any():\n",
    "            raise ValueError(\"NaN detected in image_features.\")\n",
    "        # Normalize image features: each row to unit length\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # prompts: List of [num_classes, context_length, D] (one per image feature)\n",
    "        # Each element is generated conditioned on an image feature\n",
    "        prompts, ctx, bias = self.prompt_learner(image_features) # [B , n_cls, n_ctx, D]\n",
    "        if prompts.isnan().any():\n",
    "            raise ValueError(\"NaN detected in prompts.\")\n",
    "        # prompts: [B, n_cls, n_ctx, D] -> [B * n_cls, n_ctx, D]\n",
    "        logits = []\n",
    "        all_text_features = []\n",
    "        selected_text_features = []    \n",
    "        # Iterate over batch\n",
    "        for pts_i, imf_i in zip(prompts, image_features):\n",
    "            # pts_i: [num_classes, context_length, D]\n",
    "            # tokenized_prompts: [num_classes, context_length]\n",
    "            # text_features: [num_classes, D]\n",
    "            text_features = self.text_encoder(pts_i, tokenized_prompts)\n",
    "            if text_features.isnan().any():\n",
    "                raise ValueError(\"NaN detected in text ft.\")\n",
    "            all_text_features.append(text_features)\n",
    "            # Normalize text features\n",
    "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # imf_i: [D], text_features.T: [D, num_classes]\n",
    "            # l_i: [num_classes], similarity scores between image and all class prompts\n",
    "            l_i = logit_scale * imf_i @ text_features.t()\n",
    "            # Append l_i (1D tensor) to logits list\n",
    "            logits.append(l_i)\n",
    "            best_idx = l_i.argmax()\n",
    "            best_text_feat = text_features[best_idx]  # [D]\n",
    "            selected_text_features.append(best_text_feat)\n",
    "\n",
    "        all_text_features = torch.stack(all_text_features)  # [B, num_classes, D]\n",
    "        #avarage over num_classes\n",
    "        avg_text_features = all_text_features.mean(dim=1)\n",
    "\n",
    "        # Shape: [B, D]\n",
    "        selected_text_features = torch.stack(selected_text_features)\n",
    "        # logits: list of B tensors each of shape [num_classes]\n",
    "        # stacked into a tensor of shape [B, num_classes]\n",
    "        logits = torch.stack(logits)\n",
    "        if logits.isnan().any():\n",
    "            raise ValueError(\"NaN detected in logits\")\n",
    "\n",
    "        # If in training mode, compute and return cross-entropy loss\n",
    "        if self.prompt_learner.training:\n",
    "            # logits: [B, num_classes], label: [B]\n",
    "            if get_image_features:\n",
    "                # If get_image_features is True, return logits and image features\n",
    "                return logits, F.cross_entropy(logits, label), image_features, ctx, bias, avg_text_features, selected_text_features\n",
    "            else:\n",
    "                return logits, F.cross_entropy(logits, label)\n",
    "\n",
    "        # Otherwise, return logits for evaluation: [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "    \"\"\"\n",
    "    Utility function to print the data types of all parameters and buffers in each model component for debugging.\n",
    "    \"\"\"\n",
    "    def print_all_dtypes(self):\n",
    "        print(f\"CustomCLIP dtype: {self.dtype}\")\n",
    "        print(f\"  logit_scale dtype: {getattr(self.logit_scale, 'dtype', type(self.logit_scale))}\")\n",
    "        print(f\"  tokenized_prompts dtype: {getattr(self.tokenized_prompts, 'dtype', type(self.tokenized_prompts))}\")\n",
    "        print(f\"  image_encoder: {type(self.image_encoder)}\")\n",
    "        for name, param in self.image_encoder.named_parameters():\n",
    "            print(f\"    image_encoder param {name}: {param.dtype}\")\n",
    "        for name, buf in self.image_encoder.named_buffers():\n",
    "            print(f\"    image_encoder buffer {name}: {buf.dtype}\")\n",
    "        print(f\"  text_encoder: {type(self.text_encoder)}\")\n",
    "        for name, param in self.text_encoder.named_parameters():\n",
    "            print(f\"    text_encoder param {name}: {param.dtype}\")\n",
    "        for name, buf in self.text_encoder.named_buffers():\n",
    "            print(f\"    text_encoder buffer {name}: {buf.dtype}\")\n",
    "        print(f\"  prompt_learner: {type(self.prompt_learner)}\")\n",
    "        for name, param in self.prompt_learner.named_parameters():\n",
    "            print(f\"    prompt_learner param {name}: {param.dtype}\")\n",
    "        for name, buf in self.prompt_learner.named_buffers():\n",
    "            print(f\"    prompt_learner buffer {name}: {buf.dtype}\")\n",
    "        # Also print dtype for ctx, token_prefix, token_suffix if present\n",
    "        if hasattr(self.prompt_learner, 'ctx'):\n",
    "            print(f\"    prompt_learner.ctx dtype: {self.prompt_learner.ctx.dtype}\")\n",
    "        if hasattr(self.prompt_learner, 'token_prefix'):\n",
    "            print(f\"    prompt_learner.token_prefix dtype: {self.prompt_learner.token_prefix.dtype}\")\n",
    "        if hasattr(self.prompt_learner, 'token_suffix'):\n",
    "            print(f\"    prompt_learner.token_suffix dtype: {self.prompt_learner.token_suffix.dtype}\")\n",
    "        print(f\"  clip_model: {type(self.clip_model)}\")\n",
    "        for name, param in self.clip_model.named_parameters():\n",
    "            print(f\"    clip_model param {name}: {param.dtype}\")\n",
    "        for name, buf in self.clip_model.named_buffers():\n",
    "            print(f\"    clip_model buffer {name}: {buf.dtype}\")\n",
    "    \"\"\"\n",
    "    Loads a CustomCLIP instance from a saved checkpoint.\n",
    "    Sets up model configuration, restores state dict, and reassigns classnames.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def load_from_checkpoint(classnames, checkpoint_path, device, n_ctx, clip_model, ctx_8_coop, ctx_4_coop, ctx_init=\"\"):\n",
    "        ctx_load = (\n",
    "            ctx_4_coop\n",
    "            if n_ctx == 4\n",
    "            else ctx_8_coop\n",
    "        )\n",
    "        resolution = clip_model.visual.input_resolution\n",
    "        cfg = EasyDict(\n",
    "            {\n",
    "                \"TRAINER\": {\n",
    "                    \"COCOOP\": {\n",
    "                        \"CTX_LOAD\": ctx_load,\n",
    "                        \"N_CTX\": n_ctx,\n",
    "                        \"CTX_INIT\": ctx_init,\n",
    "                        \"PREC\": \"fp16\",\n",
    "                    }\n",
    "                },\n",
    "                \"INPUT\": {\"SIZE\": [resolution, resolution]},\n",
    "            }\n",
    "        )\n",
    "        state_dict = torch.load(checkpoint_path, map_location=device)\n",
    "        n_cls = state_dict[\"prompt_learner.token_prefix\"].shape[0]\n",
    "\n",
    "        clip_model, _ = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "        model = CustomCLIP(cfg, [\"X\"] * n_cls, clip_model)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.change_classnames(classnames)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e515a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./model/cocoop/mlp_adversary.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual block with a linear layer, layer normalization, ReLU activation, dropout, and optional identity or projection shortcut.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(dim_in, dim_out)\n",
    "        self.norm = nn.LayerNorm(dim_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.residual = (\n",
    "            nn.Identity() if dim_in == dim_out else nn.Linear(dim_in, dim_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the residual block with normalization, activation, dropout, and residual connection.\n",
    "        \"\"\"\n",
    "        out = self.linear(x)\n",
    "        out = self.norm(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop(out)\n",
    "        return out + self.residual(x)\n",
    "\n",
    "\n",
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Implements a gradient reversal layer as a custom autograd function, useful in adversarial training.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        \"\"\"\n",
    "        Forward pass that returns the input as-is and stores the lambda factor for use in the backward pass.\n",
    "        \"\"\"\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass that reverses the gradient by multiplying it by -lambda.\n",
    "        \"\"\"\n",
    "        return -ctx.lambda_ * grad_output, None\n",
    "\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for the GradientReversalFunction to integrate it into a standard nn.Module.\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_=1.0):\n",
    "        super().__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Applies the gradient reversal function with the stored lambda parameter.\n",
    "        \"\"\"\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)\n",
    "\n",
    "\n",
    "class AdversarialMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron with optional bias context support, designed for adversarial learning.\n",
    "    Uses residual blocks for intermediate layers and configurable final output dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, opt, output_dim=1, use_bias_ctx=False, n_ctx=4):\n",
    "        \"\"\"\n",
    "        Initializes the adversarial MLP with given structure and optional bias context input.\n",
    "        Builds the network using residual blocks and applies Xavier initialization to weights.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dims = opt.hidden_structure\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        if use_bias_ctx:\n",
    "            # Add a bias context layer if specified\n",
    "            layers.append(nn.Linear(512*2, hidden_dims[0]))\n",
    "            layers.append(nn.ReLU())\n",
    "            for in_dim, out_dim in zip(hidden_dims[:-1], hidden_dims[1:]):\n",
    "                layers.append(ResidualBlock(in_dim, out_dim))\n",
    "        else:\n",
    "            for in_dim, out_dim in zip(hidden_dims[:-1], hidden_dims[1:]):\n",
    "                layers.append(ResidualBlock(in_dim, out_dim))\n",
    "\n",
    "        # Final output layer with configurable output_dim\n",
    "        layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "        self.model.apply(self.init_weights.__get__(self, AdversarialMLP))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the adversarial MLP model.\n",
    "        \"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass and applies a sigmoid activation to the output.\n",
    "        Squeezes output if it's a single dimension.\n",
    "        \"\"\"\n",
    "        out = self.forward(x)\n",
    "        return torch.sigmoid(out).squeeze(-1) if out.shape[-1] == 1 else torch.sigmoid(out)\n",
    "\n",
    "    def init_weights(self, m):\n",
    "        \"\"\"\n",
    "        Initializes weights of linear layers using Xavier uniform distribution and biases to zero.\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a8b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./model/cocoop/prompt_learner.py\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from clip import clip\n",
    "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
    "\n",
    "_tokenizer = _Tokenizer()\n",
    "\n",
    "\n",
    "class PromptLearner(nn.Module):\n",
    "    \"\"\"\n",
    "    The PromptLearner dynamically generates context-conditioned prompts for each class using a meta-network and learned context vectors.\n",
    "    This module supports both random and pre-initialized context tokens, and computes prompt embeddings compatible with CLIP.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        \"\"\"\n",
    "        Initialize the PromptLearner with configuration parameters, class names, and a CLIP model.\n",
    "        Loads or initializes context vectors and builds the meta-network for instance-conditioned prompt adaptation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        n_ctx = cfg.TRAINER.COCOOP.N_CTX\n",
    "        ctx_init = cfg.TRAINER.COCOOP.CTX_INIT\n",
    "        dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        vis_dim = clip_model.visual.output_dim\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "        cfg_imsize = cfg.INPUT.SIZE[0]\n",
    "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})\"\n",
    "\n",
    "        if ctx_init:\n",
    "            # use given words to initialize context vectors\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            # random initialization\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        # Optional: Load pre-trained ctx from a file\n",
    "        if hasattr(cfg.TRAINER.COCOOP, \"CTX_LOAD\") and cfg.TRAINER.COCOOP.CTX_LOAD:\n",
    "            ctx_path = cfg.TRAINER.COCOOP.CTX_LOAD\n",
    "            if os.path.isfile(ctx_path):\n",
    "                #print(f\"🔁 Loading ctx from: {ctx_path}\")\n",
    "                state_dict = torch.load(ctx_path, map_location=\"cpu\")\n",
    "                if \"ctx\" in state_dict:\n",
    "                    with torch.no_grad():\n",
    "                        self.ctx.copy_(state_dict[\"ctx\"])\n",
    "                else:\n",
    "                    raise KeyError(f\"'ctx' not found in {ctx_path}\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"CTX_LOAD path not found: {ctx_path}\")\n",
    "            #print(\"PROMPT LEARNER LOADED FROM A COOP PRETRAINED ONE\")\n",
    "\n",
    "        self.meta_net = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
    "            (\"relu\", nn.ReLU(inplace=True)),\n",
    "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
    "        ]))\n",
    "\n",
    "        if cfg.TRAINER.COCOOP.PREC == \"fp16\" and not torch.backends.mps.is_available():\n",
    "            self.meta_net.half()\n",
    "        else:\n",
    "            print(\"⚠️ Using float32 for meta_net due to MPS\")\n",
    "\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])  # (n_cls, n_tkn)\n",
    "        with torch.no_grad():\n",
    "            device = clip_model.token_embedding.weight.device\n",
    "\n",
    "            # Ensure tokenized_prompts is on the right device BEFORE embedding\n",
    "            tokenized_prompts = tokenized_prompts.to(device)\n",
    "\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts)\n",
    "\n",
    "            # Do not convert to fp16 on MPS (Apple doesn't support it fully)\n",
    "            if device.type == \"mps\" and dtype == torch.float16:\n",
    "                print(\"⚠️ fp16 not fully supported on MPS; using float32 instead\")\n",
    "                dtype = torch.float32\n",
    "\n",
    "            embedding = embedding.to(dtype)\n",
    "        # These token vectors will be saved when in save_model(),\n",
    "        # but they should be ignored in load_model() as we want to use\n",
    "        # those computed using the current class names\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
    "\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "        self.tokenized_prompts = tokenized_prompts  # torch.Tensor\n",
    "        self.name_lens = name_lens\n",
    "\n",
    "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
    "        \"\"\"\n",
    "        Construct the full tokenized prompt from context tokens, prefix (SOS), and suffix (CLS, EOS).\n",
    "        Optionally uses label indexing for training-time class selection.\n",
    "        \"\"\"\n",
    "        # dim0 is either batch_size (during training) or n_cls (during testing)\n",
    "        # ctx: context tokens, with shape of (dim0, n_ctx, ctx_dim)\n",
    "        # prefix: the sos token, with shape of (n_cls, 1, ctx_dim)\n",
    "        # suffix: remaining tokens, with shape of (n_cls, *, ctx_dim)\n",
    "\n",
    "        if label is not None:\n",
    "            prefix = prefix[label]\n",
    "            suffix = suffix[label]\n",
    "\n",
    "        prompts = torch.cat(\n",
    "            [\n",
    "                prefix,  # (dim0, 1, dim)\n",
    "                ctx,     # (dim0, n_ctx, dim)\n",
    "                suffix,  # (dim0, *, dim)\n",
    "            ],\n",
    "            dim=1,\n",
    "        )\n",
    "\n",
    "        return prompts\n",
    "\n",
    "    def forward(self, im_features):\n",
    "        \"\"\"\n",
    "        Generate the context-conditioned prompts for each class based on input image features.\n",
    "        Applies the meta-network to compute per-instance bias vectors, which shift the context tokens.\n",
    "        Returns the generated prompts along with the original context and computed bias.\n",
    "        \"\"\"\n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        ctx = self.ctx                     # (n_ctx, ctx_dim)\n",
    "        if im_features.isnan().any():\n",
    "            raise ValueError(\"NaN in im_features before meta_net\")\n",
    "        \n",
    "        #print(\"im_features stats\", im_features.min().item(), im_features.max().item(), im_features.norm(dim=1).mean().item())\n",
    "\n",
    "        meta_net_dtype = next(self.meta_net.parameters()).dtype\n",
    "        # print(f\"meta_net_dtype: {meta_net_dtype}, im_features dtype: {im_features.dtype}\")\n",
    "        bias = self.meta_net(im_features.to(meta_net_dtype))  # (batch, ctx_dim)\n",
    "        if bias.isnan().any():\n",
    "            raise ValueError(\"NaN detected in bias\")\n",
    "        bias = bias.unsqueeze(1)           # (batch, 1, ctx_dim)\n",
    "        ctx = ctx.unsqueeze(0)             # (1, n_ctx, ctx_dim)\n",
    "        ctx_shifted = ctx + bias           # (batch, n_ctx, ctx_dim)\n",
    "        if ctx_shifted.isnan().any():\n",
    "            raise ValueError(\"NaN detected in ctx_shifted\")\n",
    "        # Use instance-conditioned context tokens for all classes\n",
    "        prompts = []\n",
    "        for ctx_shifted_i in ctx_shifted:\n",
    "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1) # (n_cls, n_ctx, ctx_dim)\n",
    "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tkn, ctx_dim)\n",
    "            if pts_i.isnan().any():\n",
    "                raise ValueError(\"NaN detected in pts_i\")\n",
    "            prompts.append(pts_i)\n",
    "        prompts = torch.stack(prompts)\n",
    "\n",
    "        return prompts, ctx, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc49c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./model/coop/custom_clip.py\n",
    "import clip\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, clip_model):\n",
    "        super().__init__()\n",
    "        self.transformer = clip_model.transformer\n",
    "        self.positional_embedding = clip_model.positional_embedding\n",
    "        self.ln_final = clip_model.ln_final\n",
    "        self.text_projection = clip_model.text_projection\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts):\n",
    "        x = prompts + self.positional_embedding\n",
    "        x = x.permute(1, 0, 2)  # [batch_size, n_ctx, transformer.width] -> [n_ctx, batch_size, transformer.width]\n",
    "        x = self.transformer(x.to(dtype=self.transformer.resblocks[0].mlp.c_fc.weight.dtype))\n",
    "        x = x.permute(1, 0, 2)  # [n_ctx, batch_size, transformer.width] -> [batch_size, n_ctx, transformer.width]\n",
    "        x = self.ln_final(x)\n",
    "\n",
    "        # Take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        \n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "class PromptLearnerCoOp(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        n_cls = len(classnames)\n",
    "        n_ctx = cfg.TRAINER.COOP.N_CTX\n",
    "        ctx_init = cfg.TRAINER.COOP.CTX_INIT\n",
    "        dtype = clip_model.dtype\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
    "        clip_imsize = clip_model.visual.input_resolution\n",
    "        cfg_imsize = cfg.INPUT.SIZE[0]\n",
    "        assert cfg_imsize == clip_imsize, f\"cfg_imsize ({cfg_imsize}) must equal clip_imsize ({clip_imsize})\"\n",
    "\n",
    "        if ctx_init:\n",
    "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
    "            n_ctx = len(ctx_init.split(\" \"))\n",
    "            prompt = clip.tokenize(ctx_init)\n",
    "            with torch.no_grad():\n",
    "                embedding = clip_model.token_embedding(prompt).type(dtype)\n",
    "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
    "            prompt_prefix = ctx_init\n",
    "        else:\n",
    "            ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)\n",
    "            nn.init.normal_(ctx_vectors, std=0.02)\n",
    "            prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
    "\n",
    "        #print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        #print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        self.ctx = nn.Parameter(ctx_vectors)\n",
    "\n",
    "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding = clip_model.token_embedding(tokenized_prompts.to(clip_model.token_embedding.weight.device))\n",
    "            embedding = embedding.type(dtype)\n",
    "\n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])\n",
    "\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        self.n_cls = n_cls\n",
    "        self.n_ctx = n_ctx\n",
    "\n",
    "    def construct_prompts(self, ctx, prefix, suffix):\n",
    "        return torch.cat([prefix, ctx, suffix], dim=1)\n",
    "\n",
    "    def forward(self):\n",
    "        ctx = self.ctx.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
    "        return self.construct_prompts(ctx, self.token_prefix, self.token_suffix)\n",
    "\n",
    "\n",
    "class CustomCLIPCoOp(nn.Module):\n",
    "    def __init__(self, cfg, classnames, clip_model):\n",
    "        super().__init__()\n",
    "        self.prompt_learner = PromptLearnerCoOp(cfg, classnames, clip_model)\n",
    "        self.clip_model = clip_model\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.image_encoder = clip_model.visual\n",
    "        self.text_encoder = TextEncoder(clip_model)\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.cfg = cfg\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "    def forward(self, image, label=None):\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "\n",
    "        #print(\"Raw logit_scale:\", self.logit_scale.item())\n",
    "        #print(\"Exp logit_scale:\", logit_scale)\n",
    "\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        prompts = self.prompt_learner()\n",
    "        text_features = self.text_encoder(prompts.type(self.dtype), self.tokenized_prompts)\n",
    "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        if torch.isnan(image_features).any():\n",
    "            print(\"⚠️ NaNs in image_features!\")\n",
    "\n",
    "        if torch.isnan(text_features).any():\n",
    "            print(\"⚠️ NaNs in text_features!\")\n",
    "\n",
    "        if torch.isinf(image_features).any():\n",
    "            print(\"⚠️ Infs in image_features!\")\n",
    "\n",
    "        if torch.isinf(text_features).any():\n",
    "            print(\"⚠️ Infs in text_features!\")\n",
    "\n",
    "        #print(\"Image feature norm:\", image_features.norm(dim=-1).mean().item())\n",
    "        #print(\"Text feature norm:\", text_features.norm(dim=-1).mean().item())\n",
    "\n",
    "        logits = logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        #print(f\"is training: {self.prompt_learner.training}, label: {label}\")\n",
    "        \n",
    "        if label is not None:\n",
    "            \n",
    "            return F.cross_entropy(logits, label), logits\n",
    "\n",
    "        return logits\n",
    "    @contextmanager\n",
    "    def temporary_classnames(self, new_classnames):\n",
    "        # --- Save original state ---\n",
    "        original_classnames = self.prompt_learner.n_cls\n",
    "        original_tokenized_prompts = self.tokenized_prompts\n",
    "        original_token_prefix = self.prompt_learner.token_prefix\n",
    "        original_token_suffix = self.prompt_learner.token_suffix\n",
    "\n",
    "        # --- Apply temporary state ---\n",
    "        temp_prompt_learner = PromptLearnerCoOp(\n",
    "            cfg=self.cfg,\n",
    "            classnames=new_classnames,\n",
    "            clip_model=self.clip_model\n",
    "        )\n",
    "\n",
    "        self.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.tokenized_prompts = temp_prompt_learner.tokenized_prompts\n",
    "        self.prompt_learner.token_prefix = temp_prompt_learner.token_prefix\n",
    "        self.prompt_learner.token_suffix = temp_prompt_learner.token_suffix\n",
    "        self.prompt_learner.n_cls = len(new_classnames)\n",
    "\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            # --- Restore original state ---\n",
    "            self.tokenized_prompts = original_tokenized_prompts\n",
    "            self.prompt_learner.tokenized_prompts = original_tokenized_prompts\n",
    "            self.prompt_learner.token_prefix = original_token_prefix\n",
    "            self.prompt_learner.token_suffix = original_token_suffix\n",
    "            self.prompt_learner.n_cls = original_classnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156ee0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e29e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfa624c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/cocoop.py\n",
    "\"\"\"\n",
    "Main module for training the CoCoOp system, supporting both base and adversarial training phases.\n",
    "Includes configuration, data loading, model preparation, and training logic for zero-shot learning with CLIP.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from statistics import harmonic_mean\n",
    "from sympy.simplify.cse_main import preprocess_for_cse\n",
    "import torch\n",
    "from easydict import EasyDict\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import clip\n",
    "import numpy as np\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "# from model.cocoop.custom_clip import CustomCLIP\n",
    "# from model.cocoop.mlp_adversary import GradientReversalLayer, AdversarialMLP\n",
    "# from utils import (\n",
    "#     conditional_clustering,\n",
    "#     random_clustering,\n",
    "#     rotating_cluster_generator_shift,\n",
    "#     get_data,\n",
    "#     base_novel_categories,\n",
    "#     split_data,\n",
    "#     TensorboardLogger,\n",
    "#     CLASS_NAMES\n",
    "# )\n",
    "\n",
    "# from training_systems.training_methods import (\n",
    "#     Adversarial,\n",
    "#     KLCoCoOp,\n",
    "#     BaseCoCoOp,\n",
    "#     KLCoCoOpV2,\n",
    "# )\n",
    "# from training_systems.evaluation_methods import (\n",
    "#     ZeroShotTestStep,\n",
    "#     FineTunedTestStep,\n",
    "#     EvalStep,\n",
    "# )\n",
    "# from training_systems.core import DoubleDatasetTrainingMethod\n",
    "\n",
    "# --- Add this block for reproducibility ---\n",
    "def set_global_seed(seed):\n",
    "\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # For CUDA >= 10.2, for full determinism\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    # For PyTorch >= 1.8\n",
    "    if hasattr(torch, 'use_deterministic_algorithms'):\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "# --- End reproducibility block ---\n",
    "\n",
    "\n",
    "def checksum(model):\n",
    "    \"\"\"\n",
    "    Generate an MD5 checksum of the model's parameters to track changes across training.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to hash.\n",
    "\n",
    "    Returns:\n",
    "        str: MD5 hash string.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        all_params = torch.cat(\n",
    "            [p.view(-1).cpu() for p in model.parameters() if p.requires_grad]\n",
    "        )\n",
    "        return hashlib.md5(all_params.numpy().tobytes()).hexdigest()\n",
    "\n",
    "\n",
    "class CoCoOpSystem:\n",
    "    \"\"\"\n",
    "    Manages the full training process of the CoCoOp model, including configuration, training, evaluation,\n",
    "    checkpointing, and logging. Supports both base and adversarial training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        test_batch_size=16,\n",
    "        pseudo_base_ratio=0.7,\n",
    "        seed=42,\n",
    "        device=\"cuda\",\n",
    "        run_name=\"exp1\",\n",
    "        cnn_model=\"ViT-B/16\",\n",
    "        hparams_file,\n",
    "        optimizer_configs=None,\n",
    "        skip_tests=None,\n",
    "        train_base_checkpoint_path=None,\n",
    "        debug=False,\n",
    "        prompt_learner_opt=None,\n",
    "        kl_loss_opt=None,\n",
    "        adv_training_opt=None,\n",
    "        base_training_opt=None,\n",
    "        clustering_opt=None,\n",
    "        report=False,\n",
    "        pat=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the CoCoOp system, load data, setup the model, loss functions, optimizers, and logger.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Batch size for training.\n",
    "            device (str): Device identifier (e.g., 'cuda' or 'cpu').\n",
    "            run_name (str): Unique name for the training run.\n",
    "            cnn_model (str): Backbone CLIP model name.\n",
    "            optimizer_configs (list): Optimizer settings for base and adversarial training.\n",
    "            skip_tests (list): Booleans to skip testing after each training stage.\n",
    "            train_base_checkpoint_path (str): Optional path to a pre-trained base model.\n",
    "            debug (bool): Enables logging of additional debug information.\n",
    "            prompt_learner_opt, kl_loss_opt, adv_training_opt, base_training_opt: Configuration dictionaries.\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"run_name: {run_name}, using {cnn_model}, pat: {pat}\")\n",
    "        # --- Set global seed for reproducibility ---\n",
    "        self.seed = seed if seed is not None else 42\n",
    "        # set_global_seed(self.seed)\n",
    "        # --- End reproducibility ---\n",
    "        assert prompt_learner_opt is not None, \"prompt_learner_opt must be provided\"\n",
    "        assert kl_loss_opt is not None, \"kl_loss_opt must be provided\"\n",
    "        assert adv_training_opt is not None, \"adv_training_opt must be provided\"\n",
    "        assert base_training_opt is not None, \"base_training_opt must be provided\"\n",
    "        assert clustering_opt is not None, \"clustering_opt must be provided\"\n",
    "        assert (\n",
    "            optimizer_configs is not None and len(optimizer_configs) == 2\n",
    "        ), \"Two optimizer configs must be provided\"\n",
    "\n",
    "        # --- NEW: Pseudo-base/novel split param ---\n",
    "        self.pseudo_base_ratio = pseudo_base_ratio\n",
    "        self.pseudo_split_seed = seed\n",
    "\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.device = device\n",
    "        self.epochs = base_training_opt[\"epochs\"]\n",
    "        self.run_name = run_name\n",
    "        self.n_ctx = prompt_learner_opt[\"n_ctx\"]\n",
    "        self.ctx_init = prompt_learner_opt[\"ctx_init\"]\n",
    "        self.class_token_position = prompt_learner_opt[\"class_token_position\"]\n",
    "        self.csc = prompt_learner_opt[\"csc\"]\n",
    "        self.lambda_kl = kl_loss_opt[\"lambda_kl\"]\n",
    "        self.double_datasets_kl = kl_loss_opt.get(\"double_datasets_kl\", False)\n",
    "        self.rotation_period = kl_loss_opt.get(\"rotation_period\", \"relative\")\n",
    "        self.warmup_lambda_kl = kl_loss_opt.get(\"warmup_lambda_kl\", 0)  # Add warmup parameter\n",
    "        self.lambda_adv = adv_training_opt[\"lambda_adv\"]\n",
    "        self.gaussian_noise = adv_training_opt.get(\"gaussian_noise\", 0.0)\n",
    "        self.use_bias_ctx = adv_training_opt.get(\"use_bias_ctx\", False)\n",
    "        self.adv_training_epochs = adv_training_opt[\"adv_training_epochs\"]\n",
    "        self.cnn_model = cnn_model\n",
    "        self.warmup_epoch = base_training_opt[\"warmup_epoch\"]\n",
    "        self.warmup_cons_lr = base_training_opt[\"warmup_cons_lr\"]\n",
    "        self.using_kl = kl_loss_opt[\"using_kl\"]\n",
    "        self.grl_lambda = adv_training_opt[\"grl_lambda\"]\n",
    "        self.mlp_opt = EasyDict(adv_training_opt[\"mlp_opt\"])\n",
    "        self.skip_tests = (\n",
    "            skip_tests if skip_tests is not None else [False, False, False]\n",
    "        )\n",
    "        self.train_base_checkpoint_path = train_base_checkpoint_path\n",
    "        self.debug = debug\n",
    "        self.max_epoch = self.epochs\n",
    "        self.optimizer_configs = [EasyDict(conf) for conf in optimizer_configs]\n",
    "        self.warmup_lambda_adv = adv_training_opt[\"warmup_lambda_adv\"]\n",
    "        self.base_batch_size = base_training_opt[\"batch_size\"]\n",
    "        self.adv_batch_size = adv_training_opt[\"batch_size\"]\n",
    "        self.adv_accumulation_steps = adv_training_opt.get(\"accumulation_steps\", 1)\n",
    "        self.base_accumulation_steps = base_training_opt.get(\"accumulation_steps\", 1)\n",
    "        self.prompt_learner_warmup_epochs = adv_training_opt[\"prompt_learner_warmup_epochs\"] if \"prompt_learner_warmup_epochs\" in adv_training_opt else 0\n",
    "        self.pat = pat\n",
    "        print(\n",
    "            \"BATCH SIZES: \",\n",
    "            self.test_batch_size,\n",
    "            self.base_batch_size,\n",
    "            self.adv_batch_size,\n",
    "        )\n",
    "\n",
    "        self.ignore_no_improvement = adv_training_opt.get(\"ignore_no_improvement\", False)\n",
    "        if not report:\n",
    "            self.log_dir = f\"runs/CoCoOp/{self.run_name}\"\n",
    "        elif self.pat:\n",
    "            self.log_dir = f\"runs/report/{self.run_name}\"\n",
    "        else:\n",
    "            self.log_dir = f\"runs/report_no_pat/{self.run_name}\"\n",
    "\n",
    "        self.writer = SummaryWriter(log_dir=self.log_dir)\n",
    "        self.writer.add_text(\"Hparams yaml file\", hparams_file)\n",
    "        self.logger = TensorboardLogger(self.writer)\n",
    "\n",
    "        self.logger.log_hparams(\n",
    "            {\n",
    "                \"batch_size_test\": self.test_batch_size,\n",
    "                \"base_batch_size\": self.base_batch_size,\n",
    "                \"adv_batch_size\": self.adv_batch_size,\n",
    "                \"epochs\": self.epochs,\n",
    "                \"n_ctx\": self.n_ctx,\n",
    "                \"ctx_init\": self.ctx_init,\n",
    "                \"class_token_position\": self.class_token_position,\n",
    "                \"csc\": self.csc,\n",
    "                \"lambda_kl_first\": self.lambda_kl[0],\n",
    "                \"lambda_kl_second\": self.lambda_kl[1],\n",
    "                \"warmup_epoch\": self.warmup_epoch,\n",
    "                \"warmup_cons_lr\": self.warmup_cons_lr,\n",
    "                \"lambda_adv\": self.lambda_adv,\n",
    "                \"cnn_model\": self.cnn_model,\n",
    "                \"grl_lambda\": self.grl_lambda,\n",
    "                \"prompt_learner_warmup_epochs\" : self.prompt_learner_warmup_epochs,\n",
    "                \"double_datasets_kl\": self.double_datasets_kl,\n",
    "                \"pseudo_base_ratio\": self.pseudo_base_ratio,\n",
    "                \"pseudo_split_seed\": self.seed,\n",
    "                \"rotation_period\": self.rotation_period,\n",
    "                \"warmup_lambda_kl\": self.warmup_lambda_kl,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        \n",
    "        print(f\"patience {'disabled' if not self.pat else 'enabled'}\")\n",
    "        # Load model\n",
    "        self.clip_model, preprocess= clip.load(self.cnn_model)\n",
    "        self.clip_model = self.clip_model.to(self.device)\n",
    "        resolution = self.clip_model.visual.input_resolution\n",
    "        self.train_set, self.val_set, self.test_set = get_data(resolution=resolution, eval_transform=preprocess)\n",
    "        self.base_classes, self.novel_classes = base_novel_categories(self.train_set)\n",
    "        # --- NEW: Pseudo-base/novel split ---\n",
    "\n",
    "        # Helper to split a dataset by class list\n",
    "        # (moved to method below)\n",
    "\n",
    "        # Split train_base/val_base into pseudo_base/pseudo_novel\n",
    "        self.train_base, _ = split_data(self.train_set, self.base_classes)\n",
    "        self.val_base, self.val_novel = split_data(self.val_set, self.base_classes)\n",
    "        self.test_base, self.test_novel = split_data(self.test_set, self.base_classes)\n",
    "        print(f\"Base classes length: {len(self.base_classes)}, Novel classes length: {len(self.novel_classes)}\")\n",
    "        print(f\"Train base length: {len(self.train_base)}, Val base length: {len(self.val_base)}, Test base length: {len(self.test_base)}\")\n",
    "        print(f\"Val novel length: {len(self.val_novel)}, Test novel length: {len(self.test_novel)}\")\n",
    "\n",
    "\n",
    "        self.rotation_steps = int(len(self.base_classes)*(1-self.pseudo_base_ratio))\n",
    "\n",
    "        self.cluster_generator = rotating_cluster_generator_shift(\n",
    "            self.base_classes, \n",
    "            self.pseudo_base_ratio, \n",
    "            steps=self.rotation_steps, \n",
    "            seed=self.seed\n",
    "        )\n",
    "        \n",
    "        _, self.pseudo_base_classes, self.pseudo_novel_classes = next(self.cluster_generator)\n",
    "\n",
    "        self.train_pseudo_base = self.split_by_classes(self.train_base, self.pseudo_base_classes)\n",
    "        self.train_pseudo_novel = self.split_by_classes(self.train_base, self.pseudo_novel_classes)\n",
    "        self.val_pseudo_base = self.split_by_classes(self.val_base, self.pseudo_base_classes)\n",
    "        self.val_pseudo_novel = self.split_by_classes(self.val_base, self.pseudo_novel_classes)\n",
    "\n",
    "        # --- Model/classnames: only pseudo_base for first phase ---\n",
    "        ctx_load = (\n",
    "            \"./bin/coop/coop_ctx_4_VIT16.pth\"\n",
    "            if self.n_ctx == 4\n",
    "            else \"./bin/coop/coop_ctx_8_VIT16.pth\"\n",
    "        )\n",
    "        cfg = EasyDict(\n",
    "            {\n",
    "                \"TRAINER\": {\n",
    "                    \"COCOOP\": {\n",
    "                        \"CTX_LOAD\": ctx_load,\n",
    "                        \"N_CTX\": self.n_ctx,\n",
    "                        \"CTX_INIT\": self.ctx_init,\n",
    "                        \"PREC\": \"fp16\",\n",
    "                    }\n",
    "                },\n",
    "                \"INPUT\": {\"SIZE\": [resolution, resolution]},\n",
    "            }\n",
    "        )\n",
    "        self.model = CustomCLIP(\n",
    "\n",
    "            classnames=[CLASS_NAMES[idx] for idx in self.pseudo_base_classes],\n",
    "            cfg=cfg,\n",
    "            clip_model=self.clip_model,\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Print all dtypes of every component inside CustomCLIP\n",
    "        # self.model.print_all_dtypes()\n",
    "\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"prompt_learner\" not in name:\n",
    "                param.requires_grad_(False)\n",
    "            else:\n",
    "                param.requires_grad_(True)\n",
    "\n",
    "        self.cost_function = nn.CrossEntropyLoss()\n",
    "        self.grl = GradientReversalLayer(lambda_=self.grl_lambda)\n",
    "\n",
    "        clip_dim = self.clip_model.visual.output_dim\n",
    "\n",
    "        self.mlp_adversary = AdversarialMLP(\n",
    "            input_dim=clip_dim+len(self.base_classes), opt=self.mlp_opt, output_dim=clustering_opt[\"n_clusters\"], use_bias_ctx=self.use_bias_ctx, n_ctx=self.n_ctx\n",
    "        ).to(self.device)\n",
    "\n",
    "        print(\"mlp adversary struct: \", self.mlp_adversary)\n",
    "        self.optimizer = self.get_optimizer(self.model, None, self.optimizer_configs[0])\n",
    "        self.lr_scheduler = LambdaLR(self.optimizer, self._lr_lambda)\n",
    "\n",
    "        # --- NEW: Cluster dict for adversarial phase ---\n",
    "\n",
    "        clustering_type = clustering_opt[\"clustering_type\"]\n",
    "\n",
    "        if clustering_type == \"random\": \n",
    "            # Use random clustering\n",
    "            self.cls_cluster_dict, _ = random_clustering(\n",
    "                n_cluster=clustering_opt[\"n_clusters\"],\n",
    "                seed=self.seed,\n",
    "                distribution=\"uniform\",\n",
    "            )\n",
    "        elif clustering_type == \"semantic\": \n",
    "            # Load clustering information\n",
    "            self.cls_cluster_dict, _ = conditional_clustering(\n",
    "                n_cluster=clustering_opt[\"n_clusters\"],\n",
    "                variance=clustering_opt[\"variance\"],\n",
    "                cnn=clustering_opt[\"vision_encoder\"],\n",
    "                device=self.device,\n",
    "            )\n",
    "        elif clustering_type == \"default\":\n",
    "            # Pseudo_base: cluster 0, pseudo_novel: cluster 1\n",
    "            self.pseudo_cls_cluster_dict = {c: 0 for c in self.pseudo_base_classes}\n",
    "            self.pseudo_cls_cluster_dict.update({c: 1 for c in self.pseudo_novel_classes})\n",
    "            # For adversarial phase, use this dict\n",
    "            self.cls_cluster_dict = self.pseudo_cls_cluster_dict\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown clustering type: {clustering_type}\")\n",
    "\n",
    "    def _set_test_methods(self):\n",
    "        \"\"\"\n",
    "        Initializes the zero-shot and fine-tuned test step evaluators for both base and novel class splits.\n",
    "        \"\"\"\n",
    "        self.zero_shot_base_classes_test_method = ZeroShotTestStep(\n",
    "            model=self.clip_model,\n",
    "            batch_size=self.test_batch_size,\n",
    "            categories=self.base_classes,\n",
    "        )\n",
    "        self.zero_shot_novel_classes_test_method = ZeroShotTestStep(\n",
    "            model=self.clip_model,\n",
    "            batch_size=self.test_batch_size,\n",
    "            categories=self.novel_classes,\n",
    "        )\n",
    "        self.zero_shot_pseudo_base_test_method = ZeroShotTestStep(\n",
    "            model=self.clip_model,\n",
    "            batch_size=self.test_batch_size,\n",
    "            categories=self.pseudo_base_classes,\n",
    "        )\n",
    "        self.zero_shot_pseudo_novel_test_method = ZeroShotTestStep(\n",
    "            model=self.clip_model,\n",
    "            batch_size=self.test_batch_size,\n",
    "            categories=self.pseudo_novel_classes,\n",
    "        )\n",
    "        self.finetuned_test_method = FineTunedTestStep(\n",
    "            model=self.model,\n",
    "            batch_size=self.test_batch_size,\n",
    "        )\n",
    "\n",
    "    def _set_eval_method(self):\n",
    "        \"\"\"\n",
    "        Initializes the evaluation method used during validation.\n",
    "        \"\"\"\n",
    "        self.eval_method = EvalStep(\n",
    "            model=self.model,\n",
    "            batch_size=self.test_batch_size,\n",
    "        )\n",
    "\n",
    "    def _set_train_methods(self):\n",
    "        \"\"\"\n",
    "        Initializes the training method used for both base and adversarial phases, depending on whether KL loss is enabled.\n",
    "        Chooses between standard and KL-regularized training methods.\n",
    "        \"\"\"\n",
    "        self.adversarial_method = Adversarial(\n",
    "                lambda_adv=0.05,\n",
    "                model=self.model,\n",
    "                optimizer=self.optimizer,\n",
    "                cls_cluster_dict=self.cls_cluster_dict,\n",
    "                grl=self.grl,\n",
    "                mlp_adversary=self.mlp_adversary,\n",
    "                debug=self.debug,\n",
    "                tmp_classes=self.base_classes, \n",
    "                gaussian_noise=self.gaussian_noise,\n",
    "                use_bias_ctx=self.use_bias_ctx\n",
    "            )\n",
    "            \n",
    "        if self.using_kl[0]:\n",
    "            if self.double_datasets_kl:\n",
    "                self.basic_train_method = KLCoCoOpV2(\n",
    "                    model=self.model,\n",
    "                    optimizer=self.optimizer,\n",
    "                    debug=self.debug,\n",
    "                    lambda_kl=self.lambda_kl[0],\n",
    "                )\n",
    "            else:\n",
    "                self.basic_train_method = KLCoCoOp(\n",
    "                    model=self.model,\n",
    "                    optimizer=self.optimizer,\n",
    "                    debug=self.debug,\n",
    "                    lambda_kl=self.lambda_kl[0],\n",
    "                )\n",
    "        else:\n",
    "            self.basic_train_method = BaseCoCoOp(\n",
    "                model=self.model,\n",
    "                optimizer=self.optimizer,\n",
    "                debug=self.debug,\n",
    "            )\n",
    "\n",
    "        \n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Execute the full training pipeline: base phase, optionally followed by adversarial training and evaluation.\n",
    "        \"\"\"\n",
    "        self._set_test_methods()\n",
    "        self._set_eval_method()\n",
    "        self._set_train_methods()\n",
    "        if not self.skip_tests[0]:\n",
    "            print(\"Doing base accuracy test\")\n",
    "            base_acc, novel_acc = self.compute_evaluation(-1, base=True)\n",
    "            self._log_final_metrics(\n",
    "                \"Final metrics - CLIP ZERO SHOT\",\n",
    "                base_acc,\n",
    "                novel_acc,\n",
    "                -1,\n",
    "            )\n",
    "\n",
    "        best_model_path = os.path.join(self.log_dir, \"best_model.pth\")\n",
    "        # Ensure all methods are properly initialized prior to the base training phase\n",
    "\n",
    "        if self.train_base_checkpoint_path is None:\n",
    "            # Base training phase\n",
    "            base_end_epoch, _, best_base_epoch = self._train_base_phase(best_model_path)\n",
    "            self.best_base_epoch = best_base_epoch  # Store for later use if needed\n",
    "            # Log the best epoch to TensorBoard and logger\n",
    "            self.writer.add_scalar(\"Best Base Epoch\", best_base_epoch, base_end_epoch)\n",
    "\n",
    "            # Also print for visibility\n",
    "            print(f\"Best base model found at epoch: {best_base_epoch}\")\n",
    "            if self.epochs != 0:\n",
    "                if self.pat:\n",
    "                    print(f\"[DEBUG] Loading best model state dict after base phase from: {best_model_path}\")\n",
    "                    self.model.load_state_dict(torch.load(best_model_path))\n",
    "                    print(f\"[DEBUG] Loaded best model with classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "                else:\n",
    "                    print(f\"[DEBUG] Using last model state from base phase (patience disabled)\")\n",
    "                    print(f\"[DEBUG] Model has classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "                self.save_model(path=\"./bin/cocoop\", prefix=\"after_first_train_\")\n",
    "\n",
    "            if not self.skip_tests[1]:\n",
    "                print(\"Doing base accuracy test\")\n",
    "                base_acc, novel_acc = self.compute_evaluation(base_end_epoch)\n",
    "                self._log_final_metrics(\n",
    "                    \"Final metrics - After Base Training\",\n",
    "                    base_acc,\n",
    "                    novel_acc,\n",
    "                    base_end_epoch,\n",
    "                )\n",
    "        else:\n",
    "            base_end_epoch = 0\n",
    "            print(\"Skipping base training\")\n",
    "            print(f\"[DEBUG] Loading model state dict from: {self.train_base_checkpoint_path}\")\n",
    "            with self.model.temporary_classnames([CLASS_NAMES[c] for c in self.base_classes]):\n",
    "                self.model.load_state_dict(torch.load(self.train_base_checkpoint_path))\n",
    "            print(f\"[DEBUG] Loaded model with classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "\n",
    "        # Re-initialize test/eval/train methods after loading/training model and before adversarial phase\n",
    "        self._set_eval_method()\n",
    "        self._set_test_methods()\n",
    "\n",
    "        self.optimizer = self.get_optimizer(\n",
    "            self.model, self.mlp_adversary, self.optimizer_configs[1]\n",
    "        )\n",
    "        # After changing optimizer, ensure train methods use the new optimizer\n",
    "        self._set_train_methods()\n",
    "\n",
    "        checksum1 = None\n",
    "        if self.debug:\n",
    "            checksum1 = checksum(self.model)\n",
    "            # Adversarial phase\n",
    "            print(\"Before adv training:\", checksum1)\n",
    "\n",
    "        adv_end_epoch = self._train_adversarial_phase(base_end_epoch, best_model_path)\n",
    "\n",
    "        if self.debug and checksum1:\n",
    "            checksum2 = checksum(self.model)\n",
    "            print(\"After adv training:\", checksum2)\n",
    "            print(f\"checksum1: {checksum1}, checksum2: {checksum2}\")\n",
    "            if checksum1 != checksum2:\n",
    "                print(\"Model parameters have changed after adversarial training.\")\n",
    "\n",
    "        if not self.skip_tests[2]:\n",
    "            print(\"Doing post-adv. accuracy test\")\n",
    "            base_acc, novel_acc = self.compute_evaluation(adv_end_epoch)\n",
    "            self._log_final_metrics(\n",
    "                \"Final metrics - After Adversarial Training\",\n",
    "                base_acc,\n",
    "                novel_acc,\n",
    "                adv_end_epoch,\n",
    "            )\n",
    "\n",
    "        self.logger.close()\n",
    "        if self.adv_training_epochs != 0:\n",
    "            self.save_model(path=\"./bin/cocoop\", prefix=\"after_adv_train_\")\n",
    "            self.save_mlp_adversary()\n",
    "\n",
    "    def get_next_rotation(self):\n",
    "        \"\"\"\n",
    "        Get the next rotation of the pseudo base and pseudo novel classes.\n",
    "        \"\"\"\n",
    "        _, self.pseudo_base_classes, self.pseudo_novel_classes = next(self.cluster_generator)\n",
    "        self.train_pseudo_base = self.split_by_classes(self.train_base, self.pseudo_base_classes)\n",
    "        self.train_pseudo_novel = self.split_by_classes(self.train_base, self.pseudo_novel_classes)\n",
    "        self.val_pseudo_base = self.split_by_classes(self.val_base, self.pseudo_base_classes)\n",
    "        self.val_pseudo_novel = self.split_by_classes(self.val_base, self.pseudo_novel_classes)\n",
    "        return self.pseudo_base_classes, self.pseudo_novel_classes, self.train_pseudo_base, self.train_pseudo_novel, self.val_pseudo_base, self.val_pseudo_novel\n",
    "\n",
    "    def _train_base_phase(self, best_model_path):\n",
    "        \"\"\"\n",
    "        Train the model using KL regularization only (no adversarial objective).\n",
    "\n",
    "        Args:\n",
    "            best_model_path (str): Path to store the best base model.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, float, int]: Final epoch index, best validation score, and best epoch index.\n",
    "        \"\"\"\n",
    "        best_score = 0.0\n",
    "        patience = 8\n",
    "        patience_counter = 0\n",
    "        c = 0\n",
    "        method = self.basic_train_method\n",
    "        evaluation_period = 1 if self.pat else 2\n",
    "        \n",
    "\n",
    "\n",
    "        # Initialize rotation tracking\n",
    "        if self.rotation_period == \"random\":\n",
    "            # For random rotation, we'll track when the next rotation should happen\n",
    "            next_rotation_epoch = random.randint(1, 4)  # Sample from 1 to 4\n",
    "            rotation_epochs = None  # Not used for random\n",
    "        else:\n",
    "            rotation_epochs = int(patience * (3/4)) if self.rotation_period == \"relative\" else self.rotation_period\n",
    "            next_rotation_epoch = None  # Not used for fixed/relative\n",
    "        \n",
    "        pbar = tqdm(total=self.max_epoch, desc=\"Base Training\")\n",
    "        best_epoch = -1  # Track the epoch of the best model\n",
    "        best_epoch_path = best_model_path + \".best_epoch.txt\"  # Path to store best epoch\n",
    "\n",
    "        for e in range(self.max_epoch):\n",
    "\n",
    "            # Apply lambda_kl warmup if enabled\n",
    "            if self.warmup_lambda_kl > 0 and (isinstance(method, KLCoCoOp) or isinstance(method, KLCoCoOpV2)):\n",
    "                progress = (e + 1) / self.warmup_lambda_kl\n",
    "                current_lambda_kl = 0.1 + (self.lambda_kl[0] - 0.1) * min(progress, 1)\n",
    "                # Only update lambda_kl for methods that support it (KLCoCoOp and KLCoCoOpV2)\n",
    "                method.update_lambda_kl(current_lambda_kl)\n",
    "\n",
    "            if self.using_kl[0]:\n",
    "                if isinstance(method, DoubleDatasetTrainingMethod):\n",
    "                    # Check if rotation should happen\n",
    "                    should_rotate = False\n",
    "                    if self.rotation_period == \"random\":\n",
    "                        if e == next_rotation_epoch:\n",
    "                            should_rotate = True\n",
    "                            # Sample next rotation epoch (1 to 4 epochs from now)\n",
    "                            next_rotation_epoch = e + random.randint(1, 4)\n",
    "                    else:\n",
    "                        # Fixed or relative rotation\n",
    "                        if rotation_epochs is not None and e % rotation_epochs == 0:\n",
    "                            should_rotate = True\n",
    "                    \n",
    "                    if should_rotate:\n",
    "                        if self.rotation_period == \"random\":\n",
    "                            print(f\"[DEBUG] Random rotation at epoch {e}, next rotation at epoch {next_rotation_epoch}\")\n",
    "                        (\n",
    "                            self.pseudo_base_classes, \n",
    "                            self.pseudo_novel_classes, \n",
    "                            self.train_pseudo_base, \n",
    "                            self.train_pseudo_novel, \n",
    "                            self.val_pseudo_base, \n",
    "                            self.val_pseudo_novel\n",
    "                        ) = self.get_next_rotation()\n",
    "                    kl_loss, ce_loss, acc = method.double_datasets_train_step(\n",
    "                        self.train_pseudo_base,\n",
    "                        self.train_pseudo_novel,\n",
    "                        self.base_batch_size,\n",
    "                        [\"pseudo_base\", \"pseudo_novel KL\"],\n",
    "                        self.pseudo_base_classes,\n",
    "                        self.pseudo_novel_classes,\n",
    "                    )\n",
    "                    total_loss = ce_loss + kl_loss\n",
    "                else:\n",
    "                    total_loss, acc, ce_loss, kl_loss = method.train_step(\n",
    "                        self.train_pseudo_base,\n",
    "                        self.base_batch_size,\n",
    "                        classnames=self.pseudo_base_classes,\n",
    "                        accumulation_steps=self.base_accumulation_steps\n",
    "                    )\n",
    "            elif isinstance(method, BaseCoCoOp):\n",
    "                total_loss, acc = method.train_step(\n",
    "                    self.train_pseudo_base,\n",
    "                    self.base_batch_size,\n",
    "                    classnames=self.pseudo_base_classes,\n",
    "                    accumulation_steps=self.base_accumulation_steps\n",
    "                )\n",
    "                kl_loss = None\n",
    "                ce_loss = total_loss\n",
    "\n",
    "            self.logger.log_training_base(\n",
    "                e,\n",
    "                self.optimizer.param_groups[0][\"lr\"],\n",
    "                ce_loss,\n",
    "                acc,\n",
    "                kl_loss,\n",
    "                total_loss,\n",
    "            )\n",
    "            postfix_dict = {\n",
    "                'lr': self.optimizer.param_groups[0][\"lr\"],\n",
    "                'ce_L': ce_loss,\n",
    "                'kl_L': kl_loss,\n",
    "                'pat_c': patience_counter,\n",
    "            }\n",
    "            if e % evaluation_period == 0:\n",
    "                base_val_acc, novel_val_acc = self._evaluate_and_log(e)\n",
    "\n",
    "                score = novel_val_acc\n",
    "\n",
    "                if (score > best_score) or (not self.pat):\n",
    "                    best_score = score\n",
    "                    patience_counter = 0\n",
    "                    torch.save(self.model.state_dict(), best_model_path)\n",
    "                    best_epoch = e  # Save the epoch of the best model\n",
    "                    # Store the best epoch to disk\n",
    "                    with open(best_epoch_path, \"w\") as f:\n",
    "                        f.write(str(best_epoch))\n",
    "                    # When pat=False, we always save the current model state (last epoch)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {e}\")\n",
    "                        break\n",
    "\n",
    "                postfix_dict[\"B_val_acc\"] = base_val_acc\n",
    "                postfix_dict[\"N_val_acc\"] = novel_val_acc\n",
    "                postfix_dict[\"score\"] = score\n",
    "            \n",
    "            self.lr_scheduler.step()\n",
    "            pbar.set_postfix(**postfix_dict)\n",
    "            pbar.update(1)\n",
    "            c += 1\n",
    "\n",
    "        return c, best_score, best_epoch\n",
    "\n",
    "    def _train_adversarial_phase(self, start_epoch, best_model_path):\n",
    "        \"\"\"\n",
    "        Train the model adversarially with dynamic lambda scheduling and early stopping.\n",
    "\n",
    "        Args:\n",
    "            start_epoch (int): Starting epoch index.\n",
    "            best_model_path (str): Path to save the best adversarial model.\n",
    "\n",
    "        Returns:\n",
    "            int: Final epoch index after training.\n",
    "        \"\"\"\n",
    "        best_novel_accuracy = 0.0\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "        at_least_one_improving = False\n",
    "        warmup_epochs = self.warmup_lambda_adv\n",
    "        lambda_adv_max = self.lambda_adv\n",
    "        initial_lambda_adv = 0.05\n",
    "        pbar = tqdm(total=self.adv_training_epochs, desc=\"Adversarial Training\")\n",
    "\n",
    "        last_model_state = None  # store last model state\n",
    "\n",
    "        method = self.adversarial_method\n",
    "\n",
    "        for e in range(start_epoch, start_epoch + self.adv_training_epochs):\n",
    "            progress = (e - start_epoch + 1) / warmup_epochs\n",
    "            new_lambda_adv = initial_lambda_adv + (\n",
    "                lambda_adv_max - initial_lambda_adv\n",
    "            ) * min(progress, 1)\n",
    "\n",
    "            method.update_lambda_adv(new_lambda_adv)\n",
    "\n",
    "            if (e-start_epoch) < self.prompt_learner_warmup_epochs:\n",
    "                print(f\"[DEBUG] Prompt learner FROZEN at adv epoch {e-start_epoch}\")\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if \"prompt_learner\" in name:\n",
    "                        param.requires_grad_(False)\n",
    "            elif (e-start_epoch) == self.prompt_learner_warmup_epochs:\n",
    "                print(f\"prompt learner UNFROZEN at adv epoch {e-start_epoch}\")\n",
    "                for name, param in self.model.named_parameters():\n",
    "                    if \"prompt_learner\" in name:\n",
    "                        param.requires_grad_(True)\n",
    "                # print the frozen parameters name\n",
    "\n",
    "\n",
    "            if self.using_kl[1]:\n",
    "                total_loss, acc, ce_loss, kl_loss, adv_loss = method.train_step(\n",
    "                    self.train_base,\n",
    "                    self.base_batch_size,\n",
    "                    classnames=self.base_classes,\n",
    "                    accumulation_steps=self.base_accumulation_steps\n",
    "                )\n",
    "            else:\n",
    "                total_loss, acc, ce_loss, adv_loss = method.train_step(\n",
    "                    self.train_base,\n",
    "                    self.adv_batch_size,\n",
    "                    classnames=self.base_classes,\n",
    "                    accumulation_steps=self.adv_accumulation_steps\n",
    "                )\n",
    "                kl_loss = None\n",
    "\n",
    "            self.logger.log_training_adv(\n",
    "                e,\n",
    "                method.lambda_adv,\n",
    "                ce_loss,\n",
    "                acc,\n",
    "                adv_loss,\n",
    "                ce_loss + adv_loss + (kl_loss if kl_loss else 0.0),\n",
    "                kl_loss=kl_loss,\n",
    "            )\n",
    "            if (e-start_epoch) >= self.prompt_learner_warmup_epochs:\n",
    "\n",
    "                # Always update last_model_state to track the current state\n",
    "                last_model_state = deepcopy(self.model.state_dict())\n",
    "\n",
    "                base_val_acc, novel_val_acc = self._evaluate_and_log(\n",
    "                    e,\n",
    "                    is_adv=True,\n",
    "                )\n",
    "                if novel_val_acc > best_novel_accuracy or self.ignore_no_improvement or (not self.pat):\n",
    "                    best_novel_accuracy = novel_val_acc\n",
    "                    print(f\"[DEBUG] Saving model with classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "                    torch.save(self.model.state_dict(), best_model_path)\n",
    "                    at_least_one_improving = True\n",
    "                    patience_counter = 0\n",
    "                    # When pat=False, we always save the current model state (last epoch)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience:\n",
    "                        print(f\"Early stopping adversarial at epoch {e}\")\n",
    "                        break\n",
    "                pbar.set_postfix(\n",
    "                    PB_val_acc=base_val_acc,\n",
    "                    PN_val_acc=novel_val_acc,\n",
    "                    ce_L=ce_loss,\n",
    "                    kl_L=kl_loss,\n",
    "                    adv_L=adv_loss,\n",
    "                    lr=self.optimizer.param_groups[0][\"lr\"],\n",
    "                    pat_c=patience_counter,\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                pbar.set_postfix(\n",
    "                    adv_loss=adv_loss,\n",
    "                )\n",
    "            pbar.update(1)\n",
    "\n",
    "        # When pat=False, we want to keep the last model state (no loading from checkpoint)\n",
    "        # When pat=True, we load the best model if there was improvement\n",
    "        if self.pat and ((at_least_one_improving and self.epochs != 0) or self.ignore_no_improvement):\n",
    "            print(f\"[DEBUG] Loading best model state dict after adversarial phase from: {best_model_path}\")\n",
    "            self.model.load_state_dict(torch.load(best_model_path))\n",
    "            print(f\"[DEBUG] Loaded model with classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "            print(\"Loaded best model from adversarial checkpoint (robust, filtered mismatched keys).\")\n",
    "        else:\n",
    "            print(\n",
    "                \"Using model from last adversarial epoch (patience disabled or no improvement).\"\n",
    "            )\n",
    "            if last_model_state is not None:\n",
    "                self.model.load_state_dict(last_model_state)\n",
    "                print(\"Loaded last adversarial model state.\")\n",
    "            else:\n",
    "                # If last_model_state is None (e.g., still in warmup), the model already has the last state\n",
    "                # But to be safe, we can load from the saved checkpoint which should be the last state when pat=False\n",
    "                if not self.pat:\n",
    "                    print(f\"[DEBUG] Loading last model state from checkpoint (patience disabled)\")\n",
    "                    self.model.load_state_dict(torch.load(best_model_path))\n",
    "                    print(\"Loaded last adversarial model state from checkpoint.\")\n",
    "                else:\n",
    "                    print(\"Model already has the last adversarial state (no loading needed).\")\n",
    "\n",
    "        return start_epoch + self.adv_training_epochs\n",
    "\n",
    "    def _evaluate_and_log(self, epoch, is_adv=False):\n",
    "        \"\"\"\n",
    "        Run validation and log results for both base and novel splits.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Current training epoch.\n",
    "            is_adv (bool): Whether evaluation is during adversarial training.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: Accuracy for base and novel classes.\n",
    "        \"\"\"\n",
    "\n",
    "        if is_adv:\n",
    "            metrics_base = self.eval_method.evaluate(\n",
    "                dataset=self.val_base,\n",
    "                classnames=self.base_classes,\n",
    "                desc_add=\" - Base\",\n",
    "            )\n",
    "            base_val_loss = metrics_base[\"loss\"]\n",
    "            base_val_acc = metrics_base[\"accuracy\"]\n",
    "\n",
    "            metrics_novel = self.eval_method.evaluate(\n",
    "                dataset=self.val_novel,\n",
    "                classnames=self.novel_classes,\n",
    "                desc_add=\" - Novel\",\n",
    "            )\n",
    "            novel_val_loss = metrics_novel[\"loss\"]\n",
    "            novel_val_acc = metrics_novel[\"accuracy\"]   \n",
    "            \n",
    "        else:\n",
    "\n",
    "            metrics_base = self.eval_method.evaluate(\n",
    "                dataset=self.val_base,\n",
    "                classnames=self.base_classes,\n",
    "                desc_add=\" - Base\",\n",
    "            )\n",
    "            base_val_loss = metrics_base[\"loss\"]\n",
    "            base_val_acc = metrics_base[\"accuracy\"]\n",
    "\n",
    "            metrics_novel = self.eval_method.evaluate(\n",
    "                dataset=self.val_novel,\n",
    "                classnames=self.novel_classes,\n",
    "                desc_add=\" - Novel\",\n",
    "            )\n",
    "            novel_val_loss = metrics_novel[\"loss\"]\n",
    "            novel_val_acc = metrics_novel[\"accuracy\"]  \n",
    "\n",
    "        self.logger.log_validation(\n",
    "            epoch,\n",
    "            base_val_loss,\n",
    "            base_val_acc,\n",
    "            novel_val_loss,\n",
    "            novel_val_acc,\n",
    "            is_adv=is_adv,\n",
    "        )\n",
    "\n",
    "        return base_val_acc, novel_val_acc\n",
    "\n",
    "    def _log_final_metrics(self, tag, base_acc, novel_acc, step):\n",
    "        \"\"\"\n",
    "        Log final test results to TensorBoard.\n",
    "\n",
    "        Args:\n",
    "            tag (str): Descriptive tag for the log.\n",
    "            base_acc (float): Accuracy on base classes.\n",
    "            novel_acc (float): Accuracy on novel classes.\n",
    "            step (int): Epoch or step index for this log.\n",
    "        \"\"\"\n",
    "        self.logger.log_final_metrics(tag, base_acc, novel_acc, step)\n",
    "\n",
    "    def _lr_lambda(self, current_epoch):\n",
    "        \"\"\"\n",
    "        Learning rate scheduler with cosine annealing and warm-up.\n",
    "\n",
    "        Args:\n",
    "            current_epoch (int): Epoch index.\n",
    "\n",
    "        Returns:\n",
    "            float: Learning rate multiplier.\n",
    "        \"\"\"\n",
    "        if current_epoch < self.warmup_epoch:\n",
    "            return self.warmup_cons_lr / self.optimizer_configs[0].prompt_lr # type: ignore\n",
    "        return 0.5 * (\n",
    "            1\n",
    "            + math.cos(\n",
    "                math.pi\n",
    "                * (current_epoch - self.warmup_epoch)\n",
    "                / (self.max_epoch - self.warmup_epoch + 1e-7)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def compute_evaluation(self, epoch_idx, base=False):\n",
    "        \"\"\"\n",
    "        Run evaluation on the test split for both base and novel classes.\n",
    "\n",
    "        Args:\n",
    "            epoch_idx (int): Epoch index for logging.\n",
    "            base (bool): Whether to evaluate the frozen base CLIP model.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float]: Base and novel class test accuracy.\n",
    "\n",
    "        model = self.model if not base else self.clip_model\n",
    "        base_accuracy = test_step(model, self.test_base, self.base_classes, self.batch_size, self.device, label=\"test\", base=base)\n",
    "        novel_accuracy = test_step(model, self.test_novel, self.novel_classes, self.batch_size, self.device, label=\"test\", base=base)\n",
    "        \"\"\"\n",
    "        if base:\n",
    "            base_metrics = self.zero_shot_base_classes_test_method.evaluate(\n",
    "                dataset=self.test_base,\n",
    "                desc_add=\" - Base Zero Shot\",\n",
    "        \n",
    "            )\n",
    "            novel_metrics = self.zero_shot_novel_classes_test_method.evaluate(\n",
    "                dataset=self.test_novel,\n",
    "                desc_add=\" - Novel Zero Shot\",\n",
    "            )\n",
    "        else:\n",
    "            base_metrics = self.finetuned_test_method.evaluate(\n",
    "                dataset=self.test_base,\n",
    "                classnames=self.base_classes,\n",
    "                desc_add=\" - Base Fine Tuned\",\n",
    "            )\n",
    "            novel_metrics = self.finetuned_test_method.evaluate(\n",
    "                dataset=self.test_novel,\n",
    "                classnames=self.novel_classes,\n",
    "                desc_add=\" - Novel Fine Tuned\",\n",
    "            )\n",
    "\n",
    "        base_accuracy = base_metrics[\"accuracy\"]\n",
    "        novel_accuracy = novel_metrics[\"accuracy\"]\n",
    "        self.logger.log_test_accuracy(epoch_idx, base_accuracy, \"base_classes\")\n",
    "        self.logger.log_test_accuracy(epoch_idx, novel_accuracy, \"novel_classes\")\n",
    "        return base_accuracy, novel_accuracy\n",
    "\n",
    "    def save_model(self, path=\"./bin/cocoop\", prefix=\"\"):\n",
    "        \"\"\"\n",
    "        Save model weights to disk.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory to save the model to.\n",
    "            prefix (str): Filename prefix to distinguish models.\n",
    "        \"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(f\"[DEBUG] Saving model with classnames: {self.model.prompt_learner.n_cls} classes\")\n",
    "        with self.model.temporary_classnames([CLASS_NAMES[idx] for idx in self.base_classes]):\n",
    "            torch.save(\n",
    "                self.model.state_dict(), os.path.join(path, f\"{prefix}{self.run_name}.pth\")\n",
    "            )\n",
    "\n",
    "    def save_mlp_adversary(self, path=\"./bin/cocoop\", prefix=\"\"):\n",
    "        \"\"\"\n",
    "        Save the MLP adversary weights to disk.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory to save the MLP adversary model to.\n",
    "            prefix (str): Filename prefix to distinguish models.\n",
    "        \"\"\"\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        torch.save(\n",
    "            self.mlp_adversary.state_dict(),\n",
    "            os.path.join(path, f\"{prefix}{self.run_name}_mlp_adversary.pth\"),\n",
    "        )\n",
    "\n",
    "    def get_optimizer(self, model, mlp_adversary, config):\n",
    "        \"\"\"\n",
    "        Build an SGD optimizer with separate learning rates for different parameter groups.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): Main model.\n",
    "            mlp_adversary (torch.nn.Module): Optional adversarial MLP.\n",
    "            config: Optimizer configuration namespace.\n",
    "\n",
    "        Returns:\n",
    "            torch.optim.Optimizer: Configured optimizer.\n",
    "        \"\"\"\n",
    "        params = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p\n",
    "                    for n, p in model.named_parameters()\n",
    "                    if \"prompt_learner\" in n and p.requires_grad\n",
    "                ],\n",
    "                \"lr\": config.prompt_lr,\n",
    "            }\n",
    "        ]\n",
    "        if mlp_adversary is not None:\n",
    "            params.append(\n",
    "                {\n",
    "                    \"params\": mlp_adversary.parameters(),\n",
    "                    \"lr\": config.mlp_lr,\n",
    "                }\n",
    "            )\n",
    "        return torch.optim.SGD(\n",
    "            params, weight_decay=config.weight_decay, momentum=config.momentum\n",
    "        )\n",
    "\n",
    "    def split_by_classes(self, dataset, class_list):\n",
    "        idxs = [i for i, (_, label) in enumerate(dataset) if label in class_list]\n",
    "        return torch.utils.data.Subset(dataset, idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aac3b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/coop.py\n",
    "\"\"\"\n",
    "This module defines the CoOpSystem class for training and evaluating a prompt-tuned CLIP model using the CoOp method.\n",
    "It includes data loading, training with early stopping, evaluation, model saving/loading, and logging to TensorBoard.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from easydict import EasyDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from model.coop.custom_clip import CustomCLIPCoOp\n",
    "from utils.datasets import get_data, base_novel_categories, split_data, CLASS_NAMES\n",
    "from utils.training_coop import test_step, training_step, eval_step\n",
    "import clip\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from torch.optim import SGD, Adam\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CoOpSystem:\n",
    "    \"\"\"\n",
    "    Implements the CoOp prompt tuning system for training and evaluating CLIP-based models.\n",
    "\n",
    "    Attributes:\n",
    "        batch_size (int): Number of samples per training batch.\n",
    "        device (str): Device identifier (e.g., \"cuda:0\") to run training on.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        weight_decay (float): Weight decay used in the optimizer.\n",
    "        momentum (float): Momentum term (if applicable).\n",
    "        epochs (int): Number of training epochs.\n",
    "        run_name (str): Identifier for the experiment run (used for logging and file naming).\n",
    "        n_ctx (int): Number of context tokens for prompt tuning.\n",
    "        ctx_init (str): Initialization string for context tokens.\n",
    "        class_token_position (str): Position of the class token in the prompt.\n",
    "        csc (bool): Whether to use class-specific context.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 batch_size=16,\n",
    "                 device=\"cuda:0\",\n",
    "                 learning_rate=0.002,\n",
    "                 weight_decay=0.0005,\n",
    "                 momentum=0.9,\n",
    "                 epochs=2,\n",
    "                 run_name=\"exp1\",\n",
    "                 n_ctx=4,\n",
    "                 ctx_init=\"\",\n",
    "                 class_token_position=\"end\",\n",
    "                 csc=False,\n",
    "                 ):\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.epochs = epochs\n",
    "        self.run_name = run_name\n",
    "        self.n_ctx = n_ctx\n",
    "        self.ctx_init = ctx_init\n",
    "        self.class_token_position = class_token_position\n",
    "        self.csc = csc\n",
    "\n",
    "        # Create a logger for the experiment\n",
    "        self.writer = SummaryWriter(log_dir=f\"runs/CoOp/{run_name}\")\n",
    "        self.writer.add_scalar(f\"lr\", self.learning_rate, 0)\n",
    "        self.writer.add_scalar(f\"momentum\", self.momentum, 0)\n",
    "\n",
    "        # Get dataloaders\n",
    "\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/16\", device=self.device)\n",
    "        self.clip_model = self.clip_model.to(self.device)\n",
    "        self.clip_model = self.clip_model.float()\n",
    "        resolution = self.clip_model.visual.input_resolution\n",
    "        self.train_set, self.val_set, self.test_set = get_data(resolution=resolution)\n",
    "\n",
    "        # split classes into base and novel\n",
    "        self.base_classes, self.novel_classes = base_novel_categories(self.train_set)\n",
    "\n",
    "        # split the three datasets\n",
    "        self.train_base, _ = split_data(self.train_set, self.base_classes)\n",
    "        self.val_base, self.val_novel = split_data(self.val_set, self.base_classes)\n",
    "        self.test_base, self.test_novel = split_data(self.test_set, self.base_classes)\n",
    "\n",
    "        #self.classnames, _ = embed_dataset_classnames(dataset_name, preprocess=preprocess, model=clip_model)\n",
    "\n",
    "        resolution = self.clip_model.visual.input_resolution\n",
    "\n",
    "        cfg = EasyDict()\n",
    "        # Training configuration\n",
    "        cfg.TRAINER = EasyDict()\n",
    "        cfg.TRAINER.COOP = EasyDict()\n",
    "        cfg.TRAINER.COOP.N_CTX = self.n_ctx  # Number of context tokens\n",
    "        cfg.TRAINER.COOP.CTX_INIT = self.ctx_init  # Leave empty for random initialization\n",
    "        cfg.INPUT = EasyDict()\n",
    "        cfg.INPUT.SIZE = [resolution, resolution]  # Must match CLIP model's input resolution\n",
    "\n",
    "        # Instantiate the network and move it to the chosen device (GPU)\n",
    "        self.model = CustomCLIPCoOp(\n",
    "            classnames=[CLASS_NAMES[idx] for idx in self.base_classes],\n",
    "            cfg=cfg,\n",
    "            clip_model=self.clip_model,\n",
    "        ).to(device)\n",
    "\n",
    "        print(\"Turning off gradients in both the image and the text encoder\")\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if \"prompt_learner\" in name:\n",
    "                param.requires_grad_(True)\n",
    "            else:\n",
    "                param.requires_grad_(False)\n",
    "\n",
    "        print(f\"Total parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
    "        print(f\"Total trainable parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "        self.optimizer = self.get_optimizer(self.learning_rate, self.weight_decay, self.momentum)\n",
    "        self.cost_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the CoOp model on the base classes with early stopping and logs performance metrics.\n",
    "        Saves the best model and computes evaluation at the end of training.\n",
    "        \"\"\"\n",
    "        print(\"Before training:\")\n",
    "        print(\"Training the model...\")\n",
    "        print_epoch_interval = 1\n",
    "        pbar = tqdm(total=self.epochs, desc=\"OVERALL TRAINING\", position=0, leave=True)\n",
    "\n",
    "        best_val_acc = 0.0\n",
    "        patience = 3\n",
    "        counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            base_train_loss, base_train_accuracy = training_step(\n",
    "                model=self.model,\n",
    "                dataset=self.train_base,\n",
    "                optimizer=self.optimizer,\n",
    "                batch_size=self.batch_size,\n",
    "                classnames=self.base_classes,\n",
    "                device=self.device,\n",
    "            )\n",
    "\n",
    "            if e % print_epoch_interval == 0:\n",
    "                base_val_loss, base_val_accuracy = eval_step(\n",
    "                    model=self.model,\n",
    "                    dataset=self.val_base,\n",
    "                    cost_function=self.cost_function,\n",
    "                    new_classnames=self.base_classes,\n",
    "                    device=self.device,\n",
    "                    batch_size=self.batch_size,\n",
    "                )\n",
    "\n",
    "                self.log_values(e, base_train_loss, base_train_accuracy, \"train_base\")\n",
    "                self.log_values(e, base_val_loss, base_val_accuracy, \"validation_base\")\n",
    "\n",
    "                pbar.set_postfix(train_acc=base_train_accuracy, val_acc=base_val_accuracy)\n",
    "\n",
    "                # Early stopping check\n",
    "                if base_val_accuracy > best_val_acc:\n",
    "                    best_val_acc = base_val_accuracy\n",
    "                    counter = 0\n",
    "                    best_model_state = self.model.state_dict()\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter >= patience:\n",
    "                        print(f\"Early stopping at epoch {e}, best validation accuracy: {best_val_acc:.4f}\")\n",
    "                        break\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "        # Restore best model if early stopped\n",
    "        if best_model_state is not None:\n",
    "            self.model.load_state_dict(best_model_state)\n",
    "\n",
    "        print(\"After training:\")\n",
    "        self.compute_evaluation(self.epochs)\n",
    "        self.writer.close()\n",
    "\n",
    "        self.save_model()\n",
    "        self.save_prompt_learner()\n",
    "\n",
    "    def save_model(self, path=\"./bin/coop\"):\n",
    "        \"\"\"\n",
    "        Saves the entire model's state dictionary to disk under the specified path.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory path where the model checkpoint will be saved.\n",
    "        \"\"\"\n",
    "        #create folder if not exist\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        # Save the model\n",
    "        torch.save(self.model.state_dict(), os.path.join(path, f\"{self.run_name}.pth\"))\n",
    "\n",
    "    def save_prompt_learner(self, path=\"./bin/coop\"):\n",
    "        \"\"\"\n",
    "        Saves only the prompt learner component of the model to disk.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory path where the prompt learner checkpoint will be saved.\n",
    "        \"\"\"\n",
    "        # Create folder if not exist\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        # Save only the self.ctx parameter of the prompt learner\n",
    "        ctx_state = {\"ctx\": self.model.prompt_learner.ctx.detach().cpu()}\n",
    "        torch.save(ctx_state, os.path.join(path, f\"{self.run_name}_prompt_learner.pth\"))\n",
    "\n",
    "    def load_model(self, path=\"./bin\"):\n",
    "        \"\"\"\n",
    "        Loads a saved model checkpoint from disk and sets the model to evaluation mode.\n",
    "\n",
    "        Args:\n",
    "            path (str): Directory path from which the model checkpoint will be loaded.\n",
    "        \"\"\"\n",
    "        # Load the model\n",
    "        self.model.load_state_dict(torch.load(os.path.join(path, f\"{self.run_name}.pth\")))\n",
    "        self.model.eval()\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "    def compute_evaluation(self, epoch_idx, base=False):\n",
    "        \"\"\"\n",
    "        Evaluates the model (or zero-shot CLIP if base=True) on the base test set and logs accuracy.\n",
    "\n",
    "        Args:\n",
    "            epoch_idx (int): Index of the current epoch for logging.\n",
    "            base (bool): If True, use zero-shot CLIP model for evaluation instead of the trained model.\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy on the base test set.\n",
    "        \"\"\"\n",
    "        base_accuracy = test_step(\n",
    "            self.model if not base else self.clip_model, \n",
    "            self.test_base, \n",
    "            self.batch_size, \n",
    "            self.device, \n",
    "            self.base_classes,\n",
    "            label=\"test\", \n",
    "            base=base\n",
    "        )\n",
    "        # Log to TensorBoard\n",
    "        self.log_value(epoch_idx,  base_accuracy, \"base_classes\")\n",
    "\n",
    "        return base_accuracy\n",
    "\n",
    "    def get_optimizer(self, lr, wd, momentum):\n",
    "        \"\"\"\n",
    "        Instantiates and returns the optimizer for the model parameters.\n",
    "\n",
    "        Args:\n",
    "            lr (float): Learning rate.\n",
    "            wd (float): Weight decay.\n",
    "            momentum (float): Momentum term (unused for Adam optimizer).\n",
    "\n",
    "        Returns:\n",
    "            torch.optim.Optimizer: Configured Adam optimizer instance.\n",
    "        \"\"\"\n",
    "        optimizer = Adam([\n",
    "            {\n",
    "                \"params\": self.model.parameters()\n",
    "            }\n",
    "        ], lr=lr, weight_decay=wd)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def log_value(self, step,  accuracy, prefix):\n",
    "        \"\"\"\n",
    "        Logs a single scalar value (accuracy) to TensorBoard.\n",
    "\n",
    "        Args:\n",
    "            step (int): Training step or epoch index.\n",
    "            accuracy (float): Accuracy value to log.\n",
    "            prefix (str): Tag prefix to categorize the metric in TensorBoard.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)\n",
    "\n",
    "    def log_values(self, step, loss, accuracy, prefix):\n",
    "        \"\"\"\n",
    "        Logs both loss and accuracy values to TensorBoard.\n",
    "\n",
    "        Args:\n",
    "            step (int): Training step or epoch index.\n",
    "            loss (float): Loss value to log.\n",
    "            accuracy (float): Accuracy value to log.\n",
    "            prefix (str): Tag prefix to categorize the metrics in TensorBoard.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(f\"{prefix}/loss\", loss, step)\n",
    "        self.writer.add_scalar(f\"{prefix}/accuracy\", accuracy, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d44ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/core/DoubleDatasetTrainingMethod.py\n",
    "\"\"\"\n",
    " Abstract base class for training methods. it should have forward_backward method to be fulfilled by its children\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any, Callable\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import AverageMeter, ContiguousLabelDataset\n",
    "\n",
    "class DoubleDatasetTrainingMethod:\n",
    "    \"\"\"\n",
    "    Abstract base class for training methods. it should have forward_backward method to be fulfilled by its children\n",
    "\n",
    "    The TrainingMethod class serves as an abstract interface that standardizes the structure of different training strategies.\n",
    "    It defines key methods that all training strategies must implement, such as:\n",
    "     - `get_metrics`: to initialize and return the performance metrics.\n",
    "     - `get_data_loader`: to prepare the DataLoader tailored for the specific training strategy.\n",
    "     - `forward_backward`: to implement the forward and backward passes during training.\n",
    "     - `debug_metrics_to_pbar_args`: to convert debug information for progress bar visualization.\n",
    "     - `training_step_return`: to return summary metrics after a training epoch.\n",
    "     It also provides common functionality like `start_training`, `optimizer_step`, and `train_step` to be reused across concrete training methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: Any, optimizer: Any, title: str, debug) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the training method.\n",
    "\n",
    "        Args:\n",
    "            model (Any): The model to train.\n",
    "            optimizer (Any): The optimizer to use for training.\n",
    "            title (str): Title for identification (e.g., for progress display).\n",
    "            debug (bool): Flag to enable debug mode for extra logging.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.title = title\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self.debug = debug\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Initialize and return a dictionary of performance metrics.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: Dictionary mapping metric names to metric trackers.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_loader1(self, dataset, batch_size) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Create a data loader for the training dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: The training dataset.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch data loader instance.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    @abstractmethod\n",
    "    def get_data_loader2(self, dataset, batch_size) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Create a data loader for the training dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: The training dataset.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch data loader instance.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def debug_metrics_to_pbar_args1(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Format debug metrics for display in a progress bar.\n",
    "\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Metrics from the current step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Formatted metrics for tqdm display.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def debug_metrics_to_pbar_args2(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Format debug metrics for display in a progress bar.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Generate summary metrics after completing a training epoch.\n",
    "\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Collected training metrics.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: List of averaged metric values for reporting.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def optimizer_step(self) -> None:\n",
    "        \"\"\"\n",
    "        Apply the optimizer's step and zero the gradients.\n",
    "        \"\"\"\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def start_training(self) -> None:\n",
    "        \"\"\"\n",
    "        Set the model in training mode.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_backward1(\n",
    "            self, sample, batch_idx, metrics: Dict[str, AverageMeter], dataset: ContiguousLabelDataset, classes: list[int]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Execute forward and backward pass, compute loss, and update metrics.\n",
    "\n",
    "        Args:\n",
    "            sample: Current batch sample from data loader.\n",
    "            batch_idx (int): Index of the current batch.\n",
    "            metrics (Dict[str, AverageMeter]): Metric trackers.\n",
    "            dataset (ContiguousLabelDataset): Dataset wrapper for label mapping.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary of current debug metric values.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_backward2(\n",
    "            self, sample, batch_idx, metrics: Dict[str, AverageMeter], dataset: ContiguousLabelDataset, classes: list[int]\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Execute forward and backward pass, compute loss, and update metrics.\n",
    "        \"\"\"\n",
    "\n",
    "    def double_datasets_train_step(self, dataset1, dataset2, batch_size, names: list[str], classes1, classes2):\n",
    "        assert len(names) == 2, \"Number of names must be 2\"\n",
    "        metrics = self.get_metrics()\n",
    "        self.start_training()\n",
    "        tmp_dataset1 = ContiguousLabelDataset(dataset1, classes1)\n",
    "        tmp_dataset2 = ContiguousLabelDataset(dataset2, classes2)\n",
    "\n",
    "        dataloader1 = self.get_data_loader1(tmp_dataset1, batch_size)\n",
    "        dataloader2 = self.get_data_loader2(tmp_dataset2, batch_size)\n",
    "\n",
    "        pbar = tqdm(dataloader1, desc=f\"Training-{self.title}/{names[0]}\", position=1, leave=False)\n",
    "        for batch_idx, sample in enumerate(dataloader1):\n",
    "            debug_metrics = self.forward_backward1(sample, batch_idx, metrics, tmp_dataset1, classes1)\n",
    "            pbar.set_postfix(\n",
    "                self.debug_metrics_to_pbar_args1(debug_metrics)\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "        \n",
    "        pbar = tqdm(dataloader2, desc=f\"Training-{self.title}/{names[1]}\", position=1, leave=False)\n",
    "        for batch_idx, sample in enumerate(dataloader2):\n",
    "            debug_metrics = self.forward_backward2(sample, batch_idx, metrics, tmp_dataset2, classes2)\n",
    "            pbar.set_postfix(\n",
    "                self.debug_metrics_to_pbar_args2(debug_metrics)\n",
    "            )\n",
    "            pbar.update(1)       \n",
    "\n",
    "        return self.training_step_return(metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9902df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/core/EvaluationMethod.py\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.metrics import AverageMeter\n",
    "\n",
    "\n",
    "class EvaluationMethod(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for evaluation methods. Standardizes evaluation interface for different strategies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, batch_size: int = 32, device=None):\n",
    "        \"\"\"\n",
    "        Initialize evaluation method.\n",
    "\n",
    "        Args:\n",
    "            model: The model to evaluate.\n",
    "            batch_size (int): Evaluation batch size.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.device = next(self.model.parameters()).device if device is None else device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    @abstractmethod\n",
    "    def evaluate(self, dataset, classnames, desc_add=\"\") -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Perform the evaluation.\n",
    "\n",
    "        Args:\n",
    "            dataset: The dataset to evaluate on.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary with evaluation results (e.g., accuracy, loss).\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee004057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/core/TrainingMethod.py\n",
    "\"\"\"\n",
    " Abstract base class for training methods. it should have forward_backward method to be fulfilled by its children\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any, Callable\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils import AverageMeter, ContiguousLabelDataset\n",
    "\n",
    "\n",
    "class TrainingMethod(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for training methods. it should have forward_backward method to be fulfilled by its children\n",
    "\n",
    "    The TrainingMethod class serves as an abstract interface that standardizes the structure of different training strategies.\n",
    "    It defines key methods that all training strategies must implement, such as:\n",
    "     - `get_metrics`: to initialize and return the performance metrics.\n",
    "     - `get_data_loader`: to prepare the DataLoader tailored for the specific training strategy.\n",
    "     - `forward_backward`: to implement the forward and backward passes during training.\n",
    "     - `debug_metrics_to_pbar_args`: to convert debug information for progress bar visualization.\n",
    "     - `training_step_return`: to return summary metrics after a training epoch.\n",
    "     It also provides common functionality like `start_training`, `optimizer_step`, and `train_step` to be reused across concrete training methods.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: Any, optimizer: Any, title: str, debug) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the training method.\n",
    "\n",
    "        Args:\n",
    "            model (Any): The model to train.\n",
    "            optimizer (Any): The optimizer to use for training.\n",
    "            title (str): Title for identification (e.g., for progress display).\n",
    "            debug (bool): Flag to enable debug mode for extra logging.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.title = title\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        self.debug = debug\n",
    "        self.mlp_adversary = None\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Initialize and return a dictionary of performance metrics.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: Dictionary mapping metric names to metric trackers.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_data_loader(self, dataset, batch_size) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Create a data loader for the training dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: The training dataset.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch data loader instance.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward_backward(\n",
    "            self, sample, batch_idx, metrics: Dict[str, AverageMeter], dataset: ContiguousLabelDataset, accumulation_steps: int = 1, step: int = 0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Execute forward and backward pass, compute loss, and update metrics.\n",
    "\n",
    "        Args:\n",
    "            sample: Current batch sample from data loader.\n",
    "            batch_idx (int): Index of the current batch.\n",
    "            metrics (Dict[str, AverageMeter]): Metric trackers.\n",
    "            dataset (ContiguousLabelDataset): Dataset wrapper for label mapping.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary of current debug metric values.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def debug_metrics_to_pbar_args(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Format debug metrics for display in a progress bar.\n",
    "\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Metrics from the current step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Formatted metrics for tqdm display.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Generate summary metrics after completing a training epoch.\n",
    "\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Collected training metrics.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: List of averaged metric values for reporting.\n",
    "        \"\"\"\n",
    "\n",
    "    def optimizer_step(self) -> None:\n",
    "        \"\"\"\n",
    "        Apply the optimizer's step and zero the gradients.\n",
    "        \"\"\"\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    def start_training(self) -> None:\n",
    "        \"\"\"\n",
    "        Set the model in training mode.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "    def train_step(self, dataset, batch_size, classnames, accumulation_steps: int = 1):\n",
    "        \"\"\"\n",
    "        Perform a complete training epoch, including data loading, training, and metric collection.\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset used for training.\n",
    "            batch_size (int): Number of samples per training batch.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Averaged values for each tracked metric after the epoch.\n",
    "        \"\"\"\n",
    "        metrics = self.get_metrics()\n",
    "        self.start_training()\n",
    "        tmp_dataset = ContiguousLabelDataset(dataset, classnames)\n",
    "        dataloader = self.get_data_loader(tmp_dataset, batch_size)\n",
    "        pbar = tqdm(dataloader, desc=f\"Training-{self.title}\", position=1, leave=False)\n",
    "        for batch_idx, sample in enumerate(dataloader):\n",
    "            debug_metrics = self.forward_backward(sample, batch_idx, metrics, tmp_dataset, accumulation_steps=accumulation_steps, step=batch_idx)\n",
    "            pbar.set_postfix(\n",
    "                self.debug_metrics_to_pbar_args(debug_metrics)\n",
    "            )\n",
    "            pbar.update(1)\n",
    "        if accumulation_steps > 1:\n",
    "            if (batch_idx + 1) % accumulation_steps != 0:\n",
    "                if self.mlp_adversary is not None:\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        list(self.model.parameters()) + list(self.mlp_adversary.parameters()),\n",
    "                        max_norm=1.0,\n",
    "                        norm_type=2.0,\n",
    "                        error_if_nonfinite=True\n",
    "                    )\n",
    "\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "        \n",
    "        return self.training_step_return(metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d4c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cbb848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/evaluation_methods/EvalStep.py\n",
    "\n",
    "from typing import Dict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# from training_systems.core import EvaluationMethod\n",
    "# from utils import ContiguousLabelDataset, CLASS_NAMES, AverageMeter\n",
    "\n",
    "\n",
    "class EvalStep(EvaluationMethod):\n",
    "    \"\"\"\n",
    "    Generic evaluation step for models that support temporary class name modification.\n",
    "    \"\"\"\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataset, classnames, desc_add=\"\") -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate model performance on the provided dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset for evaluation.\n",
    "            new_classnames (list, optional): New class names to apply temporarily.\n",
    "            desc_add (str): Suffix to append to tqdm description.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary containing average loss and accuracy.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        loss_meter = AverageMeter()\n",
    "        accuracy_meter = AverageMeter()\n",
    "        tmp_dataset = ContiguousLabelDataset(dataset, classnames)\n",
    "        dataloader = DataLoader(tmp_dataset, batch_size=self.batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "        remapped_classnames = [ CLASS_NAMES[ tmp_dataset.idx2cat[i] ] for i in range(len(tmp_dataset.idx2cat)) ]\n",
    "        with self.model.temporary_classnames(remapped_classnames):\n",
    "            self.walk(loss_meter, accuracy_meter, dataloader, desc_add)\n",
    "\n",
    "        return {\"loss\": loss_meter.avg, \"accuracy\": accuracy_meter.avg}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def walk(self, loss_meter, accuracy_meter, dataloader, desc_add=\"\"):\n",
    "        \"\"\"\n",
    "        Perform the evaluation loop over the dataset.\n",
    "\n",
    "        Args:\n",
    "            loss_meter: Tracks average loss.\n",
    "            accuracy_meter: Tracks average accuracy.\n",
    "            dataloader: DataLoader to iterate over.\n",
    "            desc_add (str): Additional string to append to tqdm description.\n",
    "        \"\"\"\n",
    "        for images, targets in tqdm(dataloader, desc=\"Validation\" + desc_add, position=1, leave=False):\n",
    "            images = images.to(self.device)\n",
    "            targets = targets.to(self.device)\n",
    "            logits = self.model(images)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            correct = (predictions == targets).sum().item()\n",
    "            loss_meter.update(loss.item(), n=targets.size(0))\n",
    "            accuracy_meter.update(correct, n=targets.size(0), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e762db7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/evaluation_methods/TestSteps.py\n",
    "from typing import Dict\n",
    "\n",
    "import clip\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# from training_systems.core import EvaluationMethod\n",
    "# from utils import ContiguousLabelDataset, CLASS_NAMES, AverageMeter\n",
    "\n",
    "\n",
    "class ZeroShotTestStep(EvaluationMethod):\n",
    "    \"\"\"\n",
    "    Evaluation method for models that have been fine-tuned (e.g., CoCoOp or adversarial models).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, batch_size, categories):\n",
    "        super().__init__(model, batch_size)\n",
    "        self.categories = categories\n",
    "        # here we apply the standard CLIP template used for oxford flowers to all categories\n",
    "        text_inputs = clip.tokenize(\n",
    "            [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in self.categories]\n",
    "        ).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            # we can encode the text features once as they are shared for all images\n",
    "            # therefore we do it outside the evaluation loop\n",
    "            self.text_features = self.model.encode_text(text_inputs)\n",
    "            # and here we normalize them (standard pratice with CLIP)\n",
    "            self.text_features /= self.text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataset, desc_add=\"\") -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "        accuracy_meter = AverageMeter()\n",
    "        tmp_dataset = ContiguousLabelDataset(dataset, self.categories)\n",
    "        dataloader = DataLoader(tmp_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "        # Remap labels into a contiguous set starting from zero\n",
    "        self.walk(dataloader, accuracy_meter, desc_add)\n",
    "\n",
    "        return {\"accuracy\": accuracy_meter.avg}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def walk(self, dataloader, accuracy_meter, desc_add):\n",
    "        pbar = tqdm(total=len(dataloader), desc=\"Test (Zero Shots) \" + desc_add, position=1, leave=False)\n",
    "        for image, target in dataloader:\n",
    "            # base categories range from 0 to 50, while novel ones from 51 to 101\n",
    "            # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "            # Map targets in contiguous set starting from zero\n",
    "            # Labels needs to be .long() in pytorch\n",
    "            \n",
    "            image = image.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "\n",
    "            # forward image through CLIP image encoder\n",
    "            image_features = self.model.encode_image(image)\n",
    "            # and normalize\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "            predicted_class = (image_features @ self.text_features.T).argmax(dim=-1)\n",
    "            # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "\n",
    "            correct = (predicted_class == target).sum().item()\n",
    "            accuracy_meter.update(correct, n=target.size(0), raw=True)\n",
    "            pbar.set_postfix({\n",
    "                \"accuracy\" : accuracy_meter.avg\n",
    "            })\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "class FineTunedTestStep(EvaluationMethod):\n",
    "    \"\"\"\n",
    "    Evaluation method for the f rozen base CLIP model.\n",
    "    \"\"\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, dataset, classnames: list[int], desc_add=\"\") -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "        accuracy_meter = AverageMeter()\n",
    "        tmp_dataset = ContiguousLabelDataset(dataset, classnames)\n",
    "        dataloader = DataLoader(tmp_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "        remapped_class_names = [CLASS_NAMES[tmp_dataset.idx2cat[i]] for i in range(len(tmp_dataset.idx2cat))]\n",
    "        with self.model.temporary_classnames(remapped_class_names):\n",
    "            pbar = tqdm(total=len(dataloader), desc=\"Test (Finetuned) \" + desc_add, position=1, leave=False)\n",
    "            for images, targets in dataloader:\n",
    "                images = images.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                logits = self.model(images)\n",
    "                predictions = logits.argmax(dim=-1)\n",
    "                correct = (predictions == targets).sum().item()\n",
    "                accuracy_meter.update(correct, n=targets.size(0), raw=True)\n",
    "                pbar.set_postfix({\n",
    "                    \"accuracy\": accuracy_meter.avg\n",
    "                })\n",
    "                pbar.update(1)\n",
    "\n",
    "        return {\"accuracy\": accuracy_meter.avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f52cc37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/evaluation_methods/__init__.py\n",
    "# from training_systems.evaluation_methods.TestSteps import FineTunedTestStep, ZeroShotTestStep\n",
    "# from training_systems.evaluation_methods.EvalStep import EvalStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8affca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/training_methods/Adversarial.py\n",
    "\"\"\"\n",
    "This module implements the Adversarial training method, which incorporates a gradient reversal layer\n",
    "and an adversarial MLP to encourage domain-invariant feature learning.\n",
    "\"\"\"\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from training_systems.core import TrainingMethod\n",
    "# from utils import AverageMeter, ContiguousLabelDataset, CLASS_NAMES\n",
    "# from model.cocoop.mlp_adversary import AdversarialMLP, GradientReversalLayer\n",
    "\n",
    "\n",
    "class Adversarial(TrainingMethod):\n",
    "    \"\"\"\n",
    "    Adversarial training method using a Gradient Reversal Layer and an MLP adversary.\n",
    "\n",
    "    Attributes:\n",
    "        cls_cluster_dict (Dict[int, Any]): Maps class indices to cluster labels.\n",
    "        grl (GradientReversalLayer): The gradient reversal layer instance.\n",
    "        mlp_adversary (AdversarialMLP): The adversarial MLP used to confuse cluster prediction.\n",
    "        lambda_adv (float): Weight of the adversarial loss term.\n",
    "        debug (bool): If True, print debug information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: Any,\n",
    "            optimizer: Any,\n",
    "            cls_cluster_dict: Dict[int, Any],\n",
    "            grl: GradientReversalLayer,\n",
    "            mlp_adversary: AdversarialMLP,\n",
    "            lambda_adv,\n",
    "            tmp_classes: list,\n",
    "            debug: bool = False,\n",
    "            gaussian_noise: float = 0.0,\n",
    "            use_bias_ctx: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (Any): The main model being trained.\n",
    "            optimizer (Any): Optimizer for updating model parameters.\n",
    "            cls_cluster_dict (Dict[int, Any]): Mapping from class labels to clusters.\n",
    "            grl (GradientReversalLayer): The gradient reversal layer.\n",
    "            mlp_adversary (AdversarialMLP): The adversarial network module.\n",
    "            lambda_adv (float): Weight for the adversarial loss.\n",
    "            debug (bool, optional): Enables debug mode. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__(model, optimizer, \"Adv.\", debug)\n",
    "        self.cls_cluster_dict = cls_cluster_dict\n",
    "        self.grl = grl\n",
    "        self.mlp_adversary = mlp_adversary\n",
    "        self.lambda_adv = lambda_adv\n",
    "        self.tmp_classes = tmp_classes\n",
    "        self.gaussian_noise = gaussian_noise\n",
    "        self.use_bias_ctx = use_bias_ctx\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: Dictionary containing initialized metrics for training.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"total_loss_metric\": AverageMeter(),\n",
    "            \"ce_loss_metric\": AverageMeter(),\n",
    "            \"adv_loss_metric\": AverageMeter(),\n",
    "            \"accuracy_metric\": AverageMeter(),\n",
    "        }\n",
    "\n",
    "    def get_data_loader(self, dataset: ContiguousLabelDataset, batch_size: int) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (ContiguousLabelDataset): Dataset to be used.\n",
    "            batch_size (int): Size of each batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: Configured PyTorch DataLoader instance.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "        )\n",
    "\n",
    "    def forward_backward(\n",
    "            self,\n",
    "            sample,\n",
    "            batch_idx,\n",
    "            metrics: Dict[str, AverageMeter],\n",
    "            dataset: ContiguousLabelDataset,\n",
    "            accumulation_steps: int = 1,\n",
    "            step: int = 0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Executes the forward and backward pass.\n",
    "\n",
    "        Args:\n",
    "            sample (Tuple[Tensor, Tensor]): Batch of input data and targets.\n",
    "            batch_idx (int): Index of the current batch.\n",
    "            metrics (Dict[str, AverageMeter]): Dictionary of metrics to be updated.\n",
    "            dataset (ContiguousLabelDataset): Dataset for cluster label lookup.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary with loss and accuracy metrics.\n",
    "        \"\"\"\n",
    "        # Load data into GPU\n",
    "        inputs, targets = sample\n",
    "        inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "        targets_real_category = [dataset.idx2cat[c.item()] for c in targets]\n",
    "        cluster_target = [int(self.cls_cluster_dict[int(tl)]) for tl in targets_real_category]\n",
    "        cluster_target = torch.tensor(\n",
    "            cluster_target,\n",
    "            device=targets.device,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # Use all tmp_classes for adversarial phase\n",
    "        with self.model.temporary_classnames([CLASS_NAMES[idx] for idx in self.tmp_classes]):\n",
    "            logits, ce_loss, img_features, ctx, bias, avg_txt_features, selected_text_features = self.model(inputs, targets, get_image_features=True)\n",
    "\n",
    "            if self.gaussian_noise > 0:\n",
    "                noise = torch.randn_like(logits) * self.gaussian_noise\n",
    "                noisy_logits = logits + noise\n",
    "            else:\n",
    "                noisy_logits = logits\n",
    "\n",
    "            if self.use_bias_ctx:\n",
    "                if ctx.shape[0] == 1:\n",
    "                    ctx = ctx.expand(bias.shape[0], -1, -1)\n",
    "\n",
    "                ctx_shifted = ctx + bias  # shape: [B, L, D]\n",
    "                # concat = ctx_shifted.view(ctx_shifted.size(0), -1).to(dtype=torch.float32)\n",
    "                concat = torch.cat([selected_text_features, ctx_shifted.mean(dim=1)], dim=1).to(dtype=torch.float32)\n",
    "\n",
    "            else:\n",
    "                concat = torch.cat([avg_txt_features, noisy_logits], dim=1).to(dtype=torch.float32)\n",
    "            # print(f\"concat shape: {concat.shape}, bias shape: {bias.shape}, ctx shape: {ctx.shape}, avg_txt_features shape: {avg_txt_features.shape}, logits shape: {logits.shape}\")\n",
    "            reversed_concat = self.grl(concat)\n",
    "            # print(f\"reversed_concat shape: {reversed_concat.shape}\")\n",
    "            cluster_logits = self.mlp_adversary(reversed_concat)\n",
    "\n",
    "            if cluster_logits.shape[1] == 1:\n",
    "                # Binary classification\n",
    "                cluster_logits = cluster_logits.squeeze(-1)\n",
    "                loss_adv = F.binary_cross_entropy_with_logits(cluster_logits, cluster_target.float())\n",
    "            else:\n",
    "                # Multi-class classification\n",
    "                loss_adv = F.cross_entropy(cluster_logits, cluster_target.long())\n",
    "\n",
    "            # Skip adversarial update if prompt learner is frozen\n",
    "            if any(p.requires_grad for p in self.model.prompt_learner.parameters()):\n",
    "                ce_grads = self.get_grads(ce_loss)\n",
    "            else:\n",
    "                ce_grads = None  # Or torch.zeros_like(...), depending on downstream use\n",
    "            total_loss = ce_loss + self.lambda_adv * loss_adv\n",
    "\n",
    "            total_loss = total_loss / accumulation_steps\n",
    "            total_loss.backward()\n",
    "            # print(f\"step: {step}, total_loss: {total_loss.item():.4f}, accumulation_steps: {accumulation_steps}, \")\n",
    "            # --- accumulate grads and update ---\n",
    "            if (step + 1) % accumulation_steps == 0: \n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    list(self.model.parameters()) + list(self.mlp_adversary.parameters()),\n",
    "                    max_norm=1.0,\n",
    "                    norm_type=2.0,\n",
    "                    error_if_nonfinite=True\n",
    "                )\n",
    "                self.optimizer_step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            if batch_idx < 3 and self.debug:  # Log only a few batches for performance\n",
    "                print(f\"[Batch {batch_idx}] CE Loss: {ce_loss.item():.4f} | \"\n",
    "                      f\"Adv Loss: {loss_adv.item():.4f} | \"\n",
    "                      f\"Total Loss: {total_loss.item():.4f} | \"\n",
    "                      f\"lambda_adv: {self.lambda_adv:.4f}\")\n",
    "            batch_size = inputs.shape[0]\n",
    "            metrics[\"total_loss_metric\"].update(total_loss.item(), n=batch_size)\n",
    "            metrics[\"ce_loss_metric\"].update(ce_loss.item(), n=batch_size)\n",
    "            metrics[\"adv_loss_metric\"].update(loss_adv.item(), n=batch_size)\n",
    "            _, predicted = logits.max(dim=1)\n",
    "            correct = predicted.eq(targets).sum().item()\n",
    "            metrics[\"accuracy_metric\"].update(correct, n=batch_size, raw=True)\n",
    "            return {\n",
    "                \"total_loss\": total_loss.item(),\n",
    "                \"ce_loss\": ce_loss.item(),\n",
    "                \"adv_loss\": loss_adv.item(),\n",
    "                \"accuracy\": correct / batch_size,\n",
    "            }\n",
    "\n",
    "    def print_grads_norms(self, bce_grads, ce_grads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            bce_grads (Dict[str, Tensor]): Gradients from BCE loss.\n",
    "            ce_grads (Dict[str, Tensor]): Gradients from CE loss.\n",
    "        \"\"\"\n",
    "        for name in ce_grads:\n",
    "            ce_norm = ce_grads[name].norm().item()\n",
    "            bce_norm = bce_grads[name].norm().item()\n",
    "            print(f\"{name}: CE grad norm = {ce_norm:.4e}, BCE grad norm = {bce_norm:.4e}\")\n",
    "\n",
    "    def get_grads(self, loss):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            loss (Tensor): Loss tensor to backpropagate.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Tensor]: Dictionary of gradients.\n",
    "        \"\"\"\n",
    "        loss.backward(retain_graph=True)\n",
    "        ce_grads = {}\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if p.grad is not None and \"prompt_learner\" in name:\n",
    "                ce_grads[name] = p.grad.detach().clone()\n",
    "        # --- Zero gradients ---\n",
    "        self.optimizer.zero_grad()\n",
    "        return ce_grads\n",
    "\n",
    "    def debug_metrics_to_pbar_args(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Metrics from current training step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Same metrics, passed to progress bar.\n",
    "        \"\"\"\n",
    "        return debug_metrics\n",
    "\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Collected training metrics.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Average values of total, accuracy, CE, and adversarial losses.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            metrics[\"total_loss_metric\"].avg,\n",
    "            metrics[\"accuracy_metric\"].avg,\n",
    "            metrics[\"ce_loss_metric\"].avg,\n",
    "            metrics[\"adv_loss_metric\"].avg,\n",
    "        ]\n",
    "\n",
    "    def update_lambda_adv(self, lambda_adv) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lambda_adv (float): New value to set for lambda_adv.\n",
    "        \"\"\"\n",
    "        self.lambda_adv = lambda_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "091c472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/training_methods/BaseCoCoOp.py\n",
    "\"\"\"\n",
    "This module defines the BaseCoCoOp training method, a baseline implementation for CoCoOp-based optimization.\n",
    "It includes metric tracking, data loading, and training step execution using standard cross-entropy loss.\n",
    "\"\"\"\n",
    "import random\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# from training_systems.core import TrainingMethod\n",
    "# from utils import AverageMeter, ContiguousLabelDataset\n",
    "\n",
    "\n",
    "class BaseCoCoOp(TrainingMethod):\n",
    "    \"\"\"\n",
    "    BaseCoCoOp implements a simple training routine based on cross-entropy loss without adversarial components.\n",
    "    This class inherits from TrainingMethod and provides basic training loop functionalities.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: Any,\n",
    "            optimizer: Any,\n",
    "            debug: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__(model, optimizer, \"Base CoCoOp\", debug)\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Initializes and returns the performance metrics used during training.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: A dictionary with average meters for loss and accuracy.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"loss_metric\": AverageMeter(),\n",
    "            \"accuracy_metric\": AverageMeter(),\n",
    "        }\n",
    "\n",
    "    def get_data_loader(self, dataset: ContiguousLabelDataset, batch_size: int) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Creates and returns a DataLoader for the given dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (ContiguousLabelDataset): Dataset to load samples from.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch DataLoader configured with shuffle and multiple workers.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "        )\n",
    "\n",
    "    def forward_backward(\n",
    "            self,\n",
    "            sample,\n",
    "            batch_idx,\n",
    "            metrics: Dict[str, AverageMeter],\n",
    "            dataset: ContiguousLabelDataset,\n",
    "            accumulation_steps: int = 1,\n",
    "            step: int = 0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Executes the forward and backward pass for the BaseCoCoOp training method.\n",
    "\n",
    "        Args:\n",
    "            sample (Tuple[Tensor, Tensor]): Batch of input data and targets.\n",
    "            batch_idx (int): Index of the current batch.\n",
    "            metrics (Dict[str, AverageMeter]): Dictionary of metrics to be updated.\n",
    "            dataset (ContiguousLabelDataset): Dataset object (unused in this implementation).\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary with current loss and accuracy values.\n",
    "        \"\"\"\n",
    "        # Load data into GPU\n",
    "        inputs, targets = sample\n",
    "        # === Pseudo-base: cross-entropy ===\n",
    "        inputs_base = inputs.to(self.device)\n",
    "        targets_base = targets.to(self.device)\n",
    "\n",
    "        logits_base, loss_ce = self.model(inputs_base, targets_base)\n",
    "        # === Combine losses ===\n",
    "        print(\"SHAPES LOGITS: \",logits_base.shape, targets_base.shape)\n",
    "        \n",
    "        loss_ce = loss_ce / accumulation_steps\n",
    "        loss_ce.backward()\n",
    "\n",
    "        # optimizer step every `accumulation_steps` steps\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        self.optimizer_step()\n",
    "        batch_size_total = inputs_base.size(0)\n",
    "\n",
    "        metrics[\"loss_metric\"].update(loss_ce.item(), n=batch_size_total)\n",
    "\n",
    "        _, predicted = logits_base.max(dim=1)\n",
    "        correct = (predicted == targets_base).sum().item()\n",
    "        total = targets_base.size(0)\n",
    "        metrics[\"accuracy_metric\"].update(correct, n=total, raw=True)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss_ce.item(),\n",
    "            \"accuracy\": correct / targets_base.size(0),\n",
    "        }\n",
    "\n",
    "    def debug_metrics_to_pbar_args(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Passes debug metrics directly to be displayed in the progress bar.\n",
    "\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Metrics from the current step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Unmodified metrics suitable for display.\n",
    "        \"\"\"\n",
    "        return debug_metrics\n",
    "\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Extracts and returns the average loss and accuracy metrics after a training step.\n",
    "\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Dictionary containing tracked metrics.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: A list containing the average loss and accuracy values.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            metrics[\"loss_metric\"].avg,\n",
    "            metrics[\"accuracy_metric\"].avg,\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef7d7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/training_methods/KLCoCoOp.py\n",
    "\"\"\"\n",
    "This module defines the KLCoCoOp training method, which combines standard cross-entropy loss with KL divergence\n",
    "between a model's predictions and a frozen teacher (e.g., CLIP) to enhance generalization to novel classes.\n",
    "\"\"\"\n",
    "import random\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from training_systems.core.TrainingMethod import TrainingMethod\n",
    "\n",
    "# from utils import AverageMeter,ContiguousLabelDataset, get_kl_loss\n",
    "# from utils.datasets import CLASS_NAMES\n",
    "\n",
    "\n",
    "class KLCoCoOp(TrainingMethod):\n",
    "    \"\"\"\n",
    "    KLCoCoOp training method combining cross-entropy classification on pseudo-base samples and KL divergence\n",
    "    loss on pseudo-novel samples. It encourages transferability and generalization by mixing base and novel categories.\n",
    "\n",
    "    Attributes:\n",
    "        lambda_kl (float): Weight for the KL divergence loss component.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: Any,\n",
    "            optimizer: Any,\n",
    "            lambda_kl,\n",
    "            debug: bool = False\n",
    "    ) -> None:\n",
    "        super().__init__(model, optimizer, \"Base CoCoOp + KL\", debug)\n",
    "        self.lambda_kl = lambda_kl\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Initializes training metrics including total, cross-entropy, KL divergence losses, and classification accuracy.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: Dictionary of metric names mapped to their respective AverageMeter instances.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"total_loss_metric\": AverageMeter(),\n",
    "            \"ce_loss_metric\": AverageMeter(),\n",
    "            \"kl_loss_metric\": AverageMeter(),\n",
    "            \"accuracy_metric\": AverageMeter(),\n",
    "        }\n",
    "\n",
    "    def get_data_loader(self, dataset: ContiguousLabelDataset, batch_size: int) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns a DataLoader that splits each batch into pseudo-base and pseudo-novel subsets.\n",
    "\n",
    "        Args:\n",
    "            dataset (ContiguousLabelDataset): The dataset used for training.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch DataLoader with a custom collate function to separate base and novel samples.\n",
    "        \"\"\"\n",
    "\n",
    "        def custom_collate(batch):\n",
    "            base_samples = []\n",
    "            novel_samples = []\n",
    "            targets_in_batch = list(set([target for _, target in batch]))\n",
    "            random.shuffle(targets_in_batch)\n",
    "            split_idx = int(0.7 * len(targets_in_batch))\n",
    "            pseudo_base_ids = targets_in_batch[:split_idx]\n",
    "            pseudo_novel_ids = targets_in_batch[split_idx:]\n",
    "            for img, label in batch:\n",
    "                if label in pseudo_base_ids:\n",
    "                    base_samples.append((img, label))\n",
    "                elif label in pseudo_novel_ids:\n",
    "                    novel_samples.append((img, label))\n",
    "            return base_samples, novel_samples\n",
    "\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            collate_fn=custom_collate,\n",
    "        )\n",
    "\n",
    "    def forward_backward(\n",
    "            self,\n",
    "            sample,\n",
    "            batch_idx,\n",
    "            metrics: Dict[str, AverageMeter],\n",
    "            dataset: ContiguousLabelDataset,\n",
    "            accumulation_steps: int = 1,\n",
    "            step: int = 0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Performs forward and backward passes, computing CE loss on pseudo-base and KL loss on pseudo-novel samples.\n",
    "\n",
    "        Args:\n",
    "            sample (Tuple[List[Tuple[Tensor, int]], List[Tuple[Tensor, int]]]): Tuple of pseudo-base and pseudo-novel batches.\n",
    "            batch_idx (int): Current batch index during training.\n",
    "            metrics (Dict[str, AverageMeter]): Dictionary to update with the training metrics.\n",
    "            dataset (ContiguousLabelDataset): Dataset object used for KL divergence lookup.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Scalar values of total loss, CE loss, KL loss, and CE accuracy.\n",
    "        \"\"\"\n",
    "        # Load data into GPU\n",
    "        base_batch, novel_batch = sample\n",
    "\n",
    "        if not base_batch or not novel_batch:\n",
    "            return {\n",
    "                metric: val.avg\n",
    "                for metric, val in metrics.items()\n",
    "            }\n",
    "\n",
    "        # === Pseudo-base: cross-entropy ===\n",
    "        inputs_base = torch.stack([img for img, _ in base_batch]).to(self.device)\n",
    "        targets_base = torch.tensor([lbl for _, lbl in base_batch]).to(self.device)\n",
    "\n",
    "\n",
    "        categories_base_tensor = [dataset.idx2cat[int(c.item())] for c in list(set(targets_base))]\n",
    "        remapped_class_names = [ CLASS_NAMES[ c ] for c in categories_base_tensor ]\n",
    "\n",
    "        target_remapping = {cat:idx for idx, cat in enumerate(categories_base_tensor)}\n",
    "        target_original = [dataset.idx2cat[int(c.item())] for c in targets_base]\n",
    "        target_remapped = torch.tensor([target_remapping[c] for c in target_original]).to(self.device)\n",
    "\n",
    "        with self.model.temporary_classnames(remapped_class_names):\n",
    "            self.model.train()\n",
    "            logits_base, loss_ce = self.model(inputs_base, target_remapped)\n",
    "\n",
    "        # === Pseudo-novel: KL divergence with frozen CLIP ===\n",
    "        self.model.eval()  # needed to disable dropout etc.\n",
    "        inputs_novel = torch.stack([img for img, _ in novel_batch]).to(self.device)\n",
    "        targets_novel = [lbl for _, lbl in novel_batch]\n",
    "\n",
    "        kl_loss = get_kl_loss(self.device, inputs_novel, self.model, targets_novel, dataset)\n",
    "\n",
    "        # === Combine losses ===\n",
    "        total_loss = loss_ce + self.lambda_kl * kl_loss\n",
    "\n",
    "        total_loss = total_loss / accumulation_steps\n",
    "        total_loss.backward()\n",
    "\n",
    "        # optimizer step every `accumulation_steps` steps\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        batch_size_total = inputs_base.size(0) + inputs_novel.size(0)\n",
    "\n",
    "        metrics[\"total_loss_metric\"].update(total_loss.item(), n=batch_size_total)\n",
    "        metrics[\"ce_loss_metric\"].update(loss_ce.item(), n=inputs_base.size(0))\n",
    "        metrics[\"kl_loss_metric\"].update(kl_loss.item(), n=inputs_novel.size(0))\n",
    "\n",
    "        _, predicted = logits_base.max(dim=1)\n",
    "        correct = (predicted == target_remapped).sum().item()\n",
    "        total = target_remapped.size(0)\n",
    "        metrics[\"accuracy_metric\"].update(correct, n=total, raw=True)\n",
    "\n",
    "        return {\n",
    "            \"total_loss\": total_loss.item(),\n",
    "            \"ce_loss\": loss_ce.item(),\n",
    "            \"ce_accuracy\": correct / target_remapped.size(0),\n",
    "            \"kl_loss\": kl_loss.item(),\n",
    "        }\n",
    "\n",
    "    def debug_metrics_to_pbar_args(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Prepares debug metrics for visualization in progress bars or logs.\n",
    "\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Dictionary of debug metrics from the current step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Same metrics for direct display.\n",
    "        \"\"\"\n",
    "        return debug_metrics\n",
    "\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Returns the average values of tracked metrics after a training step.\n",
    "\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Metric dictionary to extract averages from.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Averages of total loss, accuracy, CE loss, and KL loss.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            metrics[\"total_loss_metric\"].avg,\n",
    "            metrics[\"accuracy_metric\"].avg,\n",
    "            metrics[\"ce_loss_metric\"].avg,\n",
    "            metrics[\"kl_loss_metric\"].avg,\n",
    "        ]\n",
    "\n",
    "    def update_lambda_kl(self, lambda_kl):\n",
    "        self.lambda_kl = lambda_kl\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73f61b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/training_methods/KLCoCoOpV2.py\n",
    "\"\"\"\n",
    "This module defines the KLCoCoOp training method, which combines standard cross-entropy loss with KL divergence\n",
    "between a model's predictions and a frozen teacher (e.g., CLIP) to enhance generalization to novel classes.\n",
    "\"\"\"\n",
    "import random\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# from training_systems.core import DoubleDatasetTrainingMethod\n",
    "\n",
    "# from utils import AverageMeter,CLASS_NAMES, ContiguousLabelDataset, get_kl_loss\n",
    "\n",
    "\n",
    "class KLCoCoOpV2(DoubleDatasetTrainingMethod):\n",
    "    \"\"\"\n",
    "    KLCoCoOp training method combining cross-entropy classification on pseudo-base samples and KL divergence\n",
    "    loss on pseudo-novel samples. It encourages transferability and generalization by mixing base and novel categories.\n",
    "\n",
    "    Attributes:\n",
    "        lambda_kl (float): Weight for the KL divergence loss component.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: Any,\n",
    "            optimizer: Any,\n",
    "            lambda_kl,\n",
    "            debug: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(model, optimizer, \"Base CoCoOp + KL\", debug)\n",
    "        self.lambda_kl = lambda_kl\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] Initialized with lambda_kl={self.lambda_kl}, device={self.device}\")\n",
    "\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Initializes training metrics including total, cross-entropy, KL divergence losses, and classification accuracy.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: Dictionary of metric names mapped to their respective AverageMeter instances.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(\"[KLCoCoOpV2] Initializing metrics.\")\n",
    "        return {\n",
    "            \"ce_loss_metric\": AverageMeter(),   \n",
    "            \"kl_loss_metric\": AverageMeter(),\n",
    "            \"ce_accuracy_metric\": AverageMeter(),\n",
    "        }\n",
    "\n",
    "    def get_data_loader1(self, pseudo_base: ContiguousLabelDataset, batch_size: int) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns a DataLoader that splits each batch into pseudo-base and pseudo-novel subsets.\n",
    "\n",
    "        Args:\n",
    "            dataset (ContiguousLabelDataset): The dataset used for training.\n",
    "            batch_size (int): Number of samples per batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: PyTorch DataLoader with a custom collate function to separate base and novel samples.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] Creating DataLoader1 for pseudo_base with batch_size={batch_size}.\")\n",
    "        return DataLoader(\n",
    "            pseudo_base,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "        )\n",
    "\n",
    "    def get_data_loader2(self, pseudo_novel_dataset: ContiguousLabelDataset, batch_size: int) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Returns a DataLoader that splits each batch into pseudo-base and pseudo-novel subsets.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] Creating DataLoader2 for pseudo_novel with batch_size={batch_size}.\")\n",
    "        return DataLoader(pseudo_novel_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    \n",
    "    def forward_backward1(\n",
    "            self,\n",
    "            sample,\n",
    "            batch_idx,\n",
    "            metrics: Dict[str, AverageMeter],\n",
    "            dataset: ContiguousLabelDataset,\n",
    "            classes: list[int]\n",
    "    ) -> Dict[str, float]:\n",
    "        if self.debug and batch_idx < 3:\n",
    "            print(f\"[KLCoCoOpV2] forward_backward1: batch_idx={batch_idx}\")\n",
    "            print(f\"[KLCoCoOpV2] Sample type: {type(sample)}\")\n",
    "        # Load data into GPU\n",
    "        inputs, targets = sample\n",
    "        # === Pseudo-base: cross-entropy ===\n",
    "        inputs_base = inputs.to(self.device)\n",
    "        targets_base = targets.to(self.device)\n",
    "        # Use remapped class names to match the remapped labels\n",
    "        # classes contains original category indices, but we need to map them to 0-based indices\n",
    "        # The ContiguousLabelDataset remaps labels to 0-based indices, so we need class names in the same order\n",
    "        # Use the remapped indices to get class names in the correct order\n",
    "        remapped_class_names = [ CLASS_NAMES[ dataset.idx2cat[i] ] for i in range(len(dataset.idx2cat)) ]\n",
    "        assert set(classes) == set(dataset.idx2cat.values()), \\\n",
    "            f\"Split classes ({classes}) != dataset labels ({list(dataset.idx2cat.values())})\"\n",
    "        with self.model.temporary_classnames(remapped_class_names):\n",
    "            logits_base, loss_ce = self.model(inputs_base, targets_base)\n",
    "            # === Combine losses ===\n",
    "            if self.debug and batch_idx < 3:\n",
    "                print(f\"[KLCoCoOpV2] LOGITS shape: {logits_base.shape}, TARGETS shape: {targets_base.shape}\")\n",
    "                print(f\"[KLCoCoOpV2] CE loss: {loss_ce.item()}\")\n",
    "                print(f\"[KLCoCoOpV2] Remapped class names: {remapped_class_names}\")\n",
    "                print(f\"[KLCoCoOpV2] Original classes: {classes}\")\n",
    "                print(f\"[KLCoCoOpV2] Targets: {targets_base}\")\n",
    "                print(f\"[KLCoCoOpV2] Logits max: {logits_base.max(dim=1)[1]}\")\n",
    "                print(f\"[KLCoCoOpV2] Dataset cat2idx: {dataset.cat2idx}\")\n",
    "                print(f\"[KLCoCoOpV2] Dataset idx2cat: {dataset.idx2cat}\")\n",
    "                print(f\"[KLCoCoOpV2] Expected class names order: {[CLASS_NAMES[dataset.idx2cat[i]] for i in range(len(classes))]}\")\n",
    "                print(f\"[KLCoCoOpV2] Model class names: {[CLASS_NAMES[c] for c in classes]}\")\n",
    "            loss_ce.backward()\n",
    "\n",
    "            self.optimizer_step()\n",
    "            batch_size_total = inputs_base.size(0)\n",
    "\n",
    "            metrics[\"ce_loss_metric\"].update(loss_ce.item(), n=batch_size_total)\n",
    "\n",
    "            _, predicted = logits_base.max(dim=1)\n",
    "            correct = (predicted == targets_base).sum().item()\n",
    "            total = targets_base.size(0)\n",
    "            metrics[\"ce_accuracy_metric\"].update(correct, n=total, raw=True)\n",
    "\n",
    "            if self.debug and batch_idx < 3:\n",
    "                print(f\"[KLCoCoOpV2] Batch accuracy: {correct}/{total} = {correct/total if total > 0 else 0}\")\n",
    "        return {\n",
    "            \"ce_loss\": loss_ce.item(),\n",
    "            \"accuracy\": correct / targets_base.size(0),\n",
    "        }\n",
    "\n",
    "    def forward_backward2(\n",
    "            self,\n",
    "            sample,\n",
    "            batch_idx,\n",
    "            metrics: Dict[str, AverageMeter],\n",
    "            dataset: ContiguousLabelDataset,\n",
    "            classes: list[int]\n",
    "    ) -> Dict[str, float]:\n",
    "        # Use remapped class names to match the remapped labels\n",
    "        # classes contains original category indices, but we need to map them to 0-based indices\n",
    "        # The ContiguousLabelDataset remaps labels to 0-based indices, so we need class names in the same order\n",
    "        # Use the remapped indices to get class names in the correct order\n",
    "        remapped_class_names = [ CLASS_NAMES[ dataset.idx2cat[i] ] for i in range(len(dataset.idx2cat)) ]\n",
    "        pseudo_novel_class_names = remapped_class_names\n",
    "        if self.debug and batch_idx < 3:\n",
    "            print(f\"[KLCoCoOpV2] forward_backward2: batch_idx={batch_idx}\")\n",
    "            print(f\"[KLCoCoOpV2] Sample type: {type(sample)}; Sample len: {len(sample) if hasattr(sample, '__len__') else 'N/A'}\")\n",
    "        # Load data into GPU\n",
    "        inputs_novel, targets_novel = sample\n",
    "        # === Pseudo-novel: KL divergence with frozen CLIP ===\n",
    "        inputs_novel = inputs_novel.to(self.device)\n",
    "        targets_novel = targets_novel.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features_clip = self.model.clip_model.encode_image(inputs_novel)\n",
    "            image_features_clip = image_features_clip / image_features_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            #category_idxs = [dataset.idx2cat[c.item()] for c in list(set(targets_novel))] # type: ignore\n",
    "\n",
    "            text_inputs = clip.tokenize(\n",
    "                [f\"a photo of a {cn}, a type of flower.\" for cn in pseudo_novel_class_names]\n",
    "            ).to(self.device)\n",
    "\n",
    "            text_features_clip = self.model.clip_model.encode_text(text_inputs)\n",
    "            text_features_clip = text_features_clip / text_features_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            clip_logits = image_features_clip @ text_features_clip.T\n",
    "\n",
    "\n",
    "        self.model.train()\n",
    "        with self.model.temporary_classnames(remapped_class_names):\n",
    "            student_logits, student_loss = self.model(inputs_novel, targets_novel) \n",
    "            kl_loss = torch.nn.functional.kl_div(\n",
    "                torch.nn.functional.log_softmax(student_logits, dim=-1),\n",
    "                torch.nn.functional.softmax(clip_logits, dim=-1),\n",
    "                reduction=\"batchmean\"\n",
    "            )\n",
    "\n",
    "        # === Combine losses ===\n",
    "            kl_loss = self.lambda_kl * kl_loss\n",
    "\n",
    "            if self.debug and batch_idx < 3:\n",
    "                print(f\"[KLCoCoOpV2] KL loss (weighted): {kl_loss.item()}\")\n",
    "            kl_loss.backward()\n",
    "\n",
    "            self.optimizer_step()\n",
    "\n",
    "            metrics[\"kl_loss_metric\"].update(kl_loss.item(), n=inputs_novel.size(0))\n",
    "\n",
    "        return {\n",
    "            \"kl_loss\": kl_loss.item(),\n",
    "        }\n",
    "\n",
    "    def debug_metrics_to_pbar_args1(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Prepares debug metrics for visualization in progress bars or logs.\n",
    "\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Dictionary of debug metrics from the current step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Same metrics for direct display.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] debug_metrics_to_pbar_args1: {debug_metrics}\")\n",
    "        return debug_metrics\n",
    "\n",
    "    def debug_metrics_to_pbar_args2(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Prepares debug metrics for visualization in progress bars or logs.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] debug_metrics_to_pbar_args2: {debug_metrics}\")\n",
    "        return debug_metrics\n",
    "\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Returns the average values of tracked metrics after a training step.\n",
    "\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Metric dictionary to extract averages from.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Averages of total loss, accuracy, CE loss, and KL loss.\n",
    "        \"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"[KLCoCoOpV2] training_step_return: KL={metrics['kl_loss_metric'].avg}, CE={metrics['ce_loss_metric'].avg}, Acc={metrics['ce_accuracy_metric'].avg}\")\n",
    "        return [\n",
    "            metrics[\"kl_loss_metric\"].avg,\n",
    "            metrics[\"ce_loss_metric\"].avg,\n",
    "            metrics[\"ce_accuracy_metric\"].avg,\n",
    "        ]\n",
    "\n",
    "    def update_lambda_kl(self, lambda_kl):\n",
    "        \"\"\"\n",
    "        Update the lambda_kl value used for KL loss weighting.\n",
    "        \n",
    "        Args:\n",
    "            lambda_kl (float): New lambda_kl value.\n",
    "        \"\"\"\n",
    "        self.lambda_kl = lambda_kl\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec9fd3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/training_methods/OnlyMLP.py\n",
    "\"\"\"\n",
    "This module implements the Adversarial training method, which incorporates a gradient reversal layer\n",
    "and an adversarial MLP to encourage domain-invariant feature learning.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from training_systems.core import TrainingMethod\n",
    "# from utils import AverageMeter, ContiguousLabelDataset\n",
    "# from model.cocoop.mlp_adversary import AdversarialMLP, GradientReversalLayer\n",
    "\n",
    "\n",
    "class OnlyMLP(TrainingMethod):\n",
    "    \"\"\"\n",
    "    Adversarial training method using a Gradient Reversal Layer and an MLP adversary.\n",
    "\n",
    "    Attributes:\n",
    "        cls_cluster_dict (Dict[int, Any]): Maps class indices to cluster labels.\n",
    "        grl (GradientReversalLayer): The gradient reversal layer instance.\n",
    "        mlp_adversary (AdversarialMLP): The adversarial MLP used to confuse cluster prediction.\n",
    "        lambda_adv (float): Weight of the adversarial loss term.\n",
    "        debug (bool): If True, print debug information.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: Any,\n",
    "            optimizer: Any,\n",
    "            cls_cluster_dict: Dict[int, Any],\n",
    "            mlp_adversary: AdversarialMLP,\n",
    "            debug: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (Any): The main model being trained.\n",
    "            optimizer (Any): Optimizer for updating model parameters.\n",
    "            cls_cluster_dict (Dict[int, Any]): Mapping from class labels to clusters.\n",
    "            grl (GradientReversalLayer): The gradient reversal layer.\n",
    "            mlp_adversary (AdversarialMLP): The adversarial network module.\n",
    "            lambda_adv (float): Weight for the adversarial loss.\n",
    "            debug (bool, optional): Enables debug mode. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__(model, optimizer, \"Adv.\", debug)\n",
    "        self.cls_cluster_dict = cls_cluster_dict\n",
    "        self.mlp_adversary = mlp_adversary\n",
    "\n",
    "    def get_metrics(self) -> Dict[str, AverageMeter]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            Dict[str, AverageMeter]: Dictionary containing initialized metrics for training.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"adv_loss_metric\": AverageMeter(),\n",
    "            \"accuracy_metric\": AverageMeter(),\n",
    "        }\n",
    "\n",
    "    def get_data_loader(self, dataset: ContiguousLabelDataset, batch_size: int) -> DataLoader:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset (ContiguousLabelDataset): Dataset to be used.\n",
    "            batch_size (int): Size of each batch.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: Configured PyTorch DataLoader instance.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "        )\n",
    "\n",
    "    def forward_backward(\n",
    "            self,\n",
    "            sample,\n",
    "            batch_idx,\n",
    "            metrics: Dict[str, AverageMeter],\n",
    "            dataset: ContiguousLabelDataset,\n",
    "            accumulation_steps: int = 1,\n",
    "            step: int = 0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Executes the forward and backward pass.\n",
    "\n",
    "        Args:\n",
    "            sample (Tuple[Tensor, Tensor]): Batch of input data and targets.\n",
    "            batch_idx (int): Index of the current batch.\n",
    "            metrics (Dict[str, AverageMeter]): Dictionary of metrics to be updated.\n",
    "            dataset (ContiguousLabelDataset): Dataset for cluster label lookup.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Dictionary with loss and accuracy metrics.\n",
    "        \"\"\"\n",
    "        # Load data into GPU\n",
    "        inputs, targets = sample\n",
    "        inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "\n",
    "        targets_real_category = [dataset.idx2cat[c.item()] for c in targets]\n",
    "        cluster_target = [int(self.cls_cluster_dict[int(tl)]) for tl in targets_real_category]\n",
    "        cluster_target = torch.tensor(\n",
    "            cluster_target,\n",
    "            device=targets.device,\n",
    "            dtype=torch.float32\n",
    "        )\n",
    "\n",
    "        # Forward pass + loss computation dsada\n",
    "        logits, ce_loss, img_features, ctx, bias = self.model(inputs, targets, get_image_features=True)\n",
    "        ctx = ctx.detach()  # Detach context to avoid backprop through it\n",
    "        ctx_shifted = ctx + bias.unsqueeze(1).detach()  # Add bias to context tokens\n",
    "        ctx_flat = ctx_shifted.reshape(ctx_shifted.size(0), -1).to(dtype=torch.float32)\n",
    "\n",
    "        cluster_logits = self.mlp_adversary(ctx_flat).squeeze()\n",
    "\n",
    "        loss_bce = F.binary_cross_entropy_with_logits(cluster_logits, cluster_target)\n",
    "\n",
    "        loss_bce = loss_bce / accumulation_steps\n",
    "        # Backward pass\n",
    "        loss_bce.backward()\n",
    "\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                list(self.model.parameters()) + list(self.mlp_adversary.parameters()),\n",
    "                max_norm=1.0,\n",
    "                norm_type=2.0,\n",
    "                error_if_nonfinite=True\n",
    "            )\n",
    "\n",
    "            self.optimizer_step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # Fetch prediction and loss value\n",
    "        batch_size = inputs.shape[0]\n",
    "        metrics[\"adv_loss_metric\"].update(loss_bce.item(), n=batch_size)\n",
    "\n",
    "        predicted = cluster_logits.gt(0.5).float()  # Convert logits to binary predictions\n",
    "        correct = predicted.eq(cluster_target).sum().item()\n",
    "        # Compute training accuracy\n",
    "        metrics[\"accuracy_metric\"].update(correct, n=batch_size, raw=True)\n",
    "\n",
    "        return {\n",
    "            \"adv_loss\": loss_bce.item(),\n",
    "            \"accuracy\": correct / batch_size,\n",
    "        }\n",
    "\n",
    "    def debug_metrics_to_pbar_args(self, debug_metrics: Dict[str, float]) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            debug_metrics (Dict[str, float]): Metrics from current training step.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, float]: Same metrics, passed to progress bar.\n",
    "        \"\"\"\n",
    "        return debug_metrics\n",
    "\n",
    "    def training_step_return(self, metrics: Dict[str, AverageMeter]) -> list[float]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metrics (Dict[str, AverageMeter]): Collected training metrics.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Average values of total, accuracy, CE, and adversarial losses.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            metrics[\"accuracy_metric\"].avg,\n",
    "            metrics[\"adv_loss_metric\"].avg,\n",
    "        ]\n",
    "\n",
    "    def update_lambda_adv(self, lambda_adv) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lambda_adv (float): New value to set for lambda_adv.\n",
    "        \"\"\"\n",
    "        self.lambda_adv = lambda_adv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "289bd37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./training_systems/training_methods/__init__.py\n",
    "# from training_systems.training_methods.Adversarial import Adversarial\n",
    "# from training_systems.training_methods.KLCoCoOp import KLCoCoOp\n",
    "# from training_systems.training_methods.KLCoCoOpV2 import KLCoCoOpV2\n",
    "# from training_systems.training_methods.BaseCoCoOp import BaseCoCoOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11dd093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3787eb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e8ca117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./utils/__init__.py\n",
    "# from utils.clustering import conditional_clustering, random_clustering, rotating_cluster_generator_shift\n",
    "# from utils.datasets import (\n",
    "#     get_data,\n",
    "#     base_novel_categories,\n",
    "#     split_data,\n",
    "#     CLASS_NAMES,\n",
    "#     ContiguousLabelDataset,\n",
    "# )\n",
    "# from utils.tensor_board_logger import TensorboardLogger\n",
    "# from utils.kl import get_kl_loss\n",
    "# from utils.metrics import AverageMeter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98ed96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./utils/clustering.py\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "# from utils.datasets import get_data, base_novel_categories, split_data, CLASS_NAMES\n",
    "import clip\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter, deque\n",
    "\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "def create_cluster_from_ordered_list(ordered_categories, split_ratio):\n",
    "    \"\"\"\n",
    "    Converts a list of categories into a cluster dict using split_ratio.\n",
    "    \"\"\"\n",
    "    n = len(ordered_categories)\n",
    "    n_zeros = int(n * split_ratio)\n",
    "    return {\n",
    "        cat: 0 if i < n_zeros else 1\n",
    "        for i, cat in enumerate(ordered_categories)\n",
    "    }\n",
    "\n",
    "def rotating_cluster_generator_shift(categories, split_ratio, steps=1, seed=None):\n",
    "    \"\"\"\n",
    "    Yields clusters by cyclically rotating the category list.\n",
    "\n",
    "    Args:\n",
    "        categories (list): List of category identifiers.\n",
    "        split_ratio (float): Ratio of cluster 0 elements.\n",
    "        seed (int, optional): Random seed for reproducibility.\n",
    "\n",
    "    Yields:\n",
    "        dict: Cluster mapping.\n",
    "    \"\"\"\n",
    "    cat_list = list(categories)\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    random.shuffle(cat_list)\n",
    "    cat_deque = deque(cat_list)\n",
    "\n",
    "    while True:\n",
    "        cluster = create_cluster_from_ordered_list(cat_deque, split_ratio)\n",
    "        cat0 = [cat for cat, c in cluster.items() if c == 0]\n",
    "        cat1 = [cat for cat, c in cluster.items() if c == 1]\n",
    "        yield cluster, cat0, cat1\n",
    "        cat_deque.rotate(-steps)  # rotate left\n",
    "\n",
    "def cluster_categories(device, cnn, n_clusters=2, variance=0.95, data_dir=\"../data\"):\n",
    "    \"\"\"\n",
    "    Clusters base classes using visual features extracted from a CLIP model. Applies PCA to reduce dimensionality\n",
    "    and Agglomerative Clustering on cosine distances to group the categories.\n",
    "\n",
    "    Args:\n",
    "        device (torch.device): The device to run computations on (CPU/GPU).\n",
    "        cnn (str): CLIP model architecture name (e.g., \"ViT-B/32\").\n",
    "        n_clusters (int): Number of clusters to generate.\n",
    "        variance (float): Variance ratio to preserve during PCA.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[int, int], Dict[str, int]]: Two dictionaries mapping class indices and class names to cluster IDs.\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize clip model with ViT\n",
    "    clip_model, _ = clip.load(cnn)\n",
    "    clip_model = clip_model.to(device)\n",
    "    resolution = clip_model.visual.input_resolution\n",
    "    train_set, _, _ = get_data(data_dir=data_dir, resolution=resolution)\n",
    "\n",
    "    # split classes into base and novel\n",
    "    base_classes, _ = base_novel_categories(train_set)\n",
    "\n",
    "    # split the three datasets\n",
    "    train_base, _ = split_data(train_set, base_classes)\n",
    "\n",
    "    class_feature = {}\n",
    "    with torch.no_grad():\n",
    "        for c in tqdm(base_classes, desc=\"Processing classes\"):\n",
    "            imgs_c = []\n",
    "            # Create a DataLoader to iterate over the dataset properly\n",
    "            dataloader = torch.utils.data.DataLoader(train_base, batch_size=1, shuffle=False)\n",
    "            for img, label in dataloader:\n",
    "                if label.item() == c:\n",
    "                    imgs_c.append(img.squeeze(0))\n",
    "            features = [\n",
    "                clip_model.encode_image(img.unsqueeze(0).to(device)).cpu().numpy()\n",
    "                for img in imgs_c\n",
    "            ]\n",
    "            class_feature[c] = np.mean(features, axis=0)\n",
    "\n",
    "    # class_ft_array = np.array([class_feature[c][0] for c in base_classes])\n",
    "\n",
    "    cat2idx = {}\n",
    "    idx2cat = {}\n",
    "    class_ft_array = []\n",
    "    for i, c in enumerate(base_classes):\n",
    "        cat2idx[c] = i\n",
    "        idx2cat[i] = c\n",
    "        class_ft_array.append(class_feature[c][0])\n",
    "\n",
    "    pca = PCA(n_components=variance)\n",
    "    X_reduced = pca.fit_transform(class_ft_array)\n",
    "\n",
    "    print(\n",
    "        f\"Reduced feature shape: {X_reduced.shape}, Variance explained: {pca.explained_variance_ratio_.sum()}\"\n",
    "    )\n",
    "\n",
    "    cosine_dist = cosine_distances(X_reduced)\n",
    "    # Step 5: Agglomerative clustering\n",
    "    agglo = AgglomerativeClustering(\n",
    "        n_clusters=n_clusters, metric=\"precomputed\", linkage=\"average\"\n",
    "    )\n",
    "    cluster_labels = agglo.fit_predict(cosine_dist)\n",
    "\n",
    "    cluster_labels = {idx2cat[i]: cluster for i, cluster in enumerate(cluster_labels)}\n",
    "\n",
    "    cluster_labels_text = {\n",
    "        CLASS_NAMES[base_class]: int(cluster)\n",
    "        for base_class, cluster in enumerate(cluster_labels)\n",
    "    }\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return cluster_labels, cluster_labels_text\n",
    "\n",
    "\n",
    "def random_clustering(\n",
    "    n_cluster,\n",
    "    seed=42,\n",
    "    data_dir=\"../data\",\n",
    "    distribution=\"uniform\",\n",
    "    split_ratio=0.7,  # Only for bipartite\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates random cluster assignments for a given number of clusters.\n",
    "\n",
    "    Args:\n",
    "        n_cluster (int): Number of clusters to generate.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        data_dir (str): Directory where the dataset is stored.\n",
    "        distribution (str): Distribution type for cluster assignment. Options are \"uniform\", \"random\", \"sequential\", or \"bipartite\".\n",
    "        split_ratio (float): Percentage of classes in the larger cluster (only used for \"bipartite\").\n",
    "\n",
    "    Returns:\n",
    "        Tuple[\n",
    "            Dict[int, int],        # class_id -> cluster_id\n",
    "            Dict[str, int],        # class_name -> cluster_id\n",
    "            List[int],             # class_ids in cluster 0\n",
    "            List[int],             # class_ids in cluster 1\n",
    "        ]\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    train_set, _, _ = get_data(data_dir=data_dir)\n",
    "    base_classes, _ = base_novel_categories(train_set)\n",
    "\n",
    "    cluster_labels = {}\n",
    "\n",
    "    if distribution == \"uniform\":\n",
    "        shuffled = np.random.permutation(base_classes)\n",
    "        for i, cls in enumerate(shuffled):\n",
    "            cluster_id = i % n_cluster\n",
    "            cluster_labels[cls] = cluster_id\n",
    "\n",
    "    elif distribution == \"random\":\n",
    "        for cls in base_classes:\n",
    "            cluster_id = np.random.choice(range(n_cluster))\n",
    "            cluster_labels[cls] = cluster_id\n",
    "\n",
    "    elif distribution == \"sequential\":\n",
    "        for i, cls in enumerate(base_classes):\n",
    "            cluster_id = i % n_cluster\n",
    "            cluster_labels[cls] = cluster_id\n",
    "\n",
    "    cluster_labels_text = {\n",
    "        CLASS_NAMES[cls]: int(cluster_labels[cls])\n",
    "        for cls in cluster_labels\n",
    "    }\n",
    "\n",
    "    cluster_dict_int = {int(k): v for k, v in cluster_labels.items()}\n",
    "\n",
    "    return cluster_dict_int, cluster_labels_text\n",
    "\n",
    "\n",
    "def conditional_clustering(n_cluster, variance, cnn, device, data_dir=\"../data\"):\n",
    "    \"\"\"\n",
    "    Loads existing cluster labels from disk if available, otherwise computes and saves new cluster assignments.\n",
    "\n",
    "    Args:\n",
    "        n_cluster (int): Number of clusters to generate.\n",
    "        variance (float): Variance ratio to preserve during PCA.\n",
    "        cnn (str): CLIP model architecture name (used for naming output files).\n",
    "        device (torch.device): The device to run computations on.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[int, int], Dict[str, int]]: Dictionaries for integer-labeled and text-labeled cluster assignments.\n",
    "    \"\"\"\n",
    "    cnn_sanitized = cnn.replace(\"/\", \"_\")\n",
    "    save_dir = f\"clustering_split/cluster_labels_{n_cluster}_{variance}_{cnn_sanitized}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    int_categories_path = os.path.join(save_dir, \"int_categories.pkl\")\n",
    "    text_categories_path = os.path.join(save_dir, \"text_categories.pkl\")\n",
    "\n",
    "\n",
    "    # Commented this lines because they are not used in the Jupyter version, they are meant to save and load cluster labels\n",
    "\n",
    "    # if os.path.exists(int_categories_path) and os.path.exists(text_categories_path):\n",
    "    #     print(\"🟩 CLUSTERS FILES FOUND. Loading existing cluster labels...\")\n",
    "    #     with open(int_categories_path, \"rb\") as f:\n",
    "    #         cluster_labels = pickle.load(f)\n",
    "    #         cluster_dict_int = {int(k): v for k, v in cluster_labels.items()}\n",
    "    #     with open(text_categories_path, \"rb\") as f:\n",
    "    #         cluster_labels_text = pickle.load(f)\n",
    "\n",
    "    # else:\n",
    "    print(\"🟧 NO CLUSTERS FILES FOUND (jupyter version without saves). Loading existing cluster labels...\")\n",
    "    # cluster the base classes\n",
    "    cluster_labels, cluster_labels_text = cluster_categories(\n",
    "        device, n_clusters=n_cluster, variance=variance, cnn=cnn, data_dir=data_dir\n",
    "    )\n",
    "    cluster_dict_int = {int(k): v for k, v in cluster_labels.items()}\n",
    "    # with open(int_categories_path, \"wb\") as f:\n",
    "    #     pickle.dump(cluster_labels, f)\n",
    "    # with open(text_categories_path, \"wb\") as f:\n",
    "    #     pickle.dump(cluster_labels_text, f)\n",
    "    # Count samples in each cluster\n",
    "    cluster_counts = Counter(cluster_dict_int.values())\n",
    "    for cluster_id in range(n_cluster):\n",
    "        print(f\"Cluster {cluster_id} count: {cluster_counts.get(cluster_id, 0)}\")\n",
    "\n",
    "    return cluster_dict_int, cluster_labels_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1fd4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./utils/datasets.py\n",
    "import clip\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Subset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
    "\n",
    "# --- Data Augmentation Pipeline ---\n",
    "def build_default_transform(resolution=224):\n",
    "    \"\"\"\n",
    "    Builds the default data augmentation pipeline as specified:\n",
    "    - RandomResizedCrop with bicubic interpolation\n",
    "    - RandomHorizontalFlip\n",
    "    - Normalize with given mean and std\n",
    "    \"\"\"\n",
    "    return T.Compose([\n",
    "        T.RandomResizedCrop(resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "    ])\n",
    "\n",
    "\n",
    "def build_eval_transform(resolution=224):\n",
    "    \"\"\"\n",
    "    Builds the evaluation transform pipeline (no random augmentations):\n",
    "    - Resize to resolution\n",
    "    - CenterCrop to resolution\n",
    "    - Normalize with given mean and std\n",
    "    \"\"\"\n",
    "    return T.Compose([\n",
    "        T.Resize(resolution, interpolation=T.InterpolationMode.BICUBIC),\n",
    "        T.CenterCrop(resolution),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_data(data_dir=\"./data\", train_transform=None, eval_transform=None, resolution=224):\n",
    "    \"\"\"\n",
    "    Loads the Flowers102 dataset from torchvision, returning separate splits for training, validation, and testing.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be downloaded/stored. Defaults to \"./data\".\n",
    "        train_transform (torchvision.transforms.Compose or None): Transformations to apply to training data.\n",
    "        eval_transform (torchvision.transforms.Compose or None): Transformations to apply to validation/test data.\n",
    "        resolution (int): Image resolution for transforms (default 224).\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple (train, val, test) of Flowers102 dataset splits.\n",
    "    \"\"\"\n",
    "    if train_transform is None:\n",
    "        train_transform = build_default_transform(resolution)\n",
    "    if eval_transform is None:\n",
    "        eval_transform = build_eval_transform(resolution)\n",
    "        \n",
    "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=train_transform)\n",
    "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=eval_transform)\n",
    "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=eval_transform)\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def base_novel_categories(dataset):\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes//2]\n",
    "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "\n",
    "def get_labels(dataset):\n",
    "    \"\"\"\n",
    "    Recursively retrieve labels from dataset or nested Subset.\n",
    "    Assumes the base dataset has a `_labels` attribute.\n",
    "    \"\"\"\n",
    "    if hasattr(dataset, '_labels'):\n",
    "        return dataset._labels\n",
    "    elif isinstance(dataset, Subset):\n",
    "        parent_labels = get_labels(dataset.dataset)\n",
    "        return [parent_labels[i] for i in dataset.indices]\n",
    "    else:\n",
    "        raise AttributeError(\"Dataset does not have _labels or is not a Subset of a dataset with _labels.\")\n",
    "\n",
    "def split_data(dataset, base_classes):\n",
    "    \"\"\"\n",
    "    Splits the dataset into base and novel subsets based on base_classes.\n",
    "    Works even if the input dataset is already a Subset.\n",
    "\n",
    "    Args:\n",
    "        dataset: PyTorch Dataset or Subset\n",
    "        base_classes (List[int]): List of class indices considered as base.\n",
    "\n",
    "    Returns:\n",
    "        base_dataset (Subset): Subset containing samples from base classes.\n",
    "        novel_dataset (Subset): Subset containing samples from novel classes.\n",
    "    \"\"\"\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    labels = get_labels(dataset)\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    for sample_id, label in enumerate(labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    base_dataset = Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = Subset(dataset, novel_categories_samples)\n",
    "\n",
    "    return base_dataset, novel_dataset\n",
    "\n",
    "\n",
    "class ContiguousLabelDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset wrapper that remaps arbitrary class labels to contiguous integers starting from 0.\n",
    "\n",
    "    This is useful for classification tasks where models expect class indices to be in a 0-based contiguous range.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (Dataset): The original dataset to wrap.\n",
    "        cat2idx (Dict[Any, int]): Mapping from original class labels to contiguous integer indices.\n",
    "        idx2cat (Dict[int, Any]): Reverse mapping from indices back to original class labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, class_order: list[int]):\n",
    "        self.dataset = dataset\n",
    "        # force the mapping to use your custom order\n",
    "        self.cat2idx = { cat: idx for idx, cat in enumerate(class_order) }\n",
    "        self.idx2cat = { idx: cat for cat, idx in self.cat2idx.items() }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.dataset[index]\n",
    "        mapped_label = self.cat2idx[label]\n",
    "        return image, mapped_label\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load model\n",
    "#     clip_model, preprocess = clip.load(\"ViT-B/32\", device=\"mps\")\n",
    "#     clip_model = clip_model.to(\"mps\")\n",
    "\n",
    "#     train_set, val_set, test_set = get_data(train_transform=preprocess, eval_transform=preprocess, data_dir=\"../data\")\n",
    "#     base_classes, novel_classes = base_novel_categories(train_set)\n",
    "#     print(len(base_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6395a7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./utils/debugging.py\n",
    "import torch\n",
    "\n",
    "\n",
    "def check_gradients(model):\n",
    "    print(\"=== Checking gradients ===\")\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            if param.grad is None:\n",
    "                print(f\"{name}: ❌ No gradient\")\n",
    "            elif torch.all(param.grad == 0):\n",
    "                print(f\"{name}: ⚠️ Zero gradient\")\n",
    "            else:\n",
    "                print(f\"{name}: ✅ Gradient flowing\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61f1a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./utils/kl.py\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "# from utils.datasets import CLASS_NAMES\n",
    "\n",
    "\n",
    "def get_kl_loss(device, inputs_novel, model, targets_novel, tmp_dataset):\n",
    "    \"\"\"\n",
    "    Computes the KL divergence between the student model's predictions and the CLIP model's predictions\n",
    "    for a batch of novel class images.\n",
    "\n",
    "    Args:\n",
    "        device (torch.device): The device (CPU or CUDA) to perform computation on.\n",
    "        inputs_novel (Tensor): A batch of input images from novel classes.\n",
    "        model (nn.Module): The student model that includes a CLIP backbone and prompt learner.\n",
    "        targets_novel (List[int]): Target labels corresponding to the novel class inputs.\n",
    "        tmp_dataset (ContiguousLabelDataset): Dataset wrapper with label-to-category mappings.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: A scalar tensor representing the KL divergence loss.\n",
    "    \"\"\"\n",
    "    targets_novel_tensor = torch.tensor(targets_novel).to(device) if isinstance(targets_novel, list) else targets_novel\n",
    "    categories_novel_tensor = [tmp_dataset.idx2cat[c.item()] for c in list(set(targets_novel_tensor))]\n",
    "    # print(f\"input novel shape: {inputs_novel.shape} novel base: {targets_novel_tensor.shape}\")\n",
    "    with torch.no_grad():\n",
    "        image_features_clip = model.clip_model.encode_image(inputs_novel)\n",
    "        image_features_clip = image_features_clip / image_features_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "        text_inputs = clip.tokenize(\n",
    "            [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories_novel_tensor]\n",
    "        ).to(device)\n",
    "\n",
    "        text_features_clip = model.clip_model.encode_text(text_inputs)\n",
    "        text_features_clip = text_features_clip / text_features_clip.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        clip_logits = image_features_clip @ text_features_clip.T\n",
    "\n",
    "    remapped_class_names = [ CLASS_NAMES[ c ] for c in categories_novel_tensor ]\n",
    "    \n",
    "    # targets_novel_tensor contains contiguous indices (0, 1, 2, ...)\n",
    "    # categories_novel_tensor should be the original class labels for the current batch\n",
    "    \n",
    "    # No need to remap targets; they are already correct\n",
    "    target_remapping = {cat:idx for idx, cat in enumerate(categories_novel_tensor)}\n",
    "    target_original = [tmp_dataset.idx2cat[c.item()] for c in targets_novel_tensor]\n",
    "    target_remapped = torch.tensor([target_remapping[c] for c in target_original]).to(device)\n",
    "    \n",
    "    with model.temporary_classnames(remapped_class_names):\n",
    "        model.train()\n",
    "        student_logits, student_loss = model(inputs_novel, target_remapped)  # [B, num_classes]\n",
    "        kl_loss = torch.nn.functional.kl_div(\n",
    "            torch.nn.functional.log_softmax(student_logits, dim=-1),\n",
    "            torch.nn.functional.softmax(clip_logits, dim=-1),\n",
    "            reduction=\"batchmean\"\n",
    "        )\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51f8b05e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_1 0.4000 (0.4500) loss_2 0.8000 (0.8000)\n",
      "loss_1 0.6000 (0.5000) loss_2 0.7000 (0.7500)\n"
     ]
    }
   ],
   "source": [
    "# From: ./utils/metrics.py\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Compute and store the average and current value.\n",
    "\n",
    "    Examples::\n",
    "        >>> # 1. Initialize a meter to record loss\n",
    "        >>> losses = AverageMeter()\n",
    "        >>> # 2. Update meter after every mini-batch update\n",
    "        >>> losses.update(loss_value, batch_size)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ema=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ema (bool, optional): apply exponential moving average.\n",
    "        \"\"\"\n",
    "        self.ema = ema\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1, raw=False):\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            val = val.item()\n",
    "\n",
    "        self.val = val\n",
    "        if raw:\n",
    "            self.sum += val\n",
    "        else:\n",
    "            self.sum += val * n\n",
    "\n",
    "        self.count += n\n",
    "\n",
    "        if self.ema:\n",
    "            self.avg = self.avg * 0.9 + self.val * 0.1\n",
    "        else:\n",
    "            self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class MetricMeter:\n",
    "    \"\"\"Store the average and current value for a set of metrics.\n",
    "\n",
    "    Examples::\n",
    "        >>> # 1. Create an instance of MetricMeter\n",
    "        >>> metric = MetricMeter()\n",
    "        >>> # 2. Update using a dictionary as input\n",
    "        >>> input_dict = {'loss_1': value_1, 'loss_2': value_2}\n",
    "        >>> metric.update(input_dict)\n",
    "        >>> # 3. Convert to string and print\n",
    "        >>> print(str(metric))\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, delimiter=\" \"):\n",
    "        self.meters = defaultdict(AverageMeter)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, input_dict):\n",
    "        if input_dict is None:\n",
    "            return\n",
    "\n",
    "        if not isinstance(input_dict, dict):\n",
    "            raise TypeError(\n",
    "                \"Input to MetricMeter.update() must be a dictionary\"\n",
    "            )\n",
    "\n",
    "        for k, v in input_dict.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __str__(self):\n",
    "        output_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            output_str.append(f\"{name} {meter.val:.4f} ({meter.avg:.4f})\")\n",
    "        return self.delimiter.join(output_str)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    metric = MetricMeter()\n",
    "    metric.update({\"loss_1\": torch.tensor(0.5), \"loss_2\": torch.tensor(0.8)})\n",
    "    metric.update({\"loss_1\": torch.tensor(0.4)})\n",
    "    print(str(metric))\n",
    "    metric.update({\"loss_1\": torch.tensor(0.6), \"loss_2\": torch.tensor(0.7)})\n",
    "    print(str(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85a9ed9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./utils/tensor_board_logger.py\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "\n",
    "def harmonic_mean(a, b):\n",
    "    return 2 * (a * b) / (a + b)\n",
    "\n",
    "\n",
    "class CSVLogger:\n",
    "    \"\"\"\n",
    "    A simple logger that writes training metrics to a CSV file.\n",
    "\n",
    "    Attributes:\n",
    "        filename (str): The path to the output CSV file.\n",
    "        file (file object): Open file handle for writing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"\n",
    "        Initializes the CSV logger and creates the output directory and file.\n",
    "\n",
    "        Args:\n",
    "            filename (str): Path to the CSV file for logging.\n",
    "        \"\"\"\n",
    "        self.filename = filename\n",
    "        os.makedirs(\n",
    "            os.path.dirname(filename), exist_ok=True\n",
    "        )  # Create folder if it doesn't exist\n",
    "        self.file = open(filename, \"w\")\n",
    "        self.file.write(\"epoch,base_acc,novel_acc,harmonic_mean\\n\")\n",
    "\n",
    "    def log(self, epoch, base_acc, novel_acc):\n",
    "        \"\"\"\n",
    "        Logs a training epoch's results including harmonic mean to the CSV file.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): The epoch number.\n",
    "            base_acc (float): Accuracy on base classes.\n",
    "            novel_acc (float): Accuracy on novel classes.\n",
    "        \"\"\"\n",
    "        hm = harmonic_mean(base_acc, novel_acc)\n",
    "        self.file.write(f\"{epoch},{base_acc},{novel_acc},{hm}\\n\")\n",
    "        self.file.flush()\n",
    "        print(\n",
    "            f\"Logged: epoch {epoch}, base_acc {base_acc}, novel_acc {novel_acc}, harmonic_mean {hm}\"\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the CSV file.\n",
    "        \"\"\"\n",
    "        self.file.close()\n",
    "\n",
    "\n",
    "class BaseAndNovelMetrics:\n",
    "    \"\"\"\n",
    "    Tracks base and novel accuracy values and their harmonic mean across epochs.\n",
    "\n",
    "    Attributes:\n",
    "        tmp (List[Tuple[int, float, float, float]]): Logged metrics per epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.tmp = []\n",
    "\n",
    "    def update(self, epoch, base_acc, novel_acc):\n",
    "        \"\"\"\n",
    "        Records a new set of metrics for a specific epoch.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Epoch number.\n",
    "            base_acc (float): Accuracy on base classes.\n",
    "            novel_acc (float): Accuracy on novel classes.\n",
    "        \"\"\"\n",
    "        self.tmp.append(\n",
    "            (epoch, base_acc, novel_acc, harmonic_mean(base_acc, novel_acc))\n",
    "        )\n",
    "\n",
    "    def get_metrics(self):\n",
    "        \"\"\"\n",
    "        Retrieves the collected metrics.\n",
    "\n",
    "        Returns:\n",
    "            List[Tuple[int, float, float, float]] or None: Logged metrics or None if empty.\n",
    "        \"\"\"\n",
    "        if len(self.tmp) == 0:\n",
    "            return None\n",
    "        return self.tmp\n",
    "\n",
    "\n",
    "class TensorboardLogger:\n",
    "    \"\"\"\n",
    "    Handles logging of training and evaluation metrics to TensorBoard and CSV.\n",
    "\n",
    "    Attributes:\n",
    "        writer (SummaryWriter): TensorBoard writer instance.\n",
    "        csv_logger (CSVLogger): CSV logger instance for persistent metric storage.\n",
    "        hparams (dict): Hyperparameters dictionary.\n",
    "        base_and_novel_metrics (BaseAndNovelMetrics): Metric tracker.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, writer: SummaryWriter):\n",
    "        \"\"\"\n",
    "        Initializes the TensorboardLogger.\n",
    "\n",
    "        Args:\n",
    "            writer (SummaryWriter): TensorBoard writer instance.\n",
    "        \"\"\"\n",
    "        self.writer = writer\n",
    "        self.csv_logger = CSVLogger(f\"{writer.log_dir}/metrics.csv\")\n",
    "        self.hparams = None\n",
    "        self.base_and_novel_metrics = BaseAndNovelMetrics()\n",
    "\n",
    "    def log_hparams(self, hparams: dict):\n",
    "        \"\"\"\n",
    "        Stores hyperparameters for later logging.\n",
    "\n",
    "        Args:\n",
    "            hparams (dict): Hyperparameters dictionary.\n",
    "        \"\"\"\n",
    "        self.hparams = hparams\n",
    "\n",
    "    def log_training_base(self, epoch, lr, ce_loss, acc, kl_loss, total_loss):\n",
    "        \"\"\"\n",
    "        Logs training metrics for the base training phase.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Current epoch.\n",
    "            lr (float): Learning rate.\n",
    "            ce_loss (float): Cross entropy loss.\n",
    "            acc (float): Accuracy.\n",
    "            kl_loss (float or None): KL divergence loss, optional.\n",
    "            total_loss (float): Total loss.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(\"learning_rate\", lr, epoch)\n",
    "        self.writer.add_scalar(\"train_base/ce_loss\", ce_loss, epoch)\n",
    "        self.writer.add_scalar(\"train_base/ce_accuracy\", acc, epoch)\n",
    "        if kl_loss is not None:\n",
    "            self.writer.add_scalar(\"train_base/kl_loss\", kl_loss, epoch)\n",
    "        self.writer.add_scalar(\"train_base/total_loss\", total_loss, epoch)\n",
    "\n",
    "    def log_training_adv(\n",
    "        self, epoch, lambda_adv, ce_loss, acc, adv_loss, total_loss, kl_loss=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs training metrics for the adversarial training phase.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Current epoch.\n",
    "            lambda_adv (float): Adversarial loss weight.\n",
    "            ce_loss (float): Cross entropy loss.\n",
    "            acc (float): Accuracy.\n",
    "            adv_loss (float): Adversarial loss.\n",
    "            total_loss (float): Total loss.\n",
    "            kl_loss (float or None): KL divergence loss, optional.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(\"lambda_adv\", lambda_adv, epoch)\n",
    "        self.writer.add_scalar(\"train_adv/ce_loss\", ce_loss, epoch)\n",
    "        self.writer.add_scalar(\"train_adv/ce_accuracy\", acc, epoch)\n",
    "        self.writer.add_scalar(\"train_adv/mlp_loss\", adv_loss, epoch)\n",
    "        if kl_loss is not None:\n",
    "            self.writer.add_scalar(\"train_adv/kl_loss\", kl_loss, epoch)\n",
    "        self.writer.add_scalar(\"train_adv/total_loss\", total_loss, epoch)\n",
    "\n",
    "    def log_validation(\n",
    "        self, epoch, base_loss, base_acc, novel_loss, novel_acc, is_adv=False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Logs validation metrics.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Current epoch.\n",
    "            base_loss (float): Loss on base classes.\n",
    "            base_acc (float): Accuracy on base classes.\n",
    "            novel_loss (float): Loss on novel classes.\n",
    "            novel_acc (float): Accuracy on novel classes.\n",
    "            is_adv (bool): Whether validation is adversarial.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(f\"validation_base/loss\", base_loss, epoch)\n",
    "        self.writer.add_scalar(f\"validation_base/accuracy\", base_acc, epoch)\n",
    "        self.writer.add_scalar(f\"validation_novel/loss\", novel_loss, epoch)\n",
    "        self.writer.add_scalar(f\"validation_novel/accuracy\", novel_acc, epoch)\n",
    "\n",
    "        prefix = \"validation_adv\" if is_adv else \"validation_ce\"\n",
    "        self.writer.add_scalar(f\"{prefix}_base/loss\", base_loss, epoch)\n",
    "        self.writer.add_scalar(f\"{prefix}_base/accuracy\", base_acc, epoch)\n",
    "        self.writer.add_scalar(f\"{prefix}_novel/loss\", novel_loss, epoch)\n",
    "        self.writer.add_scalar(f\"{prefix}_novel/accuracy\", novel_acc, epoch)\n",
    "\n",
    "    def log_final_metrics(self, tag, base_acc, novel_acc, step):\n",
    "        \"\"\"\n",
    "        Logs final accuracy metrics and updates CSV and metric tracker.\n",
    "\n",
    "        Args:\n",
    "            tag (str): Tag name for TensorBoard.\n",
    "            base_acc (float): Accuracy on base classes.\n",
    "            novel_acc (float): Accuracy on novel classes.\n",
    "            step (int): Training step or epoch index.\n",
    "        \"\"\"\n",
    "        harmonic = harmonic_mean(base_acc, novel_acc)\n",
    "        self.writer.add_scalars(\n",
    "            tag,\n",
    "            {\n",
    "                \"Harmonic Mean\": harmonic,\n",
    "                \"Base Accuracy\": base_acc,\n",
    "                \"Novel Accuracy\": novel_acc,\n",
    "            },\n",
    "            global_step=step + 1,\n",
    "        )\n",
    "\n",
    "        self.csv_logger.log(step + 1, base_acc, novel_acc)\n",
    "\n",
    "        if self.hparams is not None:\n",
    "            self.base_and_novel_metrics.update(step + 1, base_acc, novel_acc)\n",
    "\n",
    "        self.writer.flush()\n",
    "\n",
    "    def log_test_accuracy(self, step, acc, label):\n",
    "        \"\"\"\n",
    "        Logs test accuracy for a given label.\n",
    "\n",
    "        Args:\n",
    "            step (int): Step or epoch index.\n",
    "            acc (float): Accuracy value.\n",
    "            label (str): Label name for the accuracy metric.\n",
    "        \"\"\"\n",
    "        self.writer.add_scalar(f\"{label}/accuracy\", acc, step)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"\n",
    "        Closes the logger, writes hyperparameters and final metrics if available.\n",
    "        \"\"\"\n",
    "        metrics = self.base_and_novel_metrics.get_metrics() or []\n",
    "\n",
    "        \"\"\"\n",
    "        metric_dict = {\n",
    "            \"base_acc_after_base\": metrics[0][1] if metrics else 0,\n",
    "            \"novel_acc_after_base\": metrics[0][2] if metrics else 0,\n",
    "            \"harmonic_mean_after_base\": metrics[0][3] if metrics else 0,\n",
    "            \"base_acc_after_adv\": metrics[1][1] if metrics else 0,\n",
    "            \"novel_acc_after_adv\": metrics[1][2] if metrics else 0,\n",
    "            \"harmonic_mean_after_adv\": metrics[1][3] if metrics else 0,\n",
    "        }\"\"\"\n",
    "\n",
    "        if self.hparams is not None and metrics:\n",
    "\n",
    "            tmp = {}\n",
    "            # Set the prefix based on whether metrics are from base phase (index 0) or adversarial phase (index 1)\n",
    "            for idx, m in enumerate(metrics):\n",
    "                prefix = \"after_base\" if idx == 0 else \"after_adv\"\n",
    "                tmp[f\"epoch_{prefix}\"] = m[0]\n",
    "                tmp[f\"base_acc_{prefix}\"] = m[1]\n",
    "                tmp[f\"novel_acc_{prefix}\"] = m[2]\n",
    "                tmp[f\"harmonic_mean_{prefix}\"] = m[3]\n",
    "\n",
    "            self.writer.add_hparams(\n",
    "                hparam_dict=self.hparams,\n",
    "                metric_dict=tmp,\n",
    "            )\n",
    "        self.writer.close()\n",
    "        self.csv_logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7f3938d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: ./utils/training_coop.py\n",
    "\"\"\"\n",
    "This module provides training, evaluation, and testing functions for the CoOp model and standard CLIP evaluation,\n",
    "including fine-tuning and zero-shot classification on image datasets.\n",
    "\"\"\"\n",
    "from clip.model import CLIP\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "\n",
    "# from model.coop.custom_clip import CustomCLIPCoOp\n",
    "# from utils.datasets import ContiguousLabelDataset, CLASS_NAMES\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_step(model, dataset, cost_function, new_classnames, batch_size=32, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a given dataset using cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate.\n",
    "        dataset (Dataset): Dataset to evaluate on.\n",
    "        cost_function (Callable): Loss function to use.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "        device (str): Computation device (\"cuda\" or \"cpu\").\n",
    "        new_classnames (List[int] or None): Optional list of class indices to temporarily substitute for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Average loss and accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    tmp_dataset = ContiguousLabelDataset(dataset, new_classnames)\n",
    "    dataloader = DataLoader(tmp_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if new_classnames is not None:\n",
    "        new_classnames = [CLASS_NAMES[c] for c in new_classnames]\n",
    "        with model.temporary_classnames(new_classnames):\n",
    "            correct, total, total_loss = walk_the_dataset(correct, cost_function, dataloader, device, model, total,\n",
    "                                                          total_loss)\n",
    "\n",
    "    else:\n",
    "        correct, total, total_loss = walk_the_dataset(correct, cost_function, dataloader, device, model, total,\n",
    "                                                      total_loss)\n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def walk_the_dataset(correct, cost_function, dataloader, device, model, total, total_loss):\n",
    "    \"\"\"\n",
    "    Iterates over the dataset and computes cumulative loss and accuracy.\n",
    "\n",
    "    Args:\n",
    "        correct (int): Running count of correct predictions.\n",
    "        cost_function (Callable): Loss function used.\n",
    "        dataloader (DataLoader): DataLoader for the dataset.\n",
    "        device (str): Computation device.\n",
    "        model (nn.Module): Model being evaluated.\n",
    "        total (int): Total number of samples evaluated so far.\n",
    "        total_loss (float): Accumulated loss value.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, int, float]: Updated correct, total, and total_loss.\n",
    "    \"\"\"\n",
    "    for images, targets in tqdm(dataloader, desc=\"Validation\", position=1, leave=False):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        loss, logits = model(images, targets)\n",
    "\n",
    "        total_loss += loss.item() * targets.size(0)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    return correct, total, total_loss\n",
    "\n",
    "\n",
    "def training_step(model: CustomCLIPCoOp, dataset, optimizer, batch_size, classnames, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Performs one full training epoch for the CoOp model.\n",
    "\n",
    "    Args:\n",
    "        model (CustomCLIPCoOp): The model to train.\n",
    "        dataset (Dataset): Dataset to train on.\n",
    "        optimizer (Optimizer): Optimizer used for updating model parameters.\n",
    "        batch_size (int): Batch size for training.\n",
    "        device (str): Computation device.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float]: Average training loss and accuracy.\n",
    "    \"\"\"\n",
    "    samples = 0.0\n",
    "    cumulative_loss = 0.0\n",
    "    cumulative_accuracy = 0.0\n",
    "\n",
    "    # Set the network to training mode\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    tmp_dataset = ContiguousLabelDataset(dataset, classnames)\n",
    "    dataloader = DataLoader(tmp_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", position=1, leave=False)\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        # Load data into GPU\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        # debug if inputs and targets are taken correctly by the dataloader\n",
    "        #print(inputs.shape)\n",
    "        #print(targets.shape)\n",
    "        # Forward pass + loss computation\n",
    "        loss, logits = model(inputs, targets)\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(\"⚠️ NaN loss encountered!\")\n",
    "            #print(\"Logits:\", logits)\n",
    "            print(\"Targets:\", targets)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Parameters update\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gradients reset\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Fetch prediction and loss value\n",
    "        samples += inputs.shape[0]\n",
    "        cumulative_loss += loss.item() * inputs.shape[0]\n",
    "        _, predicted = logits.max(dim=1)  # max() returns (maximum_value, index_of_maximum_value)\n",
    "\n",
    "        # Compute training accuracy\n",
    "        cumulative_accuracy += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_postfix(train_loss=loss.item(), train_acc=cumulative_accuracy / samples )\n",
    "        pbar.update(1)\n",
    "\n",
    "    return cumulative_loss / samples, cumulative_accuracy / samples\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_step(model, dataset, batch_size, device, categories, label=\"test\", base=False):\n",
    "    \"\"\"\n",
    "    Evaluates the model using either fine-tuned or base (zero-shot) strategy.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to test.\n",
    "        dataset (Dataset): Dataset for testing.\n",
    "        batch_size (int): Batch size for testing.\n",
    "        device (str): Device used for computation.\n",
    "        label (str): Label for the progress bar.\n",
    "        base (bool): Whether to use zero-shot CLIP instead of the fine-tuned model.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy score.\n",
    "    \"\"\"\n",
    "    if not base:\n",
    "        return finetuned_test_step(model, dataset, batch_size, device, categories, label)\n",
    "    else:\n",
    "        return base_test_step(model, dataset, categories, batch_size, device, label)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def finetuned_test_step(model: CustomCLIPCoOp, dataset, batch_size, device, categories, label=\"test\"):\n",
    "    \"\"\"\n",
    "    Evaluates a fine-tuned CustomCLIPCoOp model on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        model (CustomCLIPCoOp): Fine-tuned CoOp model.\n",
    "        dataset (Dataset): Dataset for evaluation.\n",
    "        batch_size (int): Batch size.\n",
    "        device (str): Computation device.\n",
    "        label (str): Label for progress display.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of the model on the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    tmp_dataset = ContiguousLabelDataset(dataset, categories)\n",
    "    dataloader = DataLoader(tmp_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, targets in tqdm(dataloader, desc=label):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        predictions = logits.argmax(dim=-1)\n",
    "\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "@torch.no_grad()  # we don't want gradients\n",
    "def base_test_step(model: CLIP, dataset, categories, batch_size, device, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluates a zero-shot CLIP model using cosine similarity between image and text embeddings.\n",
    "\n",
    "    Args:\n",
    "        model (CLIP): Pretrained CLIP model.\n",
    "        dataset (Dataset): Dataset to evaluate.\n",
    "        categories (List[int]): List of category indices to evaluate.\n",
    "        batch_size (int): Batch size for evaluation.\n",
    "        device (str): Computation device.\n",
    "        label (str): Optional label for progress bar.\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy of zero-shot CLIP classification.\n",
    "    \"\"\"\n",
    "    # let's set the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Remap labels into a contiguous set starting from zero\n",
    "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
    "\n",
    "    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
    "    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
    "    text_inputs = clip.tokenize(\n",
    "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
    "    ).to(device)\n",
    "\n",
    "    # we can encode the text features once as they are shared for all images\n",
    "    # therefore we do it outside the evaluation loop\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "    # and here we normalize them (standard pratice with CLIP)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    # simple dataloader creation\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # here we store the number of correct predictions we will make\n",
    "    correct_predictions = 0\n",
    "    for image, target in tqdm(dataloader, desc=label):\n",
    "        # base categories range from 0 to 50, while novel ones from 51 to 101\n",
    "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
    "        # Map targets in contiguous set starting from zero\n",
    "        # Labels needs to be .long() in pytorch\n",
    "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # forward image through CLIP image encoder\n",
    "        image_features = model.encode_image(image)\n",
    "        # and normalize\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
    "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
    "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
    "        correct_predictions += (predicted_class == target).sum().item()\n",
    "\n",
    "    # and now we compute the accuracy\n",
    "    accuracy = correct_predictions / len(dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2187ae66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Using CoCoOp for training\n",
      "run_name: jupyter_notebook_test_kl+adv_20250723_130854, using ViT-B/16, pat: False\n",
      "BATCH SIZES:  10 10 10\n",
      "patience disabled\n",
      "Base classes length: 51, Novel classes length: 51\n",
      "Train base length: 510, Val base length: 510, Test base length: 2473\n",
      "Val novel length: 510, Test novel length: 3676\n",
      "mlp adversary struct:  AdversarialMLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=563, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): ResidualBlock(\n",
      "      (linear): Linear(in_features=563, out_features=256, bias=True)\n",
      "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "      (drop): Dropout(p=0.2, inplace=False)\n",
      "      (residual): Linear(in_features=563, out_features=256, bias=True)\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (linear): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (relu): ReLU()\n",
      "      (drop): Dropout(p=0.2, inplace=False)\n",
      "      (residual): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (4): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      ")\n",
      "🟧 NO CLUSTERS FILES FOUND (jupyter version without saves). Loading existing cluster labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing classes: 100%|██████████| 51/51 [01:55<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced feature shape: (51, 31), Variance explained: 0.9532072318917544\n",
      "Cluster 0 count: 19\n",
      "Cluster 1 count: 15\n",
      "Cluster 2 count: 8\n",
      "Cluster 3 count: 9\n",
      "\n",
      "Training system initialized with run name: jupyter_notebook_test_kl+adv_20250723_130854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Base Training:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n",
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_net_dtype: torch.float16, im_features dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# From: ./main.py\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "# from training_systems.cocoop import CoCoOpSystem\n",
    "# from training_systems.coop import CoOpSystem\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--device', default=os.getenv(\"DEVICE\", \"cuda:0\"))\n",
    "    parser.add_argument('--run_name', default=f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "    parser.add_argument('--using_coop', default=False, type=lambda x: x.lower() in (\"1\", \"true\", \"yes\", \"true\"))\n",
    "    parser.add_argument('--config', default=\"train_config.yaml\")\n",
    "    parser.add_argument('--debug', default=True, type=lambda x: x.lower() in (\"1\", \"true\", \"yes\", \"true\"))\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Assign parsed arguments to variables\n",
    "    # Display which device is being used\n",
    "    # Handle MPS backend by setting default tensor type to float32\n",
    "    # Indicate whether CoOp or CoCoOp is used for training\n",
    "    # Load training configuration from YAML file\n",
    "\n",
    "    runs_yaml = [\"test_kl+adv\"]\n",
    "    runs_config = [{\n",
    "        \"coop\": {\n",
    "            \"batch_size\": 10,\n",
    "            \"learning_rate\": 0.002,\n",
    "            \"weight_decay\": 0.0001,\n",
    "            \"momentum\": 0.9,\n",
    "            \"epochs\": 20,\n",
    "            \"n_ctx\": 8,\n",
    "            \"ctx_init\": \"\",\n",
    "            \"class_token_position\": \"end\",\n",
    "            \"csc\": False\n",
    "        },\n",
    "        \"cocoop\": {\n",
    "            \"report\": True,\n",
    "            \"pat\": False,\n",
    "            \"cnn_model\": \"ViT-B/16\",\n",
    "            \"test_batch_size\": 10,\n",
    "            \"train_base_checkpoint_path\": None, # Cant have outside files in the jupyter\n",
    "            \"optimizer_configs\": [\n",
    "                {\n",
    "                    \"prompt_lr\": 0.002,\n",
    "                    \"weight_decay\": 0.0001,\n",
    "                    \"momentum\": 0.9\n",
    "                },\n",
    "                {\n",
    "                    \"prompt_lr\": 0.002,\n",
    "                    \"mlp_lr\": 0.005,\n",
    "                    \"weight_decay\": 0.0005,\n",
    "                    \"momentum\": 0.8\n",
    "                }\n",
    "            ],\n",
    "            \"skip_tests\": [True, True, False],\n",
    "            \"prompt_learner_opt\": {\n",
    "                \"n_ctx\": 4,\n",
    "                \"ctx_init\": \"\",\n",
    "                \"class_token_position\": \"end\",\n",
    "                \"csc\": False\n",
    "            },\n",
    "            \"kl_loss_opt\": {\n",
    "                \"lambda_kl\": [0.1, 0.1],\n",
    "                \"using_kl\": [True, False]\n",
    "            },\n",
    "            \"adv_training_opt\": {\n",
    "                \"adv_training_epochs\": 40,\n",
    "                \"batch_size\": 10,\n",
    "                \"warmup_lambda_adv\": 10,\n",
    "                \"lambda_adv\": 3,\n",
    "                \"grl_lambda\": 1,\n",
    "                \"mlp_opt\": {\n",
    "                    \"hidden_structure\": [563, 256, 128]\n",
    "                },\n",
    "                \"prompt_learner_warmup_epochs\": 3,\n",
    "                \"ignore_no_improvement\": True,\n",
    "                \"use_bias_ctx\": True\n",
    "            },\n",
    "            \"base_training_opt\": {\n",
    "                \"epochs\": 5,\n",
    "                \"batch_size\": 10,\n",
    "                \"warmup_epoch\": 0,\n",
    "                \"warmup_cons_lr\": 1e-5\n",
    "            },\n",
    "            \"clustering_opt\": {\n",
    "                \"n_clusters\": 4,\n",
    "                \"variance\": 0.95,\n",
    "                \"vision_encoder\": \"ViT-B/32\",\n",
    "                \"clustering_type\": \"semantic\"\n",
    "            }\n",
    "            }\n",
    "        },]\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    debug = False\n",
    "    use_coop = False\n",
    "\n",
    "    for run_id, run_yaml in enumerate(runs_yaml):\n",
    "        run_name = \"jupyter_notebook_\"+ run_yaml+ \"_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        if torch.backends.mps.is_available():\n",
    "            print(\"\\u26a0\\ufe0f Forcing float32 due to MPS limitations\")\n",
    "            torch.set_default_dtype(torch.float32)\n",
    "\n",
    "        print(f\"Using {'CoOp' if use_coop else 'CoCoOp'} for training\")\n",
    "\n",
    "        config = runs_config[run_id] if isinstance(runs_config, list) else runs_config\n",
    "\n",
    "        if use_coop:\n",
    "            coop_cfg = config['coop']\n",
    "            train_sys = CoOpSystem(\n",
    "                device=device,\n",
    "                run_name=run_name,\n",
    "                **coop_cfg\n",
    "            )\n",
    "        else:\n",
    "            cocoop_cfg = config['cocoop']\n",
    "            train_sys = CoCoOpSystem(\n",
    "                device=device,\n",
    "                run_name=run_name,\n",
    "                debug=debug,\n",
    "                hparams_file=run_yaml,\n",
    "                **cocoop_cfg\n",
    "            )\n",
    "        print(f\"\\nTraining system initialized with run name: {train_sys.run_name}\")\n",
    "        train_sys.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
